---

title: "MA681 Final Project"
author: "Claudio Toro Serey"
date: "Fall 2017"
output:
  html_document: default
  fig_caption: true
  #pdf_document:
  #  pandoc_args: [
  #  "-V", "classoption=twocolumn"]  
#title: "Effort and Delay Discounting in a Foraging Environment"
#author: "Claudio Toro Serey"
#date: "MA681 Fall 2017"
 
---

```{r global_options, include=FALSE}

knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)

```

``` {r, echo=FALSE}
##------------- functions, libraries, and setups--------------
# libraries
library(tidyverse)
library(knitr)

## Miscelaneous
cols = c("#78AB05","#D9541A") # plot colors (wait, effort)
lthick = 2 # line thickness for plots

## Functions
# Permutation for 2 groups (equal n for paired)
permute <- function(group1 = 1, group2 = 2, statType = mean, nPerms = 5000, paired = FALSE){
  
    # prep data
    summaryPerm <- list()
    lOne <- length(group1)
    lTwo <- length(group2)
    bigSample <- c(group1,group2)  
    
    if (paired == FALSE) {
      
        
        for (i in 1:nPerms){
          
            # relabel samples
            tempBig <- sample(bigSample)
            tempOne <- tempBig[1:lOne]
            tempTwo <- tempBig[(lOne+1):length(bigSample)]
            
            # stats
            tempDiffs <- statType(tempOne,na.rm=T) - statType(tempTwo,na.rm=T)
            summaryPerm$jointDist[i] <- tempDiffs # statType(tempDiffs, na.rm = T) 
          
        }  
        
    } else {
      
        for (i in 1:nPerms){
          
            # shift labels in a pairwise fashion
            tempDiffs <- statType((-1)^rbinom(lOne,1,0.5) * (group1 - group2))
            summaryPerm$jointDist[i] <- tempDiffs
          
        }
      
    }
  
    # get the observed difference
    diffs <- statType(group1,na.rm=T) - statType(group2,na.rm=T)
    observedAbs <- abs(diffs) # maybe leave it as means here
    observed <- diffs
    summaryPerm$Pval <- 2 * (1 - ecdf(summaryPerm$jointDist)(observedAbs))
    if (length(unique(abs(summaryPerm$jointDist))) == 1) {summaryPerm$Pval <- 1} # if the difference is always the same, then p = 1
    summaryPerm$Observed <- observed
    
    return(summaryPerm)
    
}

# Non-parametric Bootstrap for a single group
bootstrap <- function(group = 1, statType = mean, B = 5000){
  
    # prep param
    bootStats <- rep(0,B)
    
    # iterate
    for(b in 1:B){
      
      # wait group
      x <- sample(group,length(group),replace=T)  
      bootStats[b] <- statType(x,na.rm = T)
      
    }
    
    return(bootStats)
    
}

# plot logistic (for the prop complete test, mainly, but it can be easily expanded for more flexibility in n-params)
plotLogis <- function(coeffs = 1, group = "Wait", handling = 1:14, reward = 1:25, xAxis = "Reward", plotType = lines, color = "black", lType = 1, pchType = 16){
  
    
    # divide into relevant coefficients (assumes logisComb was the best model)
    intercept <- coeffs[1]
    groupBeta <- coeffs[2]
    handBeta <- coeffs[3]
    rwdBeta <- coeffs[4]
    
    # now the parameters
    label = "Reward Amount"
    if (group == "Wait"){group = 1} else {group = 0}
    if (xAxis == "Reward"){
        xAxis = reward 
        label = "Reward Amount" 
    } else {
        xAxis = handling
        label = "Handling Time"
    }
    
    # logistic function
    model <- intercept + groupBeta*group + handBeta*handling + rwdBeta*reward 
    func <- 1 / (1 + exp(-(model)))
    plotType(xAxis,
         func,
         ylim = c(0,1),
         xlab = label,
         ylab = "Probability of Completing a Trial",
         type="l",
         col = color,
         lty = lType,
         pch = pchType,
         lwd = 2,
         xaxt = "n")
  
    # use title() to add title afterwards
}

# Cohen's D for 2 groups
# for a more flexible approach, make the data input to be a list with entries for n groups, 
# then do length(list) for the number of groups. 
cohenD <- function(group1 = 1, group2 = 2){
  
      # means
      mean1 <- mean(group1, na.rm = T)
      mean2 <- mean(group2, na.rm = T)
      
      # variance
      var1 <- var(group1, na.rm = T)
      var2 <- var(group2, na.rm = T)
      
      # equation
      out <- (mean1 - mean2) / sqrt((var1 + var2)/2)
      
      return(out)
  
}

# get the number of continuous quits before current trial
seqQuits <- function(subjData = 1){
  
    # get the lag
    tempSeq <- sequence(rle(subjData)$lengths) # sequence
    tempSeq[subjData==1] <- 0 # not interested in completed runs, just quits
    tempSeq <- c(0,tempSeq) # lag it so it can be used as a t-1 regressor

    # return data so that the length is equivalent
    out <- tempSeq[1:length(subjData)]
    
    return(out)
    
}

# calculate C-hat (dispersion) for a regular GLM
dispCalc <- function(object) {
with(object,sum((weights * residuals^2)[weights > 0])/df.residual)
}
```



``` {r, echo=FALSE}
### data loading and cleaning up

# load cognitive effort group
setwd('./data/cog')
temp = list.files(pattern="*.csv")
cogDataLong = lapply(temp, read.csv, header = FALSE)

# load wait group
setwd('../wait')
temp = list.files(pattern="*.csv")
waitDataLong = lapply(temp, read.csv, header = FALSE)

# load physical group
setwd('../phys')
temp = list.files(pattern="*.csv")
physDataLong = lapply(temp, read.csv, header = FALSE)

### cleaning data
# this will generate a simple list with only info about trials that participants quit or completed
# this is because cognitive logs also include within-trial task-specific information
# for the wait condition, it will get rid of the break indicator 

nSubs <- length(waitDataLong)

#  short lists
cogData <- list()
waitData <- list()
physData <- list()

# ASDADS
cogBefore <- list()
cogAfter <- list()
waitBefore <- list()
waitAfter <- list()
physBefore <- list()
physAfter <- list()

# clean 
for (subject in 1:nSubs){
  
  #--------- index for storing before/after----------
  breakW <- which(waitDataLong[[subject]]$V1 == 0)
  breakC <- which(cogDataLong[[subject]]$V1 == 0)
  breakP <- which(physDataLong[[subject]]$V1 == 0)
  
  cogBefore[[subject]] <- cogDataLong[[subject]][1:(breakC-1),]
  temp <- cogBefore[[subject]]$V7 # trial outcome vector
  quitTrials <- grep("Quit",temp)
  completeTrials <- grep("Reward",temp)
  decisionTrials <- sort(c(quitTrials,completeTrials))
  cogBefore[[subject]] <- cogBefore[[subject]][decisionTrials,]
  
  cogAfter[[subject]] <- cogDataLong[[subject]][(breakC+1):length(cogDataLong[[subject]]$V1),]
  temp <- cogAfter[[subject]]$V7 # trial outcome vector
  quitTrials <- grep("Quit",temp)
  completeTrials <- grep("Reward",temp)
  decisionTrials <- sort(c(quitTrials,completeTrials))
  cogAfter[[subject]] <- cogAfter[[subject]][decisionTrials,]
  
  waitBefore[[subject]] <- waitDataLong[[subject]][1:(breakW - 1),]
  waitAfter[[subject]] <- waitDataLong[[subject]][(breakW + 1):length(waitDataLong[[subject]]$V1),] 
  
  physBefore[[subject]] <- physDataLong[[subject]][1:(breakP - 1),]
  physAfter[[subject]] <- physDataLong[[subject]][(breakP + 1):length(physDataLong[[subject]]$V1),] 
  
  # assign header names
  colnames(waitBefore[[subject]]) <- c("Handling","Reward","Choice","RT","Experiment Time")
  colnames(waitAfter[[subject]]) <- c("Handling","Reward","Choice","RT","Experiment Time")

  colnames(physBefore[[subject]]) <- c("Handling","Reward","Choice","RT","Experiment Time")
  colnames(physAfter[[subject]]) <- c("Handling","Reward","Choice","RT","Experiment Time")
    
  colnames(cogBefore[[subject]]) <- c("Handling","Reward","Choice","Task RT","Choice RT","Experiment Time","Trial Outcome","Task Type","Task Setup")
  colnames(cogAfter[[subject]]) <- c("Handling","Reward","Choice","Task RT","Choice RT","Experiment Time","Trial Outcome","Task Type","Task Setup")
  #--------------------------------------------------
  
  
  # create an index for the trials in which a decision was made (completed, quit) 
  temp <- cogDataLong[[subject]]$V7 # trial outcome vector
  quitTrials <- grep("Quit",temp)
  completeTrials <- grep("Reward",temp)
  decisionTrials <- sort(c(quitTrials,completeTrials))
  
  # remove the break trial
  tempW <- waitDataLong[[subject]]$V1 != 0
  tempC <- cogDataLong[[subject]]$V1 != 0
  tempP <- physDataLong[[subject]]$V1 != 0
  
  # create a new list with only decision trials / without break
  cogData[[subject]] <- cogDataLong[[subject]][decisionTrials,]
  cogDataLong[[subject]] <- cogDataLong[[subject]][tempC,]
  waitData[[subject]] <- waitDataLong[[subject]][tempW,]
  physData[[subject]] <- physDataLong[[subject]][tempP,]
  
  # assign header names
  colnames(waitData[[subject]]) <- c("Handling","Reward","Choice","RT","Experiment Time")
  colnames(physData[[subject]]) <- c("Handling","Reward","Choice","RT","Experiment Time")
  colnames(cogData[[subject]]) <- c("Handling","Reward","Choice","Task RT","Choice RT","Experiment Time","Trial Outcome","Task Type","Task Setup")
  colnames(cogDataLong[[subject]]) <- c("Handling","Reward","Choice","Task RT","Choice RT","Experiment Time","Trial Outcome","Task Type","Task Setup") # Long contains within-trial info
  
}


#-------------------------

cBefore <- rbind(cogBefore[[1]],cogBefore[[2]]) 
cAfter <- rbind(cogAfter[[1]],cogAfter[[2]]) 
wBefore <- rbind(waitBefore[[1]],waitBefore[[2]]) 
wAfter <- rbind(waitAfter[[1]],waitAfter[[2]]) 
pBefore <- rbind(physBefore[[1]],physBefore[[2]])
pAfter <- rbind(physAfter[[1]],physAfter[[2]])

for (i in 3:11){
  
    cBefore <- rbind(cBefore,cogBefore[[i]])  
    cAfter <- rbind(cAfter,cogAfter[[i]])
    wBefore <- rbind(wBefore,waitBefore[[i]])
    wAfter <- rbind(wAfter,waitAfter[[i]])
    pBefore <- rbind(pBefore,physBefore[[i]])
    pAfter <- rbind(pAfter,physAfter[[i]])
    
}

#rm(cogBefore,cogAfter,waitBefore,waitAfter)

# aggregate ADD PHYS
cogBeforeAll <- aggregate(cBefore$Choice,
          by = list(cBefore$Reward, cBefore$Handling),
          FUN = 'mean')
cogAfterAll <- aggregate(cAfter$Choice,
          by = list(cAfter$Reward, cAfter$Handling),
          FUN = 'mean')
waitBeforeAll <- aggregate(wBefore$Choice,
          by = list(wBefore$Reward, wBefore$Handling),
          FUN = 'mean')
waitAfterAll <- aggregate(wAfter$Choice,
          by = list(wAfter$Reward, wAfter$Handling),
          FUN = 'mean')

rm(cBefore,cAfter,wBefore,wAfter)
#--------------------------
# remove unnecessary variables
rm(temp,tempW,tempC,quitTrials,completeTrials,decisionTrials,subject)
```

## Effort and Delay Discounting in a Foraging Environment
### Claudio Toro Serey
### Abstract
Both delay and effort (i.e. physical and mental) have been shown to reduce the value of prospects, such that people tend to opt for less valuable outcomes if they can avoid waiting or exerting effort. Effort demands can influence the amount of time until an outcome is received, but to this date no study has tried dissociating these factors under equivalent scales. In this project, participants foraged for rewards under either pure delay or effortful conditions, with time-until-outcome being matched for both. While I expected the effort group to accept fewer trials overall, the data showed the opposite. This finding is at odds with previous literature (suggesting that effort can boost the value of outcomes) and encourages further study into the value-modulating properties of effortful demands.

### Introduction
Decisions lie at the actionable core of human experience. Much of what we see in others, such as the way they dress or what they eat, can be conveyed as a result of their choices. An important aspect of such decisions involves weighing the value of an outcome against the cost incurred in obtaining it. This cost-value interaction has many forms and characteristics, such as the allocation of monetary resources, or the uncertainty of obtaining an outcome versus a sure gain. A considerable amount of work has been focused on understanding the nature of value (Schultz, 2015). Cost, on the other hand, is less understood. A prevalent idea is that effort can act as a discounting factor, meaning that it reduces the perceived value of prospects.  Indeed, effort has been shown to be an actively avoided cost that devalues rewards, so that decision makers will prefer small rewards tied to low demands over large rewards that require high effort (Massar, Libedinsky, Weiyan, Huettel, & Chee, 2015). Importantly, this effect is independent from the amount of errors made in the task (Apps, Grima, Manohar, & Husain, 2015; Kool, McGuire, Rosen, & Botvinick, 2010; Westbrook, Kester, & Braver, 2013).
In the animal literature, the amount of effort a subject is willing to exert for a reward can be probed by increasing the ratio of responses required per each reinforcement until an animal refuses to work for the reward (Richardson & Roberts, 1996). Although findings from such studies have provided important information about the cost of effort in general, controversy regarding the particular nature of cognitive and physical effort persists (Westbrook & Braver, 2015). This debate has motivated investigators to characterize these types of effort more precisely (Schmidt, Lebreton, Cléry-Melin, Daunizeau, & Pessiglione, 2012; Walton, Kennerley, Bannerman, Phillips, & Rushworth, 2006). However, even though effort can be seen as cost in and of itself (Botvinick, Huffstetler, & McGuire, 2009), in ecological scenarios the magnitude of effort required by a task often affects the amount of time that is spent performing it. For example, going through an additional section of one’s tax return involves weighing both the mental effort and the extra time it takes against the reward attained. This is an important consideration. Multiple studies show that delayed rewards are discounted, so that having to wait longer for an outcome makes it less rewarding (Ainslie, 1975; Frederick, Loewenstein, & O ’donoghue, 2002; Green, Fristoe, & Myerson, 1994; Mcclure, Ericson, Laibson, Loewenstein, & Cohen, 2004). Additional investigations suggest that delay is discounted in a qualitatively similar fashion to physical (Prévost, Pessiglione, Météreau, Cléry-Melin, & Dreher, 2010) and mental effort (Massar et al., 2015), but that the magnitude of discounting varies between cost types. Although these studies have examined cost differences by combining time with physical or cognitive effort, there is currently no evidence on how participants would actively choose between all three types of cost. Moreover, these studies use different scales for effort and timing (e.g. the participant can either wait for weeks to be rewarded, or type 50 words backwards-an effortful task), thus retaining the time confound of effortful tasks. Finally, recent findings demonstrate that cost related to physical and mental effort is computed differently in the brain (Hosking, Floresco, & Winstanley, 2014; Kurniawan et al., 2011; Westbrook & Braver, 2015), thus highlighting the need to examine these effects simultaneously. Because of this, it is important to understand if and how effort acts as a source of cost that is distinct from delay, and how this dissociation is represented in people’s choices.
The overarching aim of this project is to use behavioral and neuroimaging methods to investigate the potential differences in cost between time, cognitive effort, and physical effort. However, the results presented here pertain to behavioral pilot data that is limited to cognitive effort and pure delay. These attributes are examined in a between-subject manner in order to provide a baseline that will guide future within-subject examinations. In particular, we explore how participants behave when faced with a foraging task. There are two main hypothesis in this project: 1) Participants should be less likely to accept low-reward and/or long-delay/effort trials; and 2) Participants in the cognitive group will accept fewer trials overall.

### Methods
Subjects:  A total of 22 healthy subjects (age 18-23; 13 female) were recruited from Psychology 101 classes through the Boston University SONA system. Participants were told to acquire as many points as possible during the task. After the task, subjects were questioned on their within-task behavior, and all mentioned that they followed a specific strategy that they developed early on in the task. We thus found no reason to exclude any participants for this analysis.
Foraging task: Participants faced individual trials in which they wait a given amount of time for a reward (the handling time). If the reward amount was not worth the time, the participant could quit the trial by pressing a key, forfeit the reward, and wait a different amount of time (traveling time) to try a new (and potentially more rewarding) trial. Participants were shown the total amount of points they can earn at the beginning of each trial (5, 10, or 25 points, uniformly distributed). These amounts were explicitly disclosed during the practice session in order to avoid any confusion about the possible reward amounts throughout the experiment. The total experiment time was divided into six 7-minute blocks. Each block had one of three predefined combinations of handling and travel times, all of which added up to 16 seconds (handling times could be 2, 10, or 14 seconds long). All three timing combinations were shown in a semi-random order, once before and after a break, and each was visually disclosed to the participant at the beginning of each block.
Subjects were divided into two groups: the “wait” group passively waited during the handling time, while the “cognitive” group completed a random sequence of two-second-long cognitive tasks that collectively lasted the equivalent of the handling time. This kind of task switching is demonstrably effortful (Apps et al., 2015; Kool et al., 2010). If a participant made more than two errors during the handling time, they were forced to travel (forced travel trials were not included in decision-related analyses). This contingency was added to guarantee engagement.
Calculating optimal behavior:  Having predefined handling, traveling, and reward amounts allowed me to compute the optimal behavior for each block. In the foraging literature, the possible per-trial reward is often weighed against the per-second rate of reward (Constantino & Daw, 2015). This is called the opportunity rate. By multiplying the opportunity rate by the handling time, decision makers can estimate the opportunity cost of time. This measure conveys the richness of the environment, and is equivalent to the participant contemplating how much they could be making instead of waiting for the current reward. The resulting optimal behavior was to accept everything when the handling time was 2 seconds, accept 10 and 25 points when it was 10 seconds, and to only accept 25-point trials when it lasted 14 seconds.

Statistical analyses: Choices are taken to be I.I.D. Bernoulli random variables (0 = quit, 1 = accept). Due to the non-gaussian nature of the data (choice and proportion of completed trials), as well as the small sample sizes, I give preference to non-parametric analyses when possible (i.e. when I am comparing an overall measure between both groups). On the parametric end: KS test was used to compare response time distributions for all participants, since response times tend to have non-Gaussian distributions. In order to get a general sense of the difference for all combinations of experimental factors, I performed a repeated-measures ANOVA, using handling time and rewards as within-subject factors. To check for the importance of each parameter, I performed model selection on a set of logistic regressions that predicted the proportion completed by each subject across conditions. The GLM was performed under the assumption of a binomial distribution for choices, even though proportions were used. This allowed me to compute AIC. Finally, I performed an auto-regressive process to identify whether participants were merely being influenced by the number of skips they had consecutively performed. Since this analysis was performed per-subject, individual choices were used, and thus I fully assumed a binomial distribution. I dwell on the specifics of each analysis and their assumptions in the results. 

Statistical analyses: Due to the non-gaussian nature of the data (choice and proportion of completed trials), as well as the small sample sizes, I give preference to non-parametric analyses when possible (i.e. when I am comparing an overall measure between both groups). On the parametric end: KS test was used to compare response time distributions for all participants, since response times tend to have non-Gaussian distributions. In order to get a general sense of the difference for all combinations of experimental factors, I performed a repeated-measures ANOVA, using handling time and rewards as within-subject factors. To check for the importance of each parameter, I performed model selection on a set of logistic regressions that predicted the proportion completed by each subject across conditions. The GLM was performed under the assumption of a binomial distribution for choices, even though proportions were used. This allowed me to compute AIC. Finally, I performed an auto-regressive process to identify whether participants were merely being influenced by the number of skips they had consecutively performed. Since this analysis was performed per-subject, individual choices were used, and thus I fully assumed a binomial distribution. I dwell on the specifics of each analysis and their assumptions in the results. 

### Results
``` {r,echo=FALSE,fig.align="center",fig.width=6,fig.height=4, fig.cap="\\label{fig:figs}plotting example"}
## overall summaries
summaryData <- data.frame(group = c(rep("Wait",nSubs),rep("Effort",nSubs)))
  
## gather basic data
for (subj in 1:nSubs){
    
    # choice index
    tempChoiceW <- waitData[[subj]]$Choice
    tempChoiceC <- cogData[[subj]]$Choice
  
    # propotion complete
    summaryData[subj,2] <- mean(tempChoiceW)
    summaryData[subj+11,2] <- mean(tempChoiceC)
    
    # earnings
    summaryData[subj,3] <- sum(waitData[[subj]]$Reward[tempChoiceW==1])
    summaryData[subj+11,3] <- sum(cogData[[subj]]$Reward[tempChoiceC==1])
    
    # RTs
    summaryData[subj,4] <- median(waitData[[subj]]$RT[tempChoiceW==0])
    summaryData[subj+11,4] <- median(cogData[[subj]]$`Choice RT`[tempChoiceC==0])
 
    # mistakes (only for effort group)
    tempMistakes <- sum(cogDataLong[[subj]]$`Trial Outcome`=="Forced travel") # how many forced travel trials
    tempTrialnum <- length(cogData[[subj]]$Choice) # how many decision trials
    summaryData[subj+11,5] <- tempMistakes / (tempMistakes + tempTrialnum) # proportion of forced travels throughout the exp
    
}

# name the data frame columns, and indexes for groups
colnames(summaryData) <- c("Group","propComplete","Earnings","RT","forcedTravel")
indxWait <- 1:11
indxWaitLong <- 1:99
indxCog <- 12:22
indxCogLong <- 100:198

```

#### Performance errors and outliers
In the effort condition, participants were forced to travel if they made more than 2 mistakes in a trial. To check if any participant showed significant struggles, I computed the inter-quartile range for the proportion of trials in which each participant was forced to travel (inter-quartile range = 0.18). A single outlier was found. However, due to the small sample size, and the fact that the participant had a clear strategy (disclosed post-experiment), I decided to retain this outlier for all analyses.

``` {r,echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# mistakes
# significant from 0
# mistakeTest <- wilcox.test(summaryData$forcedTravel)
# mistakePerm <- permute(rep(0,11), summaryData$forcedTravel[indxCog]) # revise

# outlier
interQ <- quantile(summaryData$forcedTravel[indxCog])[2] + quantile(summaryData$forcedTravel[indxCog])[4]
outlier <- which(summaryData$forcedTravel[indxCog] > interQ)
```
#### Propotion Completed
The most basic question that guides this project is which group accepted more trials. Under the present framework, the condition that triggered the least acceptances can be argued to be the costliest. Unexpectedly, the wait group had a mean proportion acceptance of 0.53 (sd = 0.11), while the effort group's mean proportion was 0.72 (sd = 0.19). Figure 1 shows the ECDFS and 95% confidence inervals for the proportion completed by each group.


``` {r figs,echo=FALSE,fig.width=4,fig.height=3, fig.cap="ECDFs of the proportion of trials accepted for both groups. The wait group appears to have accepted fewer trials overall."}
## prop complete
# ecdfs
wait <- ecdf(summaryData$propComplete[indxWait])
cog <- ecdf(summaryData$propComplete[indxCog])
range <- seq(0,1,by=0.01)

# plot
plot(range,
     wait(range),
     main = "ECDF for Proportion Completed",
     xlab = "Proportion Completed",
     ylab = "ECDF",
     col = cols[1],
     type="l",
     lwd = lthick)
lines(range,
      cog(range),
      col = cols[2],
      lwd = lthick)
legend("bottomright",
       c("Wait","Effort"),
       fill = cols,
       cex = 0.8)

# CIs basic
CIC <- qt(.975,nSubs-1)*(sd(summaryData$propComplete[indxCog])/sqrt(nSubs))
CIW <- qt(.975,nSubs-1)*(sd(summaryData$propComplete[indxWait])/sqrt(nSubs))
lines(range,cog(range) + CIC,col = cols[2], lty = 2, lwd = lthick)
lines(range,cog(range) - CIC,col = cols[2], lty = 2, lwd = lthick) 
lines(range,wait(range) + CIW,col = cols[1], lty = 2, lwd = lthick)
lines(range,wait(range) - CIW,col = cols[1], lty = 2, lwd = lthick)

rm(CIC,CIW)

# problem is, data is not normal, so we're going to bootstrap it and DEFINITELY permute
# Bootstrap nonparametric
B <- 5000
meanBootW <- bootstrap(summaryData$propComplete[indxWait],B = B) #rep(0,B)
meanBootC <- bootstrap(summaryData$propComplete[indxCog], B = B) #rep(0,B)
varBootW <- bootstrap(summaryData$propComplete[indxWait],B = B) #rep(0,B)
varBootC <- bootstrap(summaryData$propComplete[indxCog], B = B) #rep(0,B)

#CIs wait
meanW <- mean(meanBootW)
upperW <- quantile(meanBootW,0.975) # meanBoot?
lowerW <- quantile(meanBootW,0.025)

#CIs effort
meanC <- mean(meanBootC)
upperC <- quantile(meanBootC,0.975) # meanBoot?
lowerC <- quantile(meanBootC,0.025)

# formal testing
# prop.test(sum(cogData[[i]]$Choice),length(cogData[[i]]$Choice))
propTtest <- t.test(summaryData$propComplete[indxWait],summaryData$propComplete[indxCog])
propPerm <- permute(summaryData$propComplete[indxWait],summaryData$propComplete[indxCog])

# effect size
propRsq <- cohenD(summaryData$propComplete[indxWait],summaryData$propComplete[indxCog])

rm(B,cog,wait,subj)
```


The small size of the samples makes any distinction difficult to perceive, and the eastimates unreliable. In order to accomodate for this shortcoming, I took a handful of steps. First, I bootstrapped the mean proportion for each group separately (5000 iterations), so I could get a better estimate of the mean through CIs (which were computed based on the 0.025 and 0.975 quantiles). The bootstrapped statistics for the wait (M = $`r round(meanW, digits = 2)`$, lower CI = $`r round(lowerW, digits = 2)`$, upper CI = $`r round(upperW, digits = 2)`$) and effort (M = $`r round(meanC, digits = 2)`$, lower CI = $`r round(lowerC, digits = 2)`$, upper CI = $`r round(upperC, digits = 2)`$) were then used to plot the previous ECDF with the new confidence intervals (Figure 2, left). It can now be seen that the CIs do not overlap, suggesting a true difference between these groups. To formalize this comparison, I performed a permutation test on the mean difference in proportions between both groups (relabeling the data through 5000 iterations). The estimated null distribution of this difference is shown in Figure 2 (right). This distribution was plotted as an example. Further permutations will not be plotted. The resulting p-value was `r propPerm$Pval`, thus rejecting the null hypothesis that the difference between these groups would be 0. Just to be clear, from now on the p-values will mean the probability of finding a statistic at least as extreme as the one observed under the null distribution. Finally, I used Cohen's D to estimate the effect size of this comparison, which yielded a value of $`r round(propRsq, digits = 2)`$. This shows that the effort group completed a significantly larger amount of trials overall than the wait group.

``` {r,echo=FALSE,fig.align="center",fig.width=10,fig.height=4,fig.cap="Left: ECDFs of the proportion completed for each group, with bootstrapped confidence intervals computed from its quantiles (0.975 and0.025). Right: Resulting distribution of the permuted differences in means between the effort and wait groups (wait minus effort), and the actual difference marked by the red line."}

# plot ecdf 
par(mfrow=c(1,2))
ecdfW <- ecdf(meanBootW)
ecdfC <- ecdf(meanBootC)

wait <- ecdf(summaryData$propComplete[indxWait])
cog <- ecdf(summaryData$propComplete[indxCog])
range <- seq(0,1,by=0.01)

plot(range,
     wait(range),
     main = "ECDF for Proportion Completed",
     xlab = "Proportion Completed",
     ylab = "ECDF",
     col = cols[1],
     type="l",
     lwd = lthick)
lines(range,
      cog(range),
      col = cols[2],
      lwd = lthick)
legend("bottomright",
       c("Wait","Effort"),
       fill = cols,
       cex = 0.8)

# plot(range,ecdfW(range),
#      type = "l",
#      main = "ECDF for the Bootstrapped Proportions",
#      xlab = "Proportion Completed",
#      ylab = "ECDF",
#      xlim = c(0,1),
#      col = cols[1],
#      lwd = lthick)
# lines(range,
#       ecdfC(range),
#       col = cols[2],
#       lwd = lthick)
# legend("bottomright",
#        c("Wait","Effort"),
#        fill = cols,
#        cex = 0.8)

# and CIs for each (unnecessary unless I end up plotting)
CIC <- qt(.975,nSubs-1)*(sd(meanBootC)/sqrt(nSubs))
CIW <- qt(.975,nSubs-1)*(sd(meanBootW)/sqrt(nSubs))
lines(range,cog(range) + CIC,col = cols[2], lty = 2, lwd = lthick)
lines(range,cog(range) - CIC,col = cols[2], lty = 2, lwd = lthick) 
lines(range,wait(range) + CIW,col = cols[1], lty = 2, lwd = lthick)
lines(range,wait(range) - CIW,col = cols[1], lty = 2, lwd = lthick)

# lines(range,ecdfC(range) + CIC,col = cols[2], lty = 2, lwd = lthick)
# lines(range,ecdfC(range) - CIC,col = cols[2], lty = 2, lwd = lthick) #qt(.975,nSubs-1)*(sd(meanBootC)/sqrt(nSubs))
# lines(range,ecdfW(range) + CIW,col = cols[1], lty = 2, lwd = lthick)
# lines(range,ecdfW(range) - CIW,col = cols[1], lty = 2, lwd = lthick)

rm(CIC,CIW)


# plotting the permutation
plot(density(propPerm$jointDist),
     main = "Permuted Null Distribution \n Difference in Proportions Wait - Effort",
     xlab = "Mean Difference",
     lwd = lthick)
lines(c(propPerm$Observed,propPerm$Observed), c(0,1200), lty=2, col="red", lwd = lthick)

```

#### Total Earnings

```{r,echo=FALSE,fig.align="center",fig.width=10,fig.height=4, fig.cap="Left: box plot of the differences in earnings between groups. Right: relationship between proportion completed and cumulative earnings. A linear model (fit line in black) showed that both the linear (beta = 4078, SE = 1013, p-value < 0.001) and quadratic form (beta = -3275, SE = 715, p < 0.0001) of the proportion significantly predicted the total earned"}
# descriptives
meanC <- mean(summaryData$Earnings[indxCog])
seC <- (sd(summaryData$Earnings[indxCog])) / sqrt(11)
meanW <- mean(summaryData$Earnings[indxWait])
seW <- (sd(summaryData$Earnings[indxWait])) / sqrt(11)

# wilcoxon rank sum
#earningTest <- wilcox.test(summaryData$Earnings[indxWait], summaryData$Earnings[indxCog], alternative = "two.sided")
earningsPerm <-permute(summaryData$Earnings[indxWait], summaryData$Earnings[indxCog]) # I believe this more
earningsRsq <- cohenD(summaryData$Earnings[indxWait], summaryData$Earnings[indxCog])  

# linear model (think of doing a Poisson Regression instead, since earnings are not normally distributed [although bootstrapped they're fine])
propAllsq <- summaryData$propComplete^2 
lmEarn <- lm(Earnings ~ propComplete + propAllsq, data = summaryData)
coeffsEarn <- lmEarn$coefficients
x <- seq(0,1,length=100) # for plotting

# plotting
par(mfrow=c(1,2))
boxplot(summaryData$Earnings[indxWait],
        summaryData$Earnings[indxCog],
        names = c("Wait","Effort"),
        main = "Total Earnings for Each Group",
        ylab = "Total Earnings (pts)",
        xlab = "Group")

# earnings (inverse-U, which makes sense because either over or under accepting are detrimental to earnings)
plot(summaryData$propComplete[indxWait],
     summaryData$Earnings[indxWait],
     main = "Total Earnings \n as a Function of Proportion Accepted",
     xlab = "Total Proportion Completed",
     ylab = "Total Earned",
     col=cols[1],
     xlim=c(0.2,1),
     ylim = c(1000,2000),
     pch=16,
     cex = lthick - 0.5)
points(summaryData$propComplete[indxCog],
       summaryData$Earnings[indxCog],
       col=cols[2],
       pch=16,
       cex = lthick - 0.5)
legend("topright",
       c("Wait","Effort"),
       fill = cols,
       cex = 0.8)
lines(x, coeffsEarn[1] + coeffsEarn[2]*x + coeffsEarn[3]*(x^2),
       col="black",
      lwd = lthick)
```

Next, it was of interest to examine each group's total earnings. As seen on figure 3 (left), the effort group earned slightly less (M = `r meanC`, SE = `r round(seC, digits = 2)`) than the wait group (M = `r round(meanW)`, SE = `r round(seW, digits = 2)`). A permutation analysis of differences in mean earnings per group indicated a trend (p = `r earningsPerm$Pval`; Cohen's D = `r round(earningsRsq, digits = 2)`). The figure on the right shows a nonlinear relationship between acceptance rates and total earnings. A linear model was performed to estimate this relationship, using the total earned as the dependent variable and the proportion completed (and its quadratic form) as predictors. All of the terms significantly predicted cumulative earnings (p < 0.0001), with an $R^2$ of 0.52. This is in line with the nature of the experiment, as participants should cautiously balance rewards with opportunity cost, and both under- and over-acceptance were non-optimal.

#### Response Times
After checking for acceptance and earnings, another question is how fast participants were in making decisions (as they could quit at any point during the handling time). Perhaps participants were unclear about the experimental procedures, or changed their minds mid-trial. To account for this, I pooled all the response times for quit trials (since they are representative of a participant's choice to leave) across participants for each group. With these two vectors, I plotted their respective ECDFs (Figure 4) and bootstrapped 95% CIs. As it can be seen, most decisions were made before the first second, with the wait group being slightly slower. 

```{r,echo=FALSE,fig.align="center",fig.width=4,fig.height=3, fig.cap="ECDFs of pooled response times from each group. Responses were overall quick, with the wait group being slightly slower."}
# RTs
# let's try to do some survival curves
RTW <- numeric() # vector of all decision-related RTs for the wait group
RTC <- numeric() # same for the cog group

for (subj in 1:nSubs){
  
    # index for trials quit
    tempChoiceW <- waitData[[subj]]$Choice==0
    tempChoiceC <- cogData[[subj]]$Choice==0
  
    # concatenate the subject RT to the group
    RTW <- c(RTW,waitData[[subj]]$RT[tempChoiceW])
    RTC <- c(RTC,cogData[[subj]]$`Choice RT`[tempChoiceC])
    
}

# create an array with both data (for surv fit autoplot trials)
lenC <- length(RTC)
lenW <- length(RTW)

RTgroups <- c(rep("Wait",lenW), rep("Effort",lenC))
RTall <- c(RTW,RTC)

# plot ecdf 
ecdfW <- ecdf(RTW)
ecdfC <- ecdf(RTC)
range <- seq(0,10, by = 0.001)

plot(range,ecdfW(range),
     type = "l",
     main = "ECDF and CIs for the Response Times",
     xlab = "Seconds",
     ylab = "ECDF",
     xlim = c(0,5),
     col = cols[1],
     lwd = lthick)
lines(range,
      ecdfC(range),
      col = cols[2],
      lwd = lthick)
legend("right",
       c("Wait","Effort"),
       fill = cols,
       cex = 0.8)

# and CIs for each (unnecessary unless I end up plotting)
CIC <- bootstrap(group = RTC, statType = median)  #qt(.975,nSubs-1)*(sd(RTC)/sqrt(nSubs))
CIC <- median(CIC) - (quantile(CIC,0.025))
CIW <- bootstrap(group = RTW, statType = median)  #qt(.975,nSubs-1)*(sd(RTW)/sqrt(nSubs))
CIW <- median(CIW) - (quantile(CIW,0.025))
lines(range,ecdfC(range) + CIC,col = cols[2], lty = 2, lwd = lthick)
lines(range,ecdfC(range) - CIC,col = cols[2], lty = 2, lwd = lthick) 
lines(range,ecdfW(range) + CIW,col = cols[1], lty = 2, lwd = lthick)
lines(range,ecdfW(range) - CIW,col = cols[1], lty = 2, lwd = lthick)

# just cause, Kolmogrov-Smirnov 
ksRT <- ks.test(RTW,RTC)
RTPerm <- permute(RTW,RTC)
RTCohen <- cohenD(RTW,RTC)

rm(lenC,lenW,x)

```

Lastly, I performed a KS test to check if the difference between these distribution was not 0. The test proved significant (D = `r round(ksRT$statistic, digits = 2)`, p < 0.001), although this difference is not behaviorally very relevant for the present study (milisecond differences are inconsequential when we are interested in seconds). I confirmed this result by means of permutation analysis (p < 0.0001; Cohen's D = `r round(RTCohen, digits = 2)`). The small effect size also speaks to the potential lack of true differences between groups at this level.
The quick response times indicate that participants had a clear idea of what rewards were worth their time and effort. This is in agreement with the post-experiment questionaire, in which participant explained very specific strategies that they followed throughout the session.

#### Proportion per handling-reward combination 
The analyses above give us a general sense of the characteristics of each group. However, it also necessary to asses whether decision makers were influenced differently at each timing and reward combinations, and how these factors varied by group. First, I was interested in seeing differences as a whole. Figure 5 shows the proportion accepted (and standard error) for all these combinations.

``` {r,echo=FALSE,fig.align="center",fig.width=6,fig.height=3, results = 'asis'}

# params for reference
handling <- c(2,10,14)

# the other, just summarizing for all participants
allProp <- data.frame(Group = c(rep("Wait",nSubs*3),rep("Effort",nSubs*3)),
                      Handling = c(rep(2,nSubs), rep(10,nSubs), rep(14,nSubs),rep(2,nSubs), rep(10,nSubs), rep(14,nSubs)),
                      FivePts = rep(NA,nSubs*6),
                      TenPts = rep(NA,nSubs*6),
                      TwntyfivePts = rep(NA,nSubs*6))

allProp2 <- data.frame(SubjID = c(rep(1:11,9),c(rep(12:22,9))),
                       Group = c(rep("Wait",nSubs*9),rep("Effort",nSubs*9)),
                      Handling = c(rep(2,nSubs*3), rep(10,nSubs*3), rep(14,nSubs*3),rep(2,nSubs*3), rep(10,nSubs*3), rep(14,nSubs*3)),
                      Reward = rep(c(rep(5,nSubs),rep(10,nSubs),rep(25,nSubs)),3),
                      propComplete = rep(NA,198),
                      totalTrials = rep(NA,198),
                      totalComplete = rep(NA,198))


for (subj in 1:nSubs){
  
    # prop 1
    # wait group
    # handling of 2s
    allProp$FivePts[subj] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==2 & waitData[[subj]]$Reward==5])
    allProp$TenPts[subj] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==2 & waitData[[subj]]$Reward==10])
    allProp$TwntyfivePts[subj] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==2 & waitData[[subj]]$Reward==25])
    
    # handling of 10s
    allProp$FivePts[subj+11] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==10 & waitData[[subj]]$Reward==5])
    allProp$TenPts[subj+11] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==10 & waitData[[subj]]$Reward==10])
    allProp$TwntyfivePts[subj+11] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==10 & waitData[[subj]]$Reward==25])
    
    # handling of 14s
    allProp$FivePts[subj+22] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==14 & waitData[[subj]]$Reward==5])
    allProp$TenPts[subj+22] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==14 & waitData[[subj]]$Reward==10])
    allProp$TwntyfivePts[subj+22] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==14 & waitData[[subj]]$Reward==25])
    
    # effort group
    # handling of 2s
    allProp$FivePts[subj+33] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==2 & cogData[[subj]]$Reward==5])
    allProp$TenPts[subj+33] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==2 & cogData[[subj]]$Reward==10])
    allProp$TwntyfivePts[subj+33] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==2 & cogData[[subj]]$Reward==25])
    
    # handling of 10s
    allProp$FivePts[subj+44] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==10 & cogData[[subj]]$Reward==5])
    allProp$TenPts[subj+44] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==10 & cogData[[subj]]$Reward==10])
    allProp$TwntyfivePts[subj+44] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==10 & cogData[[subj]]$Reward==25])
    
    # handling of 14s
    allProp$FivePts[subj+55] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==14 & cogData[[subj]]$Reward==5])
    allProp$TenPts[subj+55] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==14 & cogData[[subj]]$Reward==10])
    allProp$TwntyfivePts[subj+55] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==14 & cogData[[subj]]$Reward==25])
    
    
    # prop 2
    # wait group
    # handling of 2s
    allProp2$propComplete[subj] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==2 & waitData[[subj]]$Reward==5])
    allProp2$propComplete[subj+11] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==2 & waitData[[subj]]$Reward==10])
    allProp2$propComplete[subj+22] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==2 & waitData[[subj]]$Reward==25])
    allProp2$totalTrials[subj] <- length(waitData[[subj]]$Choice[waitData[[subj]]$Handling==2 & waitData[[subj]]$Reward==5])
    allProp2$totalTrials[subj+11] <- length(waitData[[subj]]$Choice[waitData[[subj]]$Handling==2 & waitData[[subj]]$Reward==10])
    allProp2$totalTrials[subj+22] <- length(waitData[[subj]]$Choice[waitData[[subj]]$Handling==2 & waitData[[subj]]$Reward==25])

    # handling of 10s
    allProp2$propComplete[subj+33] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==10 & waitData[[subj]]$Reward==5])
    allProp2$propComplete[subj+44] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==10 & waitData[[subj]]$Reward==10])
    allProp2$propComplete[subj+55] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==10 & waitData[[subj]]$Reward==25])
    allProp2$totalTrials[subj+33] <- length(waitData[[subj]]$Choice[waitData[[subj]]$Handling==10 & waitData[[subj]]$Reward==5])
    allProp2$totalTrials[subj+44] <- length(waitData[[subj]]$Choice[waitData[[subj]]$Handling==10 & waitData[[subj]]$Reward==10])
    allProp2$totalTrials[subj+55] <- length(waitData[[subj]]$Choice[waitData[[subj]]$Handling==10 & waitData[[subj]]$Reward==25])    
    
    # handling of 14s
    allProp2$propComplete[subj+66] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==14 & waitData[[subj]]$Reward==5])
    allProp2$propComplete[subj+77] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==14 & waitData[[subj]]$Reward==10])
    allProp2$propComplete[subj+88] <- mean(waitData[[subj]]$Choice[waitData[[subj]]$Handling==14 & waitData[[subj]]$Reward==25])
    allProp2$totalTrials[subj+66] <- length(waitData[[subj]]$Choice[waitData[[subj]]$Handling==14 & waitData[[subj]]$Reward==5])
    allProp2$totalTrials[subj+77] <- length(waitData[[subj]]$Choice[waitData[[subj]]$Handling==14 & waitData[[subj]]$Reward==10])
    allProp2$totalTrials[subj+88] <- length(waitData[[subj]]$Choice[waitData[[subj]]$Handling==14 & waitData[[subj]]$Reward==25])

    # effort group
    # handling of 2s
    allProp2$propComplete[subj+99] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==2 & cogData[[subj]]$Reward==5])
    allProp2$propComplete[subj+110] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==2 & cogData[[subj]]$Reward==10])
    allProp2$propComplete[subj+121] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==2 & cogData[[subj]]$Reward==25])
    allProp2$totalTrials[subj+99] <- length(cogData[[subj]]$Choice[cogData[[subj]]$Handling==2 & cogData[[subj]]$Reward==5])
    allProp2$totalTrials[subj+110] <- length(cogData[[subj]]$Choice[cogData[[subj]]$Handling==2 & cogData[[subj]]$Reward==10])
    allProp2$totalTrials[subj+121] <- length(cogData[[subj]]$Choice[cogData[[subj]]$Handling==2 & cogData[[subj]]$Reward==25])

    # handling of 10s
    allProp2$propComplete[subj+132] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==10 & cogData[[subj]]$Reward==5])
    allProp2$propComplete[subj+143] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==10 & cogData[[subj]]$Reward==10])
    allProp2$propComplete[subj+154] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==10 & cogData[[subj]]$Reward==25])
    allProp2$totalTrials[subj+132] <- length(cogData[[subj]]$Choice[cogData[[subj]]$Handling==10 & cogData[[subj]]$Reward==5])
    allProp2$totalTrials[subj+143] <- length(cogData[[subj]]$Choice[cogData[[subj]]$Handling==10 & cogData[[subj]]$Reward==10])
    allProp2$totalTrials[subj+154] <- length(cogData[[subj]]$Choice[cogData[[subj]]$Handling==10 & cogData[[subj]]$Reward==25])

    # handling of 14s
    allProp2$propComplete[subj+165] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==14 & cogData[[subj]]$Reward==5])
    allProp2$propComplete[subj+176] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==14 & cogData[[subj]]$Reward==10])
    allProp2$propComplete[subj+187] <- mean(cogData[[subj]]$Choice[cogData[[subj]]$Handling==14 & cogData[[subj]]$Reward==25])
    allProp2$totalTrials[subj+165] <- length(cogData[[subj]]$Choice[cogData[[subj]]$Handling==14 & cogData[[subj]]$Reward==5])
    allProp2$totalTrials[subj+176] <- length(cogData[[subj]]$Choice[cogData[[subj]]$Handling==14 & cogData[[subj]]$Reward==10])
    allProp2$totalTrials[subj+187] <- length(cogData[[subj]]$Choice[cogData[[subj]]$Handling==14 & cogData[[subj]]$Reward==25])

}

# total accepted (not proportion), needed for binomial test
allProp2$totalComplete <- allProp2$propComplete * allProp2$totalTrials

# add optimal vector and difference from optimal
two <- rep(1,33)
ten <- c(rep(0,11),rep(1,22))
tfive <- c(rep(0,22),rep(1,11))
allProp2$Optimal <- rep(c(two,ten,tfive),2)
allProp2$Deviation <- allProp2$Optimal - allProp2$propComplete

# sort by subject ID
#allProp2 <- allProp2[order(allProp2$SubjID),]

# actual proportions and standard errors, and the optimal summary for plotting
trueProp <- aggregate(allProp2$propComplete,
            by = list(allProp2$Reward, allProp2$Handling, allProp2$Group),
            FUN = 'mean')

trueSD <- aggregate(allProp2$propComplete,
          by = list(allProp2$Reward, allProp2$Handling, allProp2$Group),
          FUN = 'sd')
colnames(trueProp) <- c("Reward","Handling","Group","propComplete")
trueProp$SE <- (trueSD$x)/(sqrt(nSubs))

optimalProp <- aggregate(allProp2$Optimal,
            by = list(allProp2$Reward, allProp2$Handling, allProp2$Group),
            FUN = 'mean')$x

```


```{r,echo=FALSE,fig.align="center",fig.width=9,fig.height=5, fig.cap="Proportion completed and standard errors for each combination of handling and reward amounts, for each group separately"}
# trying another plot, for ANOVA (equivalent to the poster one)
# make margins big enough
# par(mar=c(6.1,4.1,5.1,2.1))
# lthick <- 2.5

# putting this here for now, so it doesn't mess up anything above. Once the plot decision is made, clean up
# trueProp <- trueProp[order(trueProp$Group,decreasing = T),]
# trueProp$SE[11:12] <- 0.001 # because otherwise it won't plot the arrows (SE = 0)
# trueProp$SE[15] <- 0.001

# # actual plot
# plot(1:3, trueProp$propComplete[1:3],
#      main = "Proportion Completed Per Handling/Reward Combination \n Mean and SE for Each Group",
#      xlab = "",
#      ylab = "Proportion Completed",
#      xlim = c(1,21), 
#      ylim = c(0,1),
#      type = "l",
#      xaxt = "n",
#      col = cols[1],
#      lwd = lthick + 2,
#      bty = "n")
# points(1:3, 
#      trueProp$propComplete[1:3],
#      col = cols[1],
#      pch = 16,
#      cex = lthick - 1,
#      bty = "n")
# arrows(1:3, trueProp$propComplete[1:3] - trueProp$SE[1:3],
#        1:3, trueProp$propComplete[1:3] + trueProp$SE[1:3], 
#        length=0.05, 
#        angle=90, 
#        code=3,
#        col = cols[1],
#        lwd = lthick + 2)
# 
# ranges <- array(4:18,dim = c(3,5)) # for selecting and placin on x-axis
# for (i in 1:5){
#   
#     # choose a different color for each group
#     color = cols[1]
#     if (i > 2){color = cols[2]}
#     
#     # grab the relevant measure indexes
#     indx <- ranges[,i]
#     
#     # SEs
#     SEup <- trueProp$propComplete[indx] + trueProp$SE[indx]
#     SEdown <- trueProp$propComplete[indx] - trueProp$SE[indx]
#     
#     # plot
#     lines(indx,
#           trueProp$propComplete[indx],
#           col = color,
#           lwd = lthick + 2)
#     points(indx, 
#            trueProp$propComplete[indx],
#            col = color,
#            cex = lthick - 1,
#            pch = 16,
#            bty = "n")      
#     arrows(indx, SEdown,
#            indx, SEup, 
#            length=0.05, 
#            angle=90, 
#            code=3,
#            col = color,
#            lwd = lthick + 2)    
# }
# 
# # legend
# legend("topright",
#        c("Wait","Effort"), 
#        fill = c(cols[1],cols[2]),
#        cex = 0.8)
# 
# # x axis
# axis(1,at = 1:9, rep(c(5,10,25),3))
# axis(1,at = 10:18, rep(c(5,10,25),3))
# axis(1,at = c(2,5,8), c(2,10,14), line = 3)
# axis(1,at = c(11,14,17), c(2,10,14), line = 3)
# mtext("Reward",1,line=1,at=-0.5)
# mtext("Handling",1,line=3,at=-0.5)
```

``` {r,echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
## REPEATED MEASURES VERSION
# I think I like this the most
prop.aov <- with(allProp2, aov(propComplete ~ factor(Group) * Handling * Reward + 
                                 Error(SubjID / (Handling * Reward))))
```

This plot portrays some important features: the first hypothesis was supported, as we can see that participants accepted fewer trials for smaller rewards and longer handling times. On the other hand, and perhaps expectedly, we can see that the effort group accepted more trials than the wait group in all cases. This is contrary to the second hypothesis.
In order to analyze these differences, I ran a 2-way repeated measures ANOVA to see how participants in each group performed at each combination of handling times and rewards. Since all three factors are relevant to the question, and since at this point I was interested in mean differences, I did not perform model selection. The table below shows the results from the analysis from variance. Main effects were found for group (F(1,186) = 5.36, p < 0.05), handling time (F(1,186) = 30.045, p < 0.0001), and reward amount (F(1,186) = 81.08, p < 0.0001). A handling by reward interaction was also found (F(1,186) = 9.42, p < 0.005), indicating that participant tendencies to accept higher rewards were reduced by longer handling times. This was somewhat expected, since the optimal behavior was to forage selectively for higher rewards as the handling time increased, and participants were fairly sensible to this structure.
This analysis opens the door to another important consideration: the optimality of each group's behavior. Since the reward-maximizing choices can be calculated, I overlayed them on the plot from figure 5 (in grey).


```{r,echo=FALSE,fig.align="center",fig.width=9,fig.height=5, fig.cap="Same as figure 6, but with the optimal choices plotted in grey. As an example, the optimal behavior for a handling time of 10 seconds is to accept 10 and 25 points, but not 5 points."}
# trying another plot, for ANOVA (equivalent to the poster one)
# make margins big enough
par(mar=c(6.1,4.1,5.1,2.1))
lthick <- 2.5

# putting this here for now, so it doesn't mess up anything above. Once the plot decision is made, clean up
trueProp <- trueProp[order(trueProp$Group,decreasing = T),]
trueProp$SE[11:12] <- 0.001 # because otherwise it won't plot the arrows (SE = 0)
trueProp$SE[15] <- 0.001

# actual plot
plot(1:3, trueProp$propComplete[1:3],
     #main = "Proportion Completed Per Handling/Reward Combination \n Mean and SE for Each Group",
     xlab = "",
     ylab = "Proportion Completed",
     xlim = c(1,21), 
     ylim = c(0,1),
     type = "l",
     xaxt = "n",
     col = cols[1],
     lwd = lthick + 2,
     bty = "n")
points(1:3, 
     trueProp$propComplete[1:3],
     col = cols[1],
     pch = 16,
     cex = lthick - 1,
     bty = "n")
arrows(1:3, trueProp$propComplete[1:3] - trueProp$SE[1:3],
       1:3, trueProp$propComplete[1:3] + trueProp$SE[1:3], 
       length=0.05, 
       angle=90, 
       code=3,
       col = cols[1],
       lwd = lthick + 2)
# plot the optimal choices
lines(1:3,optimalProp[1:3], col = "grey48",lty=3, lwd = lthick + 2)
points(1:3,optimalProp[1:3], col = "grey48",pch=17, cex = lthick - 1)

ranges <- array(4:18,dim = c(3,5)) # for selecting and placin on x-axis
for (i in 1:5){
  
    # choose a different color for each group
    color = cols[1]
    if (i > 2){color = cols[2]}
    
    # grab the relevant measure indexes
    indx <- ranges[,i]
    
    # SEs
    SEup <- trueProp$propComplete[indx] + trueProp$SE[indx]
    SEdown <- trueProp$propComplete[indx] - trueProp$SE[indx]
    
    # plot
    lines(indx,
          trueProp$propComplete[indx],
          col = color,
          lwd = lthick + 2)
    points(indx, 
           trueProp$propComplete[indx],
           col = color,
           cex = lthick - 1,
           pch = 16,
           bty = "n")      
    arrows(indx, SEdown,
           indx, SEup, 
           length=0.05, 
           angle=90, 
           code=3,
           col = color,
           lwd = lthick + 2)    
    lines(indx,optimalProp[indx], col = "grey48", lty=3, lwd = lthick + 2)
    points(indx,optimalProp[indx], col = "grey48", pch=17, cex = lthick - 1)
}

# legend
legend("topright",
       c("Wait","Effort","Optimal"), 
       fill = c(cols[1],cols[2],"grey48"),
       cex = 0.8)

# x axis
axis(1,at = 1:9, rep(c(5,10,25),3))
axis(1,at = 10:18, rep(c(5,10,25),3))
axis(1,at = c(2,5,8), c(2,10,14), line = 3)
axis(1,at = c(11,14,17), c(2,10,14), line = 3)
mtext("Reward",1,line=1,at=-0.5)
mtext("Handling",1,line=3,at=-0.5)

title(main = "Proportion Completed Per Handling/Reward Combination \n Mean and SE for Each Group",
      line = 3)
axis(3,at = 1:9, c('*','*','','*','*','','*','*','*'), tick = F, cex = 1.2)
axis(3,at = 10:18, c('*','','','*','*','','*','*',''), tick = F, cex = 1.2)
```

The first thing to note is that both groups show some level of sensitivity to the optimal behavior for each handling time. While there are no perfect matches, each group's choice behavior at each handling time was more similar to its corresponding optimal behavior than to any other ones. This suggests that participants were able to integrate the parameters of the task in a sensible way. Second, the effort group was closer to optimality for the 2 second handling time, while the wait group better approximated optimality at 14 seconds. In addition to these insights, we can ask quantitatively which group deviated the most from this ideal.

#### Deviation from optimality
The following analysis relies on a set of assumptions. First, I assumed that participants knew what the overall optimal acceptance was (partially supported by post-experimental interviews). If we consider optimal acceptances to be the null distribution against which the optimality of acceptance rates can be evaluated, then we could use the probability of finding the observed acceptance rate using a binomial distribution for each reward/handling combination. Second, since probabilities of 1 and 0 for the binomial distribution would yield zeros, I assumed that absolute certainty was impossible, and thus changed 1 to 0.999 and 0 to 0.001. This way, I calculated the probability of seeing a proportion of acceptances given that the null probability was either 0.999 or 0.001. Importantly, since I was interested in the group as a whole, I pooled the total number of acceptances and total trials for across participants in each group (organized by reward and handling time combinations). The resulting probabilities were transformed into p-values and adjusted for multiple comparisons using the Benjamini & Yekutieli method (FDR), and evaluated at an alpha of 0.05.

The asterisks on figure 9 mark where there were significant differences from optimality. This analysis confirms what was visually evident (given the SEM bars). Further it shows to some extent that the wait group was overall less optimal than the wait group. However, given the shape of acceptance rates and their actual distances from optimality, I would still argue that the wait group was more optimal at longer handling times, while the effort group matched optimality more closely at shorter ones.

``` {r,echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# In the works: the idea here is to estimate the probability of having observed a given amount of completed trials, assuming that the null is the reward maximizing behavior. That means: what is the probability of having seen this acceptance rate, assuming the optimal was to either accept or not (1 or 0)? If subjects knew the optimal behavior, then very low probabilities indicate that they were affected by the cost significantly. We cannot really assume that participants know this, but post-experiment interviews indicate that many of them had a clear idea of what the approxiimate optimal behavior was, even if they violated the rule during the experiment.

# might use https://www.rdocumentation.org/packages/pracma for eps()

TC <- aggregate(allProp2$totalComplete, by = list(allProp2$Reward, allProp2$Handling, allProp2$Group), FUN = 'sum')
TC <- TC[order(TC$Group.3,decreasing = T),]
TT <- aggregate(allProp2$totalTrials, by = list(allProp2$Reward, allProp2$Handling, allProp2$Group), FUN = 'sum')
TT <- TT[order(TT$Group.3,decreasing = T),]
OPT <- aggregate(allProp2$Optimal, by = list(allProp2$Reward, allProp2$Handling, allProp2$Group), FUN = 'mean')
OPT <- OPT[order(OPT$Group.3,decreasing = T),]

trueProp$totalComplete <- TC$x
trueProp$totalTrials <- TT$x
trueProp$Optimal <- OPT$x
trueProp$Optimal[trueProp$Optimal==1] <- 0.99 # consider just getting eps(), like in matlab
trueProp$Optimal[trueProp$Optimal==0] <- 0.01

for (i in 1:18){

    # parameters of interest for the binomial test
    completed <- trueProp$totalComplete[i]
    total <- trueProp$totalTrials[i]
    prob <- trueProp$Optimal[i]
  
    # estimate the probability of having observed the acceptance rate, assuming reward-maximizing behavior
    trueProp$binomProb[i] <- round(pbinom(completed,total,prob), digits = 10)

}

# this is important. Since each test is comparing against either the 1 or 0 side, the pvalue calculated from the probability needs to be signed appropriately. For optimal acceptances of 1, the probability given by pbinom() is the pvalue, but for 0 it is 1 - pbinom().
trueProp$pval <- abs(rep(c(0,0,0,1,0,0,1,1,0),2) - trueProp$binomProb)

# do multiple-test corrections
trueProp$adjPval <- round(p.adjust(trueProp$pval, "BY"), digits = 4)
```

#### Before/After break strategy changes
**Another question that is worth prusuing is whether participants changed their behavior before and after the break. This is particularly important because they saw the same handling and travel time combination blocks (in the same order) before and after this break. To evaluate this, I performed per-subject paired permutation tests (paired by handling-reward combination), and extracted the p-values from the permuted null distributions. After correction for multiple comparisons (using false discovery rate), there were no significant changes before and after the break. These results suggest that participants were consistent in their acceptance behavior, and were following determinate strategies. This is in addition to the response time evidence examined above.**

```{r,echo=FALSE,fig.align="center",fig.width=6,fig.height=4}

# Before/After per subject diffs
# wilcoxon signed rank won't work because the vectors are likely to be of different lengths
# prop.test works as test of proportions (chi-square ish, but evaluated with a z-distribution?)
# just say it's a chi-squared test of proportions per subject, and that only one subject had a different proportion completed before and after the break. All other null could not be rejected.
# Problem: this computes the proportion without accounting for handling or reward amounts. Problem is, there aren't enough trials per combination to really account for every bit. Could be interesting to try, though.

cogABPval <- numeric()
waitABPval <- numeric()

for (i in 1:11){

      # wait
      tempPropB <- aggregate(waitBefore[[i]]$Choice, by = list(waitBefore[[i]]$Handling, waitBefore[[i]]$Reward), FUN = 'mean')
      tempPropA <- aggregate(waitAfter[[i]]$Choice, by = list(waitAfter[[i]]$Handling, waitAfter[[i]]$Reward), FUN = 'mean')
      tempPerm <- permute(tempPropB$x,tempPropA$x,nPerms = 5000, paired = TRUE)
      waitABPval[i] <- tempPerm$Pval
      
      # effort
      if (i == 9) { cogABPval[i] = 1} else {
      tempPropB <- aggregate(cogBefore[[i]]$Choice, by = list(cogBefore[[i]]$Handling, cogBefore[[i]]$Reward), FUN = 'mean')
      tempPropA <- aggregate(cogAfter[[i]]$Choice, by = list(cogAfter[[i]]$Handling, cogAfter[[i]]$Reward), FUN = 'mean')
      tempPerm <- permute(tempPropB$x,tempPropA$x,nPerms = 5000, paired = TRUE)
      cogABPval[i] <- tempPerm$Pval}
      
}

# Before after group. Don't worry about plotting
# paired t-tests also show that there are no differences before and after the break, indicating consistency of strategies
abWait <- t.test(waitBeforeAll$x,waitAfterAll$x,paired = T)
abCog <- t.test(cogBeforeAll$x,cogAfterAll$x,paired = T)
```

```{r,echo=FALSE,fig.align="center",fig.width=6,fig.height=4, fig.cap="P-values for the pairwise permutation of before and after break performance, each plot showing each group"}
# ## plotting
# plot(1:11,
#      waitABPval,
#      main = "P-values for permuted pairwise comparisons before and after break",
#      xlab = "Subjects",
#      ylab = "P values",
#      ylim = c(0,1),
#      xlim = c(0,22),
#      pch = 16,
#      col = cols[1])
# points(12:22,
#        cogABPval,
#        pch = 16,
#        col = cols[2])
# # legend
# legend("right",
#        c("Wait","Effort"), 
#        fill = c(cols[1],cols[2]),
#        cex = 0.8)
```

#### Logistic regression 
The analyses above were meant to get a sense of performance differences between groups while assuming that all experimental parameters were relevant, but I was also interested in estimating the importance of each factor in predicting choice. Such predictions can often be done with linear models. However, due to the non-Gaussian nature of the proportion of choices, as well as the predictors, I decided to run a logistic regression to predict the proportion of trials per subject, using group, handling time, and reward amount as predictors *(using a quasi-binomial GLM)*. I used Akaike Information Criterion (AIC) to identify which combinations of predictors significantly reduced the -2 log-likelihood while penalizing the addition of predictors. Figure 9 shows the results.

```{r,echo=FALSE,fig.align="center",fig.width=9,fig.height=6}
# logistic regression tests ($deviance for LRTs, if desired. AIC should be fine though)
logisBase <- glm(propComplete ~ 1, family = "binomial", data = allProp2) # intercept only
logisGroup <- glm(propComplete ~ factor(Group), family = "binomial", data = allProp2) 
logisHand <- glm(propComplete ~ factor(Group) + Handling, family = "binomial", data = allProp2)
logisRwd <- glm(propComplete ~ factor(Group) + Reward, family = "binomial", data = allProp2)
logisComb <- glm(propComplete ~ factor(Group) + Handling + Reward, family = "binomial", data = allProp2)
logisInt <- glm(propComplete ~ factor(Group) + Handling * Reward, family = "binomial", data = allProp2)
logisIntAll <- glm(propComplete ~ factor(Group) * Handling * Reward, family = "binomial", data = allProp2)

# Quasibinomial GLM versions
logisBase <- glm(propComplete ~ 1, family = "quasibinomial", data = allProp2) # intercept only
logisGroup <- glm(propComplete ~ factor(Group), family = "quasibinomial", data = allProp2) 
logisHand <- glm(propComplete ~ factor(Group) + Handling, family = "quasibinomial", data = allProp2)
logisRwd <- glm(propComplete ~ factor(Group) + Reward, family = "quasibinomial", data = allProp2)
logisComb <- glm(propComplete ~ factor(Group) + Handling + Reward, family = "quasibinomial", data = allProp2)
logisInt <- glm(propComplete ~ factor(Group) + Handling * Reward, family = "quasibinomial", data = allProp2)
logisIntAll <- glm(propComplete ~ factor(Group) * Handling * Reward, family = "quasibinomial", data = allProp2)

# Quasibinomial GLM versions (with proper proportion setup)
totalQuit <- allProp2$totalTrials - allProp2$totalComplete
testBinom <- cbind(allProp2$totalComplete,totalQuit)
logisBase <- glm(testBinom ~ 1, family = "quasibinomial", data = allProp2) # intercept only
logisGroup <- glm(testBinom ~ factor(Group), family = "quasibinomial", data = allProp2) 
logisHand <- glm(testBinom ~ factor(Group) + Handling, family = "quasibinomial", data = allProp2)
logisRwd <- glm(testBinom ~ factor(Group) + Reward, family = "quasibinomial", data = allProp2)
logisComb <- glm(testBinom ~ factor(Group) + Handling + Reward, family = "quasibinomial", data = allProp2)
logisInt <- glm(testBinom ~ factor(Group) + Handling * Reward, family = "quasibinomial", data = allProp2)
logisIntAll <- glm(testBinom ~ factor(Group) * Handling * Reward, family = "quasibinomial", data = allProp2)

# Model selection (with interactions is better)---------- 
## NOTES ON RESULTS

# According to the logistic fit, there are main effects for all predictors, but no interactions. It also says that the overall odds for the wait group to accept versus the effort is around 16%. Similarly, increasing the handling time results in a decrease in the multiplicative odds of ~25%. Increments in reward increase the odds of acceptance by ~40%.

# Results: using handling and rewards as factors decreased the likelihood, so I removed those models 
# For the logistic one, the last meaningful decrease in likelihood happened with logisComb, which also goes against the ANOVA (since no-interactions are significant). I think this model will work best.
# I also double checked this by looking at the progressive deviances. The difference in model deviance (or -2LL of the model) for each pair of models is evaluated using a chi-squared distribution (with df = diff in params). This difference in logs is analogous to a logged division of the likelihoods, which is an LRT (or R-squared), with the null model going in the denominator. Using this in a forward-selection way, I found again that the last model to significantly improve the fit was the logisComb. So, using the null deviance and AIC yielded the same conclusion.
aicLogis <- AIC(logisBase,logisGroup,logisHand,logisRwd,logisComb,logisInt,logisIntAll)
anovaLogis <- anova(logisBase,logisGroup,logisHand,logisRwd,logisComb,logisInt,logisIntAll,test = "Chisq")

# effect size
logisComb$Rsq <- 1 - (logisComb$deviance[1]/200.239) # denominator is the null deviance
```


``` {r,echo=FALSE,fig.align="center",fig.width=4,fig.height=3, fig.cap="AIC values for each model. The model with all main effects was chosen. G = group, H = handling, R = Reward"}
par(mar=c(5.1,2.1,2.1,2.1))    
# plot the deviance for all models
# ver 3
modelLabels <- c('Intercept','G Only','H Only','R Only','G + H + R','G + H * R','G * H * R')
plot(anovaLogis$`Resid. Dev`,
     main = "Residual Deviance for All Models",
     xlab = "",
     ylab = "Res. Deviance",
     pch = 16,
     xaxt = "n")
axis(1,at = 1:7, labels = F)
text(1:7, 
     par("usr")[3] - 22,
     labels = modelLabels, 
     srt = 45, 
     pos = 1, 
     xpd = TRUE)     
```

This showed that all experimental parameters were relevant to the prediction, using either all main effects (AIC = 112) or a handling by reward interaction (AIC = 111). To choose between these two models I used forward selection, comparing each model's deviance with the next. I evaluated the difference in deviance against a ${\chi}^2$ distribution (with df equal to the difference in parameters). This showed that the model with group, handling, and reward as main effects (D = 70.05) was the last to make significant distributions to the prediction (D = 68.8 for the model using a handling by reward interaction). Since this model is also more parsimonious than the one with interactions, I chose it. 
The resulting logistic model showed significant main effect of group as a factor (wait, beta = -1.9, SE = 0.73, p < 0.0001), indicating that the odds of accepting a trial for the wait group are 85% lower than for the effort group (odds are equal to exp(-1.9) = 0.149). I also found a significant main effect of handling time (beta = -0.33, SE = 0.059, p < 0.0001), suggesting that as the handling time increases, the odds of acceptance decrease by 29% (odds = 0.71). Finally, reward amounts also played a significant role in predicting choice (beta = 0.35, SE = 0.069, p < 0.0001), showing that as rewards increase, the odds of acceptance are boosted by 41% (odds = 1.41). 
In order to visualize these results (figure 10), I computed the probability of acceptance for each combination of factors, given by the following equation (with group being either 0 for effort, and 1 for wait):
$${\frac{e^{(\beta_0 + Group*\beta_1 + Handling*\beta_2+Reward*\beta_3)}}{1 + e^{(\beta_0 + Group*\beta_1 + Handling*\beta_2+Reward*\beta_3)}}}$$  
As expected, as the handling time increased, higher rewards were needed to increase the probability of acceptance, regardless of group. Importantly, we can see that the wait group required higher rewards than the effort group to accept a trial regardless of handling time. In the peresent context, the significance of these results is two-fold: First, participants acted as predicted by the first hypothesis, integrating all experimental parameters adequately. Second, they estimated the cost of time to be higher than the effort group did. Figure 10 (right) also shows the residuals from the model, which suggested an overall good fit.

``` {r,echo=FALSE,fig.align="center",fig.width=6,fig.height=4, fig.cap="Left: Sigmoidal functions depicting the probability of accepting a trial for each group, depicted for each handling-reward combination. Right: Residuals for the chosen model."}
par(mfrow=c(1,2))
## Plotting the winning model--------------------
rewards <- seq(1,25,length = 1000)
# wait
plotLogis(coeffs = logisComb$coefficients,
          handling = 2, 
          reward = rewards, 
          plotType = plot, 
          color = cols[1],
          lType = 6)

plotLogis(coeffs = logisComb$coefficients,
          handling = 10, 
          reward = rewards, 
          color = cols[1],
          lType = 3)

plotLogis(coeffs = logisComb$coefficients,
          handling = 14, 
          reward = rewards, 
          color = cols[1])


# effort
plotLogis(coeffs = logisComb$coefficients,
          group = "Effort",
          handling = 2, 
          reward = rewards, 
          color = cols[2],
          lType = 6)

plotLogis(coeffs = logisComb$coefficients,
          group = "Effort",
          handling = 10, 
          reward = rewards, 
          color = cols[2],
          lType = 3)

plotLogis(coeffs = logisComb$coefficients,
          group = "Effort",
          handling = 14, 
          reward = rewards, 
          color = cols[2])

# title
title("Probabilities for All Handling-Reward Combinations \n for Each Group")

# x axis
xticks <- c(min(rewards[rewards>5]),min(rewards[rewards>10]),25)
axis(1,at = xticks,c(5,10,25))

# legend
legend("bottomright",
       c("Wait: 2s","Wait: 10s","Wait: 14s","Effort: 2s","Effort: 10s","Effort: 14s"), 
       fill = c(cols[1],cols[1],cols[1],cols[2],cols[2],cols[2]),
       lty = rep(c(6,3,1),2),
       cex = 1)


# # actual proportions (maybe, maybe not)
# points(c(5,10,25),trueProp$propComplete[1:3], col = cols[1], pch=17) # wait
# points(c(5,10,25),trueProp$propComplete[4:6], col = cols[1], pch=17)
# points(c(5,10,25),trueProp$propComplete[7:9], col = cols[1], pch=17)
# 
# points(c(5,10,25),trueProp$propComplete[10:12], col = cols[2], pch=17) # effort
# points(c(5,10,25),trueProp$propComplete[13:15], col = cols[2], pch=17)
# points(c(5,10,25),trueProp$propComplete[16:18], col = cols[2], pch=17)


# and the residuals--------------------------
plot(logisComb$residuals,
     main = "Residuals for the Logistic Regression",
     xlab = "trial",
     ylab = "Residuals")
# -------------------------------------------
```

#### Sequential effects
Finally, I was curious whether there were sequential effects. One could think that quitting too many times consecutively would increase the probability of accepting any reward (in fear of skipping too much and lowering the amount of points they get overall). I checked for this possibility by creating a vector with the number of consecutive quits that happened before each trial, and used it as a predictor in a binomial GLM. This auto-regressive process was performed on each subject separately, using choice (a set of Bernoulli random variables, quit = 0, accept = 1) as the dependent variable and the quit-run vector as a predictor. 
While sequential quits were significantly predictive of choice for a couple of subjects (p < 0.05), the effect sizes were minimal. Deviances barely changed, if at all, and none of them were significantly reduced the null deviance (evaluated under a ${\chi}^2$ distribution). This indicates that previous trials had little bearing on choices. Instead, as seen above, participants focused mainly on the experimental parameters.

``` {r,echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# auto-regressive process?  
# for that, an lm() would have to contain choice data and its lags at -1, -2, -x. Let's see...
# model <- lm(choice ~ (choice lag - 1) + (choice lag - 2) + ... (choice lag - x))
# better to use (per subject) arima(waitData[[1]]$Choice, order = c(2,0,0)) # the last zeros indicate no moving average process.
# you get AIC and likelihoods with these, and thus can run model selection with different autoregressive lags.
# also, the coefficients and s.e. work just like with linear models, so that beta/se gives a z statistic that can be evaluated against the null of 0.
# plot with pacf or acf (they looked the same to me: but acf starts with autocorrelation of 1, which is always 1)

# BIG NOTE: This is for ys with gaussian noise (or continuous), for discrete data we should just use the GLM for our purposes. 
# However, Dan said that the GLM approach should be fine, since it's imposing the betas of each lag onto each other. Should try using individual predictors for each lag, but meh...

# store the models, so each subject's can be summarized
quitModelsW <- list()
quitModelsC <- list()

for (i in 1:nSubs){
  
    # subject data (y)
    yW <- waitData[[i]]$Choice
    yC <- cogData[[i]]$Choice
    
    # lag data (x predictors)
    xW <- seqQuits(yW)
    xC <- seqQuits(yC)
    
    # models
    quitModelsW[[i]] <- glm(yW ~ xW, family = "binomial")
    quitModelsC[[i]] <- glm(yC ~ xC, family = "binomial")
    
}


  # Result notes: while a couple of participants had significant p-values, the effect sizes are tiny. Deviance barely moves. That means that the odds of acceptance are independent of how many trials they quit before the current trial. No reason to add it as a covariante in the linear models, but it's worth talking about it.
  
```
### Conclusion

In this project I investigated the possible value-discounting effects of cognitive effort and passive delay. I utilized data from a modified foraging task, in which participants decided whether to accept or skip a rewarding trial based on the richness of the environment. The original hypothesis stated that having to perform cognitive tasks during the delay would be extra aversive, making participants estimate the opportunity cost of time as higher than it truly is. However, the present findings demonstrate that effort might not always discount the value of a reward, although it nonetheless affects a subject’s ability to evaluate rewards.
My results suggest that performing cognitive tasks while waiting for a reward facilitates optimality in short delays, whereas passive waiting promotes it during long intervals. In general, participants that performed tasks had a marked tendency to over-accept rewards, even when the choice was detrimental to their overall earning rate. This is contrary to our original hypothesis, and perhaps counterintuitive in the presence of previous literature (Kool et al., 2010). Although the present data does not provide further insights into why this might be, there are some ideas that might account for this behavior. One reason could be that participants are motivated to complete trials due to their accurate performance in the cognitive tasks. Such effects have been reported before under the name of ‘learned industriousness’ (Eisenberger & Cameron, 1996). In addition, the tasks themselves might have proven entertaining, and thus intrinsically rewarding. Such an effect could also contribute to a misinterpretation of the trial-wise elapsed time. In other words, our foraging experiment might have accidentally captured the notion that time goes by faster when you are busy. Notably, supervised training and self-reported strategies argue against the possibility of the effort group being unable to act optimally due to task distractions. Before/after break performance, response time, and optimality analyses also help us discard distraction effects.
Some limitations remain to be addressed. For example, the fact that participants immediately experienced each condition could limit the scope of our results. Discounting effects are known to vary in magnitude depending on the relative temporal position of the options, especially when they are far into the future (Green et al., 1994). Another shortcoming is the lack of monetary rewards. It is possible that participants were not motivated enough by earning points. Perhaps more importantly, comparing effortful and passive waiting between subjects does not give us a full picture of their relative costs. In the future, this task will give each subject the option to forage through both conditions. In addition, a measure of physical activity (e.g. hand grip) will be included in order to further tease apart the relative cost of time and effort (the study’s original plan). First, however, I will replicate the current study, this time using monetary rewards. While the current results look promising, it is important to remember that this was a minimally-sampled pilot study, which limits its power. Perhaps more importantly, even with sufficient power and low p-values, the current study still risks a high false positive rate if the prior odds of finding this difference are against the alternative hypothesis (Benjamin et al., 2017; Nuzzo, 2014). This is a complicated issue to solve at the moment, especially since the present findings go against a literature that expects cognitive effort to provoke fewer responses than passive delays. Despite these limitations, the current behavioral task was geared to provide a baseline of the cost of each discounting type, thus giving us a reference that can guide future within-subject manipulations.
The ongoing study has broad implications for understanding cost. First, discriminating between cost features can have fruitful clinical applications, helping us better understand motivational dysfunctions associated with mood disorders (Westbrook & Braver, 2015). Second, by incorporating both physical and cognitive effort in the context of time for the first time, it provides an opportunity to determine the specific influence of each type of effort in discounting value. This result can directly benefit fields such as economics and learning theory, particularly by clarifying the extent to which physical and mental effort can be interchangeably used in studies of valuation.

### References
Ainslie, G. (1975). Specious Reward: A Behavioral Theory of Impulsiveness and Impulse Control.  Psych Bull,  82: 463-496.

Apps, M. A. J., Grima, L. L., Manohar, S., & Husain, M. (2015). The role of cognitive effort in subjective reward devaluation and risky decision-making.  Scientific Reports ,  5 , 16880. https://doi.org/10.1038/srep16880

Benjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E. J., Berk, R., ... & Cesarini, D. (2017). Redefine statistical significance. Nature Human Behaviour, 1.

Botvinick, M., Huffstetler, S., & McGuire, J. T. (2009). Effort discounting in human nucleus accumbens.  Behavioural Neuroscience ,  9 (1), 22. https://doi.org/10.3758/CABN.9.1.16.Effort

Constantino, S. M., & Daw, N. D. (2015). Learning the opportunity cost of time in a patch-foraging task.  Cognitive, Affective & Behavioral Neuroscience ,  15 (4), 837–53. https://doi.org/10.3758/s13415-015-0350-y

Eisenberger, R., & Cameron, J. (1996). Detrimental effects of reward. Reality or myth?  The American Psychologist ,  51 (11), 1153–1166. https://doi.org/10.1037/0003-066X.51.11.1153

Frederick, S., Loewenstein, G., & O ’donoghue, T. (2002). Time Discounting and Time Preference: A Critical Review.  Journal of Economic Literature Time Discounting Journal of Economic Literature ,  XL XL , 351–401.

Green, L., Fristoe, N., & Myerson, J. (1994). Temporal discounting and preference reversals in choice between delayed outcomes.  Psychonomic Bulletin & Review ,  1 (3), 383–389.

Hosking, J. G., Floresco, S. B., & Winstanley, C. A. (2014). Dopamine Antagonism Decreases Willingness to Expend Physical, But Not Cognitive, Effort: A Comparison of Two Rodent Cost&sol;Benefit Decision-Making Tasks.  Neuropsychopharmacology ,  40 (10). https://doi.org/10.1038/npp.2014.285

Kool, W., McGuire, J. T., Rosen, Z. B., & Botvinick, M. M. (2010). Decision Making and the Avoidance of Cogntive Demand.  Journal of Experimental Psychology: General ,  139 (4), 665–682. https://doi.org/10.1037/a0020198.Decision

Kurniawan, I. T., Guitart-Masip, M., & Dolan, R. J. (2011). Dopamine and effort-based decision making.  Frontiers in Neuroscience . https://doi.org/10.3389/fnins.2011.00081

Massar, S. A. A., Libedinsky, C., Weiyan, C., Huettel, S. A., & Chee, M. W. L. (2015). Separate and overlapping brain areas encode subjective value during delay and effort discounting. NeuroImage,  120(2015): 104-113. https://doi.org/10.1016/j.neuroimage.2015.06.080

Mcclure, S. M., Ericson, K. M., Laibson, D. I., Loewenstein, G., & Cohen, J. D. (2004). Behavioral/Systems/Cognitive Time Discounting for Primary Rewards.  Science,  306: 503-507. https://doi.org/10.1523/JNEUROSCI.4246-06.2007

Nuzzo, R. (2014). Statistical errors. Nature, 506(7487), 150.

Prévost, C., Pessiglione, M., Météreau, E., Cléry-Melin, M.-L., & Dreher, J.-C. (2010). Behavioral/Systems/Cognitive Separate Valuation Subsystems for Delay and Effort Decision Costs.  Journal of Neuroscience,  30(42):14080-14090. https://doi.org/10.1523/JNEUROSCI.2752-10.2010

Richardson, N. R., & Roberts, D. C. S. (1996). Progressive ratio schedules in drug self-administration studies in rats: A method to evaluate reinforcing efficacy.  Journal of Neuroscience Methods ,  66 (1), 1–11. https://doi.org/10.1016/0165-0270(95)00153-0

Schmidt, L., Lebreton, M., Cléry-Melin, M. L., Daunizeau, J., & Pessiglione, M. (2012). Neural mechanisms underlying motivation of mental versus physical effort.  PLoS Biology,  10: e1001266. https://doi.org/10.1371/journal.pbio.1001266

Schultz, W. (2015). Neuronal Reward and Decision Signals: From Theories to Data.  Physiol Rev , (95), 853–951.

Walton, M. E., Kennerley, S. W., Bannerman, D. M., Phillips, P. E. M., & Rushworth, M. F. S. (2006). Weighing up the benefits of work: Behavioral and neural analyses of effort-related decision making.  Neural Networks,  19(2006): 1302-1314. https://doi.org/10.1016/j.neunet.2006.03.005

Westbrook, A., & Braver, T. S. (2015). Cognitive effort: A neuroeconomic approach.  Cogn Affect Behav Neurosci,  15: 395-415. https://doi.org/10.3758/s13415-015-0334-y

Westbrook, A., Kester, D., & Braver, T. S. (2013). What Is the Subjective Cost of Cognitive Effort? Load, Trait, and Aging Effects Revealed by Economic Preference.  PLoS ONE,  8: e68210. https://doi.org/10.1371/journal.pone.0068210
