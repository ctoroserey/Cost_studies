---
title: Apparent preferences for cognitive effort fade when multiple forms of effort
  and delay are interleaved in a foraging environment
author: "Claudio Toro-Serey^1^2, Gary Kane^1^2, & Joseph T. McGuire^1^2"
header-includes:
- \usepackage{setspace}\doublespacing
- \usepackage[left]{lineno}
- \linenumbers
- \usepackage{float}
- \newcommand{\beginsupplement}{ \setcounter{table}{0} \renewcommand{\thetable}{S\arabic{table}}
  \setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}} }
- \usepackage{indentfirst}
output:
  pdf_document:
    highlight: haddock
    keep_tex: no
  word_document: #default
    reference_docx: Style_reference_draft.docx
  html_document:
    df_print: paged
csl: apa.csl
indent: true
bibliography: cost_paper.bib
---

```{r Global options, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, fig.pos = 'H')
```

```{r Setups, include = FALSE, warning = FALSE}
## libraries
library(tidyverse)
library(ggfortify)
library(knitr)
library(pander) 
library(nloptr)
library(pwr)
library(lme4)
#library(lmerTest)
library(gridExtra)
library(reshape2)
library(corrplot)
library(survival)
library(ggfortify)
library(patchwork)
library(data.table)

## figures at the end of the manuscropt?
figsEnd <- FALSE


## aesthetic options
lbls <- c("Wait","Cognitive","Physical","Easy") # between subj
colsBtw = c("#78AB05","#D9541A","deepskyblue4", "darkgoldenrod2") # plot colors (wait, effort)
colsWth <- c("#D9541A", "#78AB05", "dodgerblue4", "deepskyblue3")#"grey30", "grey70") # plot colors (wait, effort)
lthick = 2 # line thickness for plots

## functions
# parameter recovery function
testParams <- function(params = list(Gamma = 1), model = expr(params$Gamma * Handling), subjData, handling = c(2, 10, 14), offer = c(4, 8, 20), plot = F) {
  
  # this just computes choices made by a given parameterized model
  # or compares a putative model with participant choices
  #
  # params: a list of parameters that can be called by the model (besides behavioral observations from subjData)
  # model: must use expr() and call for extra parameters from params if needed (see default)
  # subjData: optional data to be compared
  # handling / offer: in case new experimental parameters should be tested (probably won't use)
  # plot: T/F whether to plot the comparison between modeled and actual behavior
  #
  
  # if no subject data was provided, use the default options and only output the recovered data
  if (missing(subjData)) {
    
    # compute
    df <- expand.grid(Handling = handling, Offer = offer) %>% 
      mutate(OC = eval(model),
             Choice = ifelse(Offer > OC, 1, 0))
    
    # plot
    if (plot) {
      
      ggplot(data = df, aes(interaction(Offer, Handling), Choice, color = Gamma)) +
        geom_line(aes(group = interaction(Handling, Gamma)), size = 1) +
        geom_point(size = 3) +
        labs(x = "Offer.Handling", y = "Proportion Accepted") +
        facet_wrap(vars(Gamma), ncol = 2) +
        theme(legend.key = element_blank(),
              panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.background = element_blank(),
              axis.line = element_line(colour = "black"),
              text = element_text(size = 16))
      
    }
    
    # otherwise give a comparison between recovered and observed choice for a subject's data + gamma
  } else {
    
    # compute
    df <- subjData %>% 
      mutate(OC = eval(model),
             Choice = ifelse(Offer > OC, 1, 0))
    
    # plot
    if (plot) {
      
      print(ggplot(data = df) +
        geom_line(aes(interaction(Offer, Handling), propAccept, group = Handling), size = 1, color = "darkgreen") +
        geom_line(aes(interaction(Offer, Handling), Choice, group = Handling), size = 1, color = "blue") +
        geom_point(aes(interaction(Offer, Handling), propAccept), size = 3, color = "darkgreen") +
        geom_point(aes(interaction(Offer, Handling), Choice), size = 3, color = "blue") +
        labs(x = "Offer.Handling", y = "Proportion Accepted") +
        #scale_color_manual()
        theme(legend.key = element_blank(),
              panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.background = element_blank(),
              axis.line = element_line(colour = "black"),
              text = element_text(size = 16)))
    }
    
  }
  
  return(df)
  
}
  
# get the number of continuous quits before current trial
seqQuits <- function(subjData = 1){
  
    # get the lag
    tempSeq <- sequence(rle(subjData)$lengths) # sequence
    tempSeq[subjData==1] <- 0 # not interested in completed runs, just quits
    tempSeq <- c(0,tempSeq) # lag it so it can be used as a t-1 regressor

    # return data so that the length is equivalent
    out <- tempSeq[1:length(subjData)]
    
    return(out)
    
}

# Cohen's D for 2 groups (could just use DesctTools...)
# for a more flexible approach, make the data input to be a list with entries for n groups, 
# then do length(list) for the number of groups. 
cohenD <- function(group1 = 1, group2 = 2){
  
      # means
      mean1 <- mean(group1, na.rm = T)
      mean2 <- mean(group2, na.rm = T)
      
      # variance
      var1 <- var(group1, na.rm = T)
      var2 <- var(group2, na.rm = T)
      
      # equation
      out <- (mean1 - mean2) / sqrt((var1 + var2)/2)
      
      return(out)
  
}

# generic permutation for 2 groups
permute <- function(group1 = 1, group2 = 2, statType = mean, nPerms = 5000, paired = FALSE, simple = FALSE){
  
    # prep data
    summaryPerm <- list()
    lOne <- length(group1)
    lTwo <- length(group2)
    bigSample <- c(group1,group2)  
    
    if (paired == FALSE) {
      
        
        for (i in 1:nPerms){
          
            # relabel samples
            tempBig <- sample(bigSample)
            tempOne <- tempBig[1:lOne]
            tempTwo <- tempBig[(lOne+1):length(bigSample)]
            
            # stats
            tempDiffs <- statType(tempOne,na.rm=T) - statType(tempTwo,na.rm=T)
            summaryPerm$jointDist[i] <- tempDiffs # statType(tempDiffs, na.rm = T) 
          
        }  
        
    } else {
      
        for (i in 1:nPerms){
          
            # shift labels in a pairwise fashion
            tempDiffs <- statType((-1)^rbinom(lOne,1,0.5) * (group1 - group2))
            summaryPerm$jointDist[i] <- tempDiffs
          
        }
      
    }
  
    # get the observed difference
    diffs <- statType(group1,na.rm=T) - statType(group2,na.rm=T)
    observedAbs <- abs(diffs) # maybe leave it as means here
    observed <- diffs
    summaryPerm$Pval <- 2 * (1 - ecdf(summaryPerm$jointDist)(observedAbs))
    if (length(unique(abs(summaryPerm$jointDist))) == 1) {summaryPerm$Pval <- 1} # if the difference is always the same, then p = 1
    summaryPerm$Observed <- observed
    
    # return the results
    if (simple) {
      
      return(summaryPerm$Pval)
      
    } else if (!simple) {
      
      return(summaryPerm)
      
    }
    
}

# Non-parametric Bootstrap for a single group
bootstrap <- function(group = 1, statType = mean, B = 5000){
  
    # prep param
    bootStats <- rep(0,B)
    
    # iterate
    for(b in 1:B){
      
      # wait group
      x <- sample(group,length(group),replace=T)  
      bootStats[b] <- statType(x,na.rm = T)
      
    }
    
    return(bootStats)
    
}

# basic power calculation for a one way ANOVA
powerCalc <- function(d = 1.2, Za = 1.96, Zb = 0.842){
  
  out <- 2*((Za + Zb)/d)^2 + (0.25*(Za^2))
  
  return(round(out))
  
}

# OC-specific computation of the negative log likelihood for model optimization
negLogLik <- function(params, choice, handling, reward) {
  gamma <- exp(params[2]) * handling
  model <- exp(params[1]) * (reward - gamma)
  p = 1 / (1 + exp(-model))
  p[p == 1] <- 0.999
  p[p == 0] <- 0.001
  tempChoice <- rep(NA, length(choice))
  tempChoice[choice == 1] <- log(p[choice == 1])
  tempChoice[choice == 0] <- log(1 - p[choice == 0]) # log of probability of choice 1 when choice 0 occurred
  negLL <- -sum(tempChoice) 
  return(negLL)
}

# optimize the OC model
optimizeOCModel <- function(Data, Algorithm = "NLOPT_LN_NEWUOA", simplify = F, optfun = negLogLik) {
  
  # Data: The participant's log
  # Algorithm: probably let be
  # optfun: an external function to minimize (in this case OC, separately defined as negloglik)
  
  # Prep data
  handling <- Data$Handling
  reward <- Data$Offer
  choice <- Data$Choice
  
  # Prep list of results to be returned
  out <- list()
  out$percentQuit <- mean(choice == 0) * 100
  out$percentAccept <- mean(choice == 1) * 100 
  miss <- (choice != 1) & (choice != 0)
  out$percentMiss <- mean(miss)  * 100
  choice <- choice[!miss]
  reward <- as.numeric(reward[!miss])
  handling <- as.numeric(handling[!miss])
  
  # Establish lower and upper bounds
  LB <- round(log((min(reward)/max(handling)) * 0.99), digits = 4)
  UB <- round(log((max(reward)/min(handling)) * 1.01), digits = 4) # in reality this should be the second largest, since no one would reject the highest val
  
  # Begin defining parameters
  # If choices are one-sided (i.e. all accepted), assign the upper or lower bound
  if ((sum(choice) == length(choice)) | (sum(choice) == 0)) {
    ifelse(sum(choice) == length(choice), out$Gamma <- exp(LB), out$Gamma <- exp(UB)) 
    out$temperature <- 1 # it was NA, but in theory a temperature of 1 also indicates noiseless estimates, and allows for easier fit computations
    out$LL <- 0
  } else {
    # Create a feasible region (search space)
    params <- as.matrix(expand.grid(temperature = c(-1, 1), gamma = seq(LB, UB, length = 3)))
    # Create a list to check the minimization of the negative log lik.
    info <- list()
    info$negLL <- Inf
    # Define the options to be used during optimization
    # Consider looking into other optimization algorithms and global minima
    opts <- list("algorithm" = Algorithm,
               "xtol_rel" = 1.0e-8)
    # Optimize the sOC over all possible combinations of starting points
    for (i in seq(nrow(params))) {
      tempInfo <- nloptr(x0 = params[i, ], 
                   eval_f = optfun, 
                   lb = c(log(0.001), LB),
                   ub = c(-log(0.001), UB),
                   opts = opts,
                   choice = choice, 
                   handling = handling,
                   reward = reward)
      if (tempInfo$objective < info$negLL) {
        #print("Minimized")
        info$negLL <- tempInfo$objective
        info$params <- tempInfo$solution
      }
    }
    out$Gamma <- exp(info$params[2])
    out$temperature <- exp(info$params[1])
    out$LL <- -info$negLL
  }
  
  # Summarize the outputs
  out$LL0 <- log(0.5) * length(choice)
  out$Rsquared <- 1 - (out$LL/out$LL0) # pseudo r-squred, quantifying the proportion of deviance reduction vs chance
  out$subjOC <- out$Gamma * handling
  out$probAccept <- 1 / (1 + exp(-(out$Scale*(reward - out$subjOC))))
  out$predicted <- reward > out$subjOC
  out$predicted[out$predicted == TRUE] <- 1
  out$percentPredicted <- mean(out$predicted == choice) 
  
  # adjust the probabilities in case of extreme gammas
  if (out$Gamma <= exp(LB)) {
    out$Gamma <- exp(LB) # temporary condition because Nlopt is not respecting the lower bound
    out$probAccept <- rep(1, length(choice))
  } else if (out$Gamma == exp(UB)) {
    out$probAccept <- rep(0, length(choice))
  } 
  
  # if doing this with dplyr::do(), return a simplified data.frame instead with the important parameters
  if (simplify) {
    out <- data.frame(out[c(seq(8), 12)])
  }   
  
  return(out)

}

# simpler form of optimization that allows inputting any model expression into a single function call
optimizeModel <- function(subjData, params, model, simplify = F) {
  # this function finds the combination of parameter values that minimizes the neg log likelihood of a logistic regression
  # used to rely on NLOPTR, but it's too cumbersome for the low-dimensional estimates I'm performing.
  #
  # subjData: a participant's log
  # params: a list of vectors. Each vector is the possible values a given parameter can take. Names in list must match model expression
  # model: using `expr()`, define the model (use <param>[[1]] for free parameters to be estimated. R limitation.). Ex: expr(temp[[1]] * (reward - (gamma[[1]] * handling)))
  
  
  # extract basic choice information
  handling <- subjData$Handling
  reward <- subjData$Offer
  choice <- subjData$Choice
  rt <- subjData$RT
  cost <- subjData$Cost
  trial <- subjData$TrialN
  rawChoice <- subjData$rawChoice
  
  # combine parameters into every possible combination
  params <- expand.grid(params)
  
  # Prep list of results to be returned
  out <- list()
  out$percentQuit <- mean(choice == 0) * 100
  out$percentAccept <- mean(choice == 1) * 100 
  
  LLs <- sapply(seq(nrow(params)), function(i) {
    
    # isolate the parameters for this iteration
    # and then store them as variables
    # FIGURE OUT HOW TO NOT STORE THEM AS DATAFRAMES
    pars <- params[i, ]
    lapply(seq_along(pars), function(variable) {assign(colnames(pars)[variable], pars[variable], envir = .GlobalEnv)})
    
    # estimate the probability of acceptance per the model
    p = 1 / (1 + exp(-eval(model)))
    p[p == 1] <- 0.999
    p[p == 0] <- 0.001
    
    # get the likelihood of the observations based on the model
    tempChoice <- rep(NA, length(choice))
    tempChoice[choice == 1] <- log(p[choice == 1])
    tempChoice[choice == 0] <- log(1 - p[choice == 0]) # log of probability of choice 1 when choice 0 occurred
    negLL <- -sum(tempChoice)
  })
  
  # chosen parameters  
  out$LL <- min(LLs)
  chosen_params <- params[which(LLs == out$LL), ]
  lapply(seq_along(chosen_params), function(variable) {assign(colnames(chosen_params)[variable], chosen_params[variable], envir = .GlobalEnv)})
  
  # Summarize the outputs
  out$LL0 <- -(log(0.5) * length(choice))
  out$Rsquared <- 1 - (out$LL / out$LL0) # pseudo r-squred, quantifying the proportion of deviance reduction vs chance
  out$probAccept <- 1 / (1 + exp(-eval(model)))
  out$Params <- chosen_params
  #out$predicted <- reward > out$subjOC
  #out$predicted[out$predicted == TRUE] <- 1
  #out$percentPredicted <- mean(out$predicted == choice) 
  
  # if doing this with dplyr::do(), return a simplified data.frame instead with the important parameters
  if (simplify) {
    out <- round(data.frame(out[-6]), digits = 2)
    colnames(out) <- c("percentQuit",
                       "percentAccept",
                       "LL",
                       "LL0",
                       "Rsq",
                       colnames(chosen_params))
  }
  
  return(out)
}

# summary matrices for refrence-changing models
betaMatrix <- function(model, rearrange = NA) {
# get a similarity matrix of the resulting coefficient pairings for the cost conditions
# first, do a full_join based on column names on the list of coefficient vectors from each dummy code relevel
# then match the names of columns and rows so NAs are in the diagonal
  
  # get the names of the reference group per model iteration
  refnames <- names(model)
  
  # coefficient matrix
  temp <- lapply(model, function(data) {coefficients(data)$SubjID[1, 2:4]})
  mixCoeffs <- bind_rows(temp) 
  preln <- ifelse("Cost" %in% substr(names(mixCoeffs), 1, 4), 4, 5) # count how many characters precede the name of each cost (diff across studies)
  dimnames(mixCoeffs) <- list(refnames, substr(names(mixCoeffs), preln + 1, 20))
  mixCoeffs <- as.matrix(mixCoeffs[, match(rownames(mixCoeffs), colnames(mixCoeffs))])
  mixCoeffs[is.na(mixCoeffs)] <- 0
  
  # now the pvals
  temp <- lapply(model, function(data) {as.list(summary(data)$coefficients[2:4, 4])})
  mixPvals <- as.matrix(bind_rows(temp)) 
  dimnames(mixPvals) <- list(refnames, substr(colnames(mixPvals), preln + 1, 20))
  mixPvals <- as.matrix(mixPvals[, match(rownames(mixPvals), colnames(mixPvals))])
  mixPvals[is.na(mixPvals)] <- 1

  # if you would like to re-arrange the coefficient order, supply a vector with the desired sequence
  if (length(rearrange) > 1) {
    mixCoeffs <- mixCoeffs[rearrange, rearrange]
    dimnames(mixCoeffs) <- list(rearrange, rearrange)
    mixPvals <- mixPvals[rearrange, rearrange]
    dimnames(mixPvals) <- list(rearrange, rearrange)
  }
  
  # combine matrices into list to return
  out <- list(Betas = round(mixCoeffs, digits = 2),
              Pvals = round(mixPvals, digits = 5))
  
  return(out)
  
}

# gather basic data (this is better as a function, then rbind)
summarizeData <- function(Data = waitData, nSubs = nSubsW, type = "Wait"){
  
  tempSummary <- data.frame(group = rep(type, nSubs))
  
  if (type == "Cognitive") {
    
    for (subj in seq(nSubs)){
      
        # Choice index
        tempChoice <- Data[[subj]]$Choice
        
        # Proportion complete
        tempSummary[subj,2] <- mean(tempChoice)
      
        # earnings
        tempSummary[subj,3] <- sum(Data[[subj]]$Offer[tempChoice==1])
        
        # RTs
        tempSummary[subj,4] <- median(Data[[subj]]$`Choice RT`[tempChoice==0])
    
        # Mistakes (only for cognitive effort group)
        tempMistakes <- sum(cogDataLong[[subj]]$`Trial Outcome`=="Forced travel") # how many forced travel trials
        tempTrialnum <- length(Data[[subj]]$Choice) # how many decision trials
        tempSummary[subj,5] <- tempMistakes / (tempMistakes + tempTrialnum) # proportion of forced travels throughout the exp        
    
    }    
    
  } else {
    
    for (subj in seq(nSubs)){
        
        # choice index
        tempChoice <- Data[[subj]]$Choice
      
        # propotion complete
        tempSummary[subj,2] <- mean(tempChoice)
        
        # earnings
        tempSummary[subj,3] <- sum(Data[[subj]]$Offer[tempChoice==1])
        
        # RTs
        tempSummary[subj,4] <- median(Data[[subj]]$RT[tempChoice==0])
        
        # Mistakes (to allow for mistakes on the cognitive group)
        tempSummary[subj,5] <- NA
        
    }
  
  }
  
  return(tempSummary)
  
}

# this helps clean p-values for inclusion in text
# it takes the value, and reports < 0.001 if needed, or rounds to the second digit otherwise
report_p <- function(pval) {
  # make sure the p-values entered are numeric (sometimes they can be characters..thanks R)
  pval <- as.numeric(pval)
  
  # do p < 0.001 or round depending on the value
  # maybe adapt with case_when(), so there are three levels: observed, < 0.01, and < 0.001
  adapted_p <- ifelse(pval < 0.001, "< 0.001", paste("=", round(pval, digits = 3)))
  
  return(adapted_p)
}

# remove break time and start counting from 0
standardize_time <- function(subjData) {
  # remove the break time (variable across subjects) and start counting time from 0 (otherwise it can add physical effort calibration)
  breakTime <- min(subjData$ExpTime[subjData$Block == 4]) - max(subjData$ExpTime[subjData$Block == 3])
  subjData$ExpTime[which(subjData$Block > 3)] <- subjData$ExpTime[which(subjData$Block > 3)] - breakTime + 16
  subjData$ExpTime <- subjData$ExpTime - min(subjData$ExpTime)
  
  return(subjData)
}
```

```{r Load data for between-subject experiment, echo = FALSE, warning = F}
### data loading and cleaning up
# forced travels are switched to acceptances, as they reflect a preference for that trial
# rawChoice will be used to compute the # of mistakes
# The RT is upper-bounded because a glitch in the code made two 14s trials last longer (among all p's) 
setwd("./Cost2/data")
files <- dir(pattern = '_log.csv')

# load data
dataBtw <- tibble(SubjID = files) %>%
  mutate(contents = map(SubjID, ~ suppressWarnings(read_csv(., col_types = cols()))))  %>%
  mutate(Cost = substring(SubjID, 5, 8),
         Cost = case_when(Cost == "wait" ~ "Wait",
                          Cost == "cogT" ~ "Cognitive",
                          Cost == "phys" ~ "Physical",
                          Cost == "phea" ~ "Easy"),
         SubjID = as.integer(substring(SubjID, 0, 3))) %>%
  unnest() %>%
  rename(TrialN = X1,
         ExpTime = Experiment.Time) %>%
  mutate(rawChoice = Choice, 
         RT = ifelse(RT > 14.1, 14, RT),
         Choice = ifelse(Choice == 2, 1, Choice), # forced travels (2) become acceptances (1)
         Completed = ifelse(rawChoice == 2, 0, rawChoice),
         Half = ifelse(Block < 4, "Half_1", "Half_2"),
         Cost = factor(Cost, levels = c("Physical", "Cognitive", "Wait", "Easy")),
         optimal = case_when(
          (Handling == 10 & Offer < 8) ~ 0,
          (Handling == 14 & Offer < 20) ~ 0,
          TRUE ~ 1
          )
         ) %>%
  group_by(SubjID) %>% 
  do(standardize_time(.)) %>% 
  group_by(SubjID, Block) %>%
  mutate(blockTime = ExpTime - min(ExpTime),
         blockElapsed = blockTime - dplyr::lag(blockTime, default = 0)) %>% # how much time elapsed between trials, counting per block
  ungroup()

# load the cognitive task performance logs
setwd("./extras")
files <- dir(pattern = '_log.csv')

dataBtw_coglogs <- tibble(SubjID = files) %>%
  mutate(contents = map(SubjID, ~ suppressWarnings(read_csv(., col_types = cols()))))  %>%
  unnest() %>%
  rename(Offer = Reward) %>%
  mutate(SubjID = substring(SubjID, 8, 10),
         Trial_Time = round(Choice.RT),
         Choice = ifelse(Choice == 2, 1, Choice), # forced travels (2) become acceptances (1)
         Half = ifelse(Block < 4, "Half_1", "Half_2"),
         optimal = case_when(
          (Handling == 10 & Offer < 8) ~ 0,
          (Handling == 14 & Offer < 20) ~ 0,
          TRUE ~ 1
          )
         ) 

# get a simple subject list and the number of subjects
subjList_btw <- unique(dataBtw$SubjID)
nSubjs_btw <- length(subjList_btw)

```

```{r Load data for within-subject experiment, echo = FALSE, warning = F}
# First looks at the new data
# The RT is upper-bounded because a glitch in the code made one 10s last 14s
setwd('./Cost3/data/')
files <- dir(pattern = 'main_log.csv')

# load the data and remove extreme subjects
dataWth <- data_frame(SubjID = files) %>% 
  mutate(contents = map(SubjID, ~ suppressWarnings(read_csv(., col_types = cols()))))  %>%
  mutate(SubjID = substring(SubjID, 0, 3)) %>%
  unnest() %>%
  mutate(RT = ifelse(RT > 10.1, 10, RT),
         Half = ifelse(Block < 4, "Half_1", "Half_2"),
         Btype = BlockType) %>%
  unite(Cost, Cost, BlockType) %>%
  rename(TrialN = X1) %>%
  mutate(rawChoice = Choice,
         Choice = ifelse(Choice == 2, 1, Choice),
         Completed = ifelse(rawChoice == 2, 0, rawChoice),
         Cost = case_when(Cost == "WAIT_0" ~ "Wait-C",
                           Cost == "COGNITIVE_0" ~ "Cognitive",
                           Cost == "GRIP_1" ~ "Physical",
                           Cost == "WAIT_1" ~ "Wait-P"),
         Cost = as.factor(Cost)) %>%
  group_by(SubjID) %>%
  mutate(BlockOrder = ifelse(Btype[1] == 0, "Cognitive1st", "Physical1st")) %>%
  group_by(SubjID) %>% 
  do(standardize_time(.)) %>% 
  group_by(SubjID, Block) %>%
  mutate(blockTime = ExpTime - min(ExpTime)) %>%
  ungroup()

# load the cognitive task performance logs
files <- dir(pattern = 'coglog')
colname <- c("Handling", "Offer", "Outcome", "RT", "Trial_Time", "ExpTime", "Trial_outcome","Type", "Setup")

dataWth_coglogs <- data_frame(SubjID = files) %>% 
  mutate(contents = map(SubjID, ~ suppressWarnings(read_csv(., col_names = colname, col_types = cols()))),
         SubjID = substring(SubjID, 8, 10)) %>%
  unnest() %>%
  mutate(Trial_time = ifelse(RT > 10.1, 10, RT),
         Trial_Time = round(Trial_Time)) %>%
  group_by(SubjID) %>%
  mutate(Half = ifelse(ExpTime < (max(ExpTime) / 2), "Half_1", "Half_2")) %>%
  ungroup()


# Get just the subject list and number of subjects
subjList_wth <- unique(dataWth$SubjID)
nSubjs_wth <- length(subjList_wth)
```

1.  Department of Psychological and Brain Sciences, Boston University, Boston, USA

2.  Center for Systems Neuroscience, Boston University, Boston, USA

\bigskip

Corresponding authors: Claudio Toro-Serey (ctoro@bu.edu) & Joseph T. McGuire (jtmcg@bu.edu)


\newpage

## Abstract

  Cognitive and physical effort are typically regarded as costly, but recent findings have suggested that exerting effort can boost the value of prospects under certain conditions. Here we embedded mental and physical effort in a "diet choice" foraging task, which required decision makers not only to evaluate the magnitude and delay of a focal prospective reward, but also to estimate the general opportunity cost of time. In two experiments, independent sets of participants collected rewards that required equivalent periods of cognitive effort, physical effort, or unfilled delay. Monetary offers varied per trial, and the two experiments differed in whether the type of effort or delay cost was the same on every trial (between-subjects, n=21 per condition), or varied across trials (within-subjects, n=48). Participants were free either to accept or reject the cost/reward prospect offered on each trial. All participants were more likely to accept offers when rewards were higher and delays shorter (in line with a reward-maximizing strategy), almost never reversed their acceptance decisions, and committed few errors while completing the effort requirements. When participants faced only one type of cost, cognitive effort persistently produced the highest acceptance rate compared to trials with an equivalent period of either physical effort or unfilled delay. We theorized that if cognitive effort were intrinsically rewarding, we would observe the same pattern of preferences when participants foraged for varying cost types in addition to rewards. In the within-subject experiment, an initially higher acceptance rate for cognitive effort trials disappeared over time amid an overall decline in acceptance rates as participants gained experience with all three conditions. Our results extend the view that cognitive demands may reduce the discounting effect of delays, but also suggest that differences in cost can eventually fade if individuals actively experience alternative forms of demand. Rather than assigning intrinsic value to cognitive effort, our findings support the idea that a cognitive effort requirement might influence contextual factors such as subjective delay durations or the perceived opportunity cost of time. Such altered estimations can be recalibrated if multiple forms of demand are interleaved.

\newpage

## Introduction

  Evaluating whether to pursue a desired outcome often requires assessing the demands required for obtaining it. In psychology and economics, demands for both time and effort have traditionally been identified as costs that detract from the net subjective value of rewards [@Shenhav2017 ;@Westbrook2015]. Indeed, decision makers tend to prefer low-effort and immediate rewards over larger rewards that demand longer delays and higher effort [@Ainslie1975; @Hull1943; @Kool2010; @Kool2018; @Treadway2009; @Westbrook2013]. However, people often change their attitudes towards exerting effort in everyday decisions, resulting in pursual of effortful avenues instead. Emerging experimental and theoretical work has chiefly explained this phenomenon by suggesting a reciprocal relationship between effort and outcome, whereby exerting effort boosts the perceived value of its outcomes [@HernandezLallement2014; @Kacelnik2002; @Mochon2012], and positive outcomes imbue value into their preceding effortful behavior [i.e. learned industriousness; @Eisenberger1992]. Though intuitively valid and empirically supported, this explanation leaves open questions about what features of an effortful task can make it attractive in its own right, and how contextual factors mold its perceived costs [@Inzlicht2018]. Understanding these intrinsic qualities could improve our ability to motivate completion of effortful daily activities, such as schoolwork or physical exercise.
  
  What is often missing from existing accounts is the common case of choosing among types of behavioral demands. Studies have traditionally compared the reduction of subjective value imposed by each type of demand, often through discounting functions that capture how much less appealing a reward becomes as a function of the amount or intensity of its associated behavior [@Ainslie1975; @Frederick2002; @Green1994; @Kable2007; @Kool2010; @Westbrook2013]. Although this line of work suggests that discounting functions are similar for delay and effort [@Massar2015; @Prevost2010; @Seaman2018], there is ongoing debate about the shape of the discounting function across effort types [@Arulpragasam2018; @Biaaszek2017; @Chong2017; @Hartmann2013; @Klein-Flugge2016; @Kool2018; @Prevost2010], and discount rates do not always correlate across cost domains [@Seaman2018; @Westbrook2013]. However, sporadic preferences for effort could arise from the availability of other types of demands in the same choice environment—in other words, from its context [@Kool2018]. For example, exerting high effort for a reward might be aversive, but the same level of work might become valuable when compared to the boredom of passive waiting (e.g. taking the cumbersome way home rather than waiting for a delayed train, even if the resulting time of arrival is the same). Conversely, even though people tend to donate more to charitable events that require them to perform effortful activities (i.e. the martyrdom effect), they are also less likely to engage in such events once an easier alternative to raise funds becomes available [@Olivola2013]. Recent work has proposed that the subjective costs produced by delays and cognitive effort is modulated by the possibility of missing out on better concurrent alternatives, namely the opportunity cost of time [OC; @Fawcett2012; @Kurzban2013; @Otto2019]. This evidence sets the stage for examining what people would actually select when given the option for multiple types of demands, including the unexplored question of deciding between cognitive and physical effort [@Inzlicht2018; @Kool2018].

  Single choice foraging paradigms provide a natural way to probe preferences among demands, by asking individuals whether the current offer outweighs what could be obtained and experienced during that time instead [@Stephens2019]. Foraging has recently attracted high scientific interest due to its ecological validity, which is rooted in evolutionary choice behaviors [@Hayden2018; @Mobbs2018], and offers a number of desirable qualities. First, it allows for the estimation of optimal, reward maximizing choice patterns based on the opportunity cost of time [@Charnov1976; @Krebs1977]. We can therefore interpret observed deviations in acceptance rates from such optimal behavior in addition to comparing them among demands. Second, the sequential nature of foraging allows individuals to emphasize different elements of the decision context. For example, the cost (or value) of effort or delay could globally depend on the availability of alternative demands (as suggested above), impacting the perceived opportunity cost of time. Alternatively, it could be intrinsic and robust to such manipulation, and/or depend on task-specific factors that are adjusted once individuals experience different forms of behavioral costs. Foraging rests on an abundant modeling history that can help to computationally capture such emphases [@Stephens2019].

  In this study, we tested the hypothesis that preferences would vary depending on whether individuals face a single or multiple types of behavioral demands (i.e. cognitive effort, physical effort, and delay). We explored this question in two foraging experiments: a between-subjects design in which participants foraged for rewards while performing a single demand, and a within-subjects extension that had individuals forage for demands in addition to rewards. We leveraged the ecological and computational richness of foraging paradigms to provide new insights on a number of open questions [@Inzlicht2018]. First, the existence of a well-defined optimal choice strategy helped us identify when demands boost the value of rewards (i.e. overharvesting poor offers). Second, increasing the level of exposure to types of demands allowed us to asses whether their cost was intrinsic or relative. Finally, confronting participants with both mental and physical effort let us probe whether these preferences were domain general or task-specific, and in the case of the latter, isolate factors that influenced preferences beyond the often-cited associative relationship between effort and outcome.

  Our results provided mixed support for our pre-registered hypotheses, which are detailed in https://osf.io/2rsgm/. Confirming our predictions, we found that individuals could approximate optimality by preferring higher rewards and shorter delays, regardless of the richness of the environment or whether they faced a single or intermixed demands. When faced with a single type of demand, people in the cognitive effort group accepted more trials than optimal, whereas physical effort and wait groups under-accepted, partially matching our predictions and highlighting that cognitive effort can add value to outcomes. However, against our expectations, we found that a similar preference for cognitive effort disappeared over time as decision makers foraged for types of demands in addition to rewards. Computational modeling suggested that cognitive effort was valuable because it shortened the subjective elapsed time of trials, a perception that could be recalibrated as individuals experienced multiple forms of demands. The shorter time sensitivity of cognitive effort could be due to its constant recruitment of working memory, impeding estimations of the nominal time, or because of the discrete nature of the task (versus a continuous configuration of the physical and wait tasks). These results expand our understanding of the attractiveness of effort, bringing forth outcome-independent features that can be leveraged to motivate effort engagement in diverse everyday scenarios. 

## Methods
 
### **Experiment 1: Between-subjects comparison of costs**

### Participants

  Both between- and within-subject experiments were pregeristered with the Open Science Framework (https://osf.io/2rsgm/registrations). All experimental procedures were approved by the Boston University Institutional Review Board, and written consent was acquired for all participants. For the between-subject experiment, we recruited individuals until a desired number of 84 eligible participants was achieved (58 Female, median age = 21, range = 18 - 31; number excluded before reaching goal = 8). The sample size was determined by means of power analysis (ANOVA), using a significance level of 0.05, power of 0.8, an effect size of f = 0.45 (calculated from a pilot study), and three groups (one for each cost type). The resulting per-group sample was 20, which we increased to 21 in order to match three possible block orders. We added an extra group of 21 participants who experienced a minimally effortful condition in order to determine whether effort or pure engagement were driving our results.
  
We excluded participant datasets based on four preregistered criteria: 1) Consent: If they withdrew their participation; 2) Inattentiveness: a catch trial was placed at the end of each experimental block, asking participants to press a key within 3 seconds (time requirement based on pilot study response times). A participant who failed two or more of these checks was excluded and replaced. 3) Improbable choice behavior: The task was structured so that one reward amount must always be accepted. A participant who quit every trial in at least one block was assumed not to have followed or understood task instructions, or to have disengaged from the task altogether. 4) Performance: Participants were forced to travel if they made 2 mistakes in a cognitive effort trial (see task procedures below), or if they gripped below threshold during physical trials. Any participant with more than 30% forced travels was excluded.


### Foraging Task 

  All experimental tasks were implemented using PsychoPy 2 [v1.85.1, @Peirce2019] on a Macbook Pro laptop. In this task, participants foraged for monetary rewards in an environment in which each trial required either physical effort or cognitive effort for a set period of time (the “handling time”), or an equivalent unfilled delay (Figure 1). Their goal was to maximize their gains within a fixed amount of time. On each trial a monetary offer was displayed for 2 s, and participants had the opportunity to expend time and/or effort during the handling time in order to earn it. Upon completion of a trial participants saw a 2 s window displaying the reward obtained, which was followed by a travel time to the next offer. Alternatively, the participant could quit at any point during the handling time by pressing the spacebar on the computer, and immediately start traveling. 
  
  Each participant was assigned to one of three types of costs (cognitive effort, physical effort, or delay), and a fourth group of equal size faced an effortless physical task that involved minimal gripping. Each group was unaware that other cost conditions existed. Participants exerted physical effort by maintaining grip on a handheld dynamometer (*VERSION*, BIOPAC Systems, United States) using their dominant hand. Gripping requirements were calibrated at 20% of maximum voluntary contraction (MVC, acquired at the beginning of the session). Cognitive effort entailed switching among Stroop, dot motion coherence, and flanker tasks. In Stroop, one of three color names was displayed on the screen (red, blue or green), with a font color that was either congruent (e.g. word red painted in red) or incongruent (e.g. word red painted in blue). Participants had to select the color of the font, not the word displayed (i.e. they had to suppress their tendency to read the word). For motion coherence, 100 solid white dots moved on the screen. A fraction of these dots moved cohesively to the left or right, while the rest moved in random directions (coherence could be either 30% or 40%, uniformly sampled). Participants had to respond with the direction of the cohesive set of dots. In flanker, rows of arrowheads pointed either to the left or the right (maximum of 3 rows, 3 to 13 arrowheads per row). Participants responded with the direction of the center arrowhead, which could point in the same or opposite direction from its neighbors. These tasks were configured so that responses always involved a left or right key press (e.g. for Stroop, two colored circles were presented at each side of the screen). During the handling time, cognitive tasks and their configurations were randomly sampled, and were presented for 1 s followed by a 1 s inter-stimulus interval. Participants were asked to respond within each task’s presentation time. Before the experiment, participants trained in each cognitive task until they correctly performed six consecutive tasks of each kind. While in the handling time, if participants failed to maintain above-threshold gripping or made two mistakes during the cognitive task, they were forced to travel and missed the reward.  
  
  There were three block types, in which handling times of 2, 10, or 14 seconds were paired with travel times of 14, 6, and 2 seconds, respectively (note that all combinations add up to 16 seconds). Timing parameters were held constant within each 7-min block. Each of these blocks was experienced in a pseudo-randomized order, and repeated in the same order after a short break in order to probe choice stability (total session length = 42 minutes). Reward amounts varied uniformly per trial (4, 8, or 20 cents), with the constraint that every reward was presented twice every six trials. This prevented sequences from being dominated by a single amount during any window of time. Timing information was disclosed at the beginning of each block, and rewards displayed during a 2 s offer window before each trial began. Participants received training prior to the experimental session, and were told about all possible environmental statistics in order to preclude biases due to experience-dependent learning [@Dundon2020; @Garrett2020]. 

```{r Trial plot (Fig. 1), eval = !figsEnd, out.width = "80%", fig.align = "center", fig.cap= "General foraging trial structure. On each trial, participants were offered to earn money (4, 8, or 20 cents) by sustaining effort or waiting during the handling time (2, 10, or 14 s). The end of a trial was followed by a travel time (handling and travel times always added up to 16 s). Participants could skip unfavorable trials and immediately start traveling to a potentially better offer. In the between-subject experiment, cost was fixed per participant and handling time varied per block. In the within-subject version, handling time was fixed at 10 s, but a combination of effort and delay trials changed per block. Possible costs, handling times, and rewards were fully disclosed in order to avoid experience-dependent learning."}

include_graphics("./Images/FIG1_short.png")
```


### Operationalization of Cost

  Foraging theory posits that accepting a delayed reward should depend on the opportunity cost (OC) of time incurred in obtaining it, given by the richness of the environment [@Charnov1976; @Stephens2019]. In this study, the richness of the environment was manipulated by the length of the handling and travel times, where shorter travel times produced richer environments. Since time combinations were fixed per block, we calculated each block’s optimal accept/reject strategy by computing all decision strategies according to the following equation:
  
$$\begin{aligned} 
\dfrac {\sum p_i R_i} {\sum p_i H + 3T},p \in \{0, 1\}
\end{aligned}$$
  
  where *R* and *p* are the reward, and acceptance probability of offer i, respectively, *H* is the handling time for the block, and *T* its associated travel time. This gave us the total reward per second attainable in each block as a function of the acceptance threshold (i.e. the lowest amount accepted). Figure 2 shows possible earnings per second for each choice strategy, as well as the lowest amount participants should accept in order to maximize their rewards (circled dots). For example, a participant in a 10 s handling block should accept 8-cent and 20-cent rewards (and reject 4-cent rewards) to maximize their reward rate. Note that accepting every offer was often detrimental to participant earnings. Significant deviations from the optimal strategy can be understood as reflecting changes in subjective OC. This approach allowed us to test if certain demands boosted the value of offers, expressed as a subjective OC that fell below the optimal rate.

```{r Optimal plot (Fig. 2), eval = !figsEnd, out.width = "80%", fig.align = "center", fig.cap= "Possible earnings per second for each acceptance threshold (i.e. the smallest amount accepted) for each handling time. Circles denote the reward-maximizing threshold for each block, which is described in the table. Experiments shared the 10 s handling block, in which it was optimal to skip all 4 cent offers."}

include_graphics("./Images/FIG2.png")
```

### Analyses

  All analyses were performed in R 3.5.2 [@RTeam2018]. First we tested whether decision makers integrated delay and reward information. To address the prediction that participants would be more likely to accept higher reward and shorter handling time trials, we fit a mixed-effects logistic regression to predict trial-wise acceptances [using the lme4 package, @Bates2015], giving a random intercept to each subject to account for biases in participant behavior. We included regressors for handling time and reward, as well as two covariates that probed the influence of recent history on choices. The first was an autoregressive regressor containing the number of consecutive offer misses prior to a given trial (misses could be due to quits or forced travels). The second regressor tracked the sum of the previous *n* offers. We identified the *n* that minimized Akaike's Information Criterion (AIC) among 6 versions of the model (tracing from 2 previous trials up to 7, beyond which the model failed to converge), and reported all coefficients from the winning model.
  
  Our foraging task was configured such that over and under accepting were detrimental to total earnings. To confirm this, we fit a general linear model with constant, linear, and quadratic terms to estimate the correspondence between proportion of trials completed (independent variable) and total earnings (dependent variable). A significant quadratic coefficient thus would signal that the task statistics operated as expected. Next, to determine the optimality of each group’s decisions, we performed two-sided one-sample t-tests to see if the proportion of acceptances for each time/reward combination was significantly different from the optimal rate (see Operationalization of Cost). This resulted in 36 independent tests (3 rewards amounts, 3 handling/travel time combinations, and 4 groups), so we corrected for multiple comparisons using False Discovery Rate [FDR, @Benjamini2001].
  
  Next, we compared preferences among cost conditions. We first performed a one-way ANOVA on the proportion of trials accepted using group as a factor. We then compared the proportion completed among all 4 groups using non-parametric permutation contrasts (6 tests). On each of 5000 permutation iterations, the group assignment was randomly shuffled without replacement, and the difference in mean acceptance rates across iterations created a putative null distribution. The unpermuted group mean difference was then evaluated against this permuted distribution. The same approach was used for total earnings. This gave us an initial glimpse on the potential differences in cost among conditions. 
  
  In order to further look at the effect of handling, offer, and cost type, we computed the probability of accepting a trial with a mixed-effects logistic regression. Based on the task structure and our main question, our a priori model of interest included cost condition, handling time, and reward amount as fixed main effects, and a random intercept per subject. Cost condition was modeled with three categorical terms, with the fourth condition as the reference condition. We ran three versions of the model with different reference conditions, in order to test all pairwise differences among the four cost conditions. We then examined whether this a priori model outperformed both simpler and more complex models. We used both AIC and Bayesian Information Criterion (BIC) to determine the model that minimized the negative log-likelihood while penalizing the addition of parameters. The regression with each combination of predictors was fitted in a forward order: 1) intercept only; 2) condition only; 3) handling time only; 4) offer amount only; 5) handling time and offer amount (added to the preregistered comparison); 6) condition, handling time, and reward main effects (a priori model from above); 7) adding a handling-by-reward interaction; and 8) all possible two-way interactions. We predicted that model 6 (the a priori setup) would have the lowest AIC and BIC. Nested models with similar AIC were statistically compared using an analysis-of-deviance. The significance test was computed as the probability of the reduction in deviance, based on a chi-square distribution with degrees of freedom equal to the difference in the number of parameters between models.
  
  Next, we examined whether participants were consistent and stable in their choices. For consistency, we visually examined survival curves indicating at what point during the handling time participants quit a trial (censored points included trial completions and forced travels), in addition to quitting time distributions. We interrogated stability by computing each participant’s total proportion of acceptances pre- and post-midpoint for every block type, comparing the mean proportion of acceptances using paired permutations (5000 iterations). We then tested whether the observed differences in costs were still present on each half of the experiment by applying winning model from the mixed-effects logistic comparison to each half separately. In each case, the reference cost category was rotated in order to assess pairwise differences among all costs. We evaluated the consistency of the relative costs by visually comparing the patterns of significant differences between experimental halves.

### **Experiment 2: Within-subjects comparison of costs**

### Participants 

  We collected data from a new sample of 48 eligible participants  (39 Female, median age = 21, range = 18 - 36; number excluded before reaching goal = 6). Sample size was once again determined by means of a power analysis (repeated measures ANOVA), using a significance level of 0.05, power of 0.8, a desired effect size of f = 0.5, and four factors (one for each condition). The resulting sample size was 45, which we increased to 48 in order to balance the potential order of blocks. We excluded participants using the same criteria as in the between-subject experiment.

### Foraging Task Adaptation

  The original between-subjects task was modified so that participants foraged for cost types in addition to rewards. Every participant experienced all forms of cost (delay, physical effort, and cognitive effort). There were two block types, in which delay trials were interspersed with either physical-effort or cognitive-effort trials. This setup prevented participants from having to rapidly switch between response modalities across trials (i.e. keyboard press and handgrip). Block combinations (i.e. physical/wait and cognitive/wait) were experienced three times by each participant in 7-minute-long, interleaved blocks, thus matching the experiment’s length to the between-subject version. Half of the participants experienced a block sequence that started with the physical block. Timing parameters were matched to the middle condition from Experiment 1 (i.e. 10 s handling and 6 s travel), establishing a balance between acquiring enough observations per trial type and being able to compare results across experiments. Participants were informed that this timing combination would be constant prior to beginning the experiment. Cost type combinations were disclosed at the beginning of each block, and reward/cost offers displayed before each trial begins (e.g. “8 cents for physical effort”). Unlike the previous experiment, participants could express their decision to quit during the offer window (instead of during the handling time). Participants trained in each type of demand until they reached the same criteria as in the between-subject experiment.

### Analyseses

  The analytical pipeline was mostly preserved from the between-subject experiment. We first ran tests to determine whether decision makers integrated reward information. We performed a mixed-effects logistic regression with regressors for offer amount, number of consecutive offer misses, and the sum of the *n* previously observed offers (we identified *n* by comparing the AIC of different history lengths). We then confirmed that the under- and over-acceptance were detrimental to total earnings by performing the same quadratic linear model explained above. To determine the optimality of the group’s decisions, we examined whether each cost type produced a bias to over or under-accept at 4 and 8 cents (assuming that 20 cents were always be accepted, given our design). The reward-maximizing strategy was to always reject 4 cents and accept 8 cents, yielding a combined optimal proportion of acceptances of 50% (see Operationalization of Costs for details). We performed a two-sided one-sample chi-squared test of proportions against the null probability of 0.5 for each type of cost. Therefore, a significant difference indicated that participants either over or under accepted rewards.
  
  To compare acceptances among cost conditions, we first computed the probability of accepting a trial with a mixed-effects logistic regression. The model included cost condition and reward amount as fixed main effects, and participant ID as a random intercept (handling time is not modeled, as participants experienced a single time combination). Cost condition was modeled with three categorical terms, with the fourth condition as the reference condition (two effort conditions, and two delay conditions corresponding to each effort type). We ran three versions of the model with different reference conditions, in order to test all relevant pairwise differences among the four cost conditions. We compared the a priori model to alternative parameterizations using AIC and BIC, varying model complexity in a forward order: 1) intercept only; 2) cost only; 3) reward only; 4) cost and reward main effects (a priori model); and 5) adding a two-way interaction. We predicted that model 4 would have the last considerable decrease in the negative log-likelihood. Models with similar AIC were formally compared using the analysis-of-deviance approach from the between-subjects experiment.
  
  Next we probed consistency and stability in people's choices. First, we qualitatively assessed the number of quits performed during during the handling time through a survival curve, and plotted the quit response time distribution as a function of block to examine choice consistency over time. Next, we computed the proportion of quits that were performed during the choice window versus during the handling time. We interpreted as confidence if a participant made at least 80% of their choices during the offer window. We then computed each participant’s total proportion of acceptances on the first two and last two blocks, and compared them using a paired permutation analysis (5000 iterations). We avoided the third and fourth block because their effort designation was counterbalanced across participants, which would have biased our estimates.

### **Computational Modeling of Foraging Behavior**
  
  We performed model comparisons to identify qualities that explained findings from both experiments. Based on the observed choice behavior, an adequate model had to meet seven criteria: 1) show deviations from optimality that were not simply due to overharvesting 2) keep track of recent offer history; 3) experience a slight reduction of acceptances in effortful conditions; 4) persistently display higher acceptances for cognitive effort when faced with single behavioral costs; 5) initially match this preference for cognitive effort trials when foraging for demands and rewards; 6) display a disappearance of this preference as acceptances decreased to a common rate across costs; and 7) not be affected by block order effects. These conditions discarded a number of popular explanatory candidates that were not modeled, such as learned industriousness, boredom, fatigue, sunk cost effects, or asymmetric learning of environmental richness (see Discussion). 
  
  We started by fitting a base model derived from the normative predictions of the Marginal Value Theorem [MVT; @Charnov1976], which states that a delayed reward should be accepted if its earning rate surpasses the opportunity cost (OC) of time incurred in obtaining it. This decision rule can be expressed as

$$\begin{aligned}
Model_{base} = R_i - \gamma H_i
\end{aligned}$$

  such that a trial’s reward $R_i$ should be accepted if it is larger than the average amount one could earn per second in the environment $\gamma$ (gamma; the OC) during the handling time $H_i$. Individuals with higher gammas therefore required higher rewards in order to accept a given offer. We fit this and other models using a softmax function to predict the probability of accepting a given trial

$$\begin{aligned}
P(accept)_i = \dfrac {1} {1 + e^{-(1/\beta)(Model_{base})}}
\end{aligned}$$

  All free paremeters of the softmax function (in this case, $\gamma$ and the temperature parameter $\beta$) were estimated independently for each participant, using a maximum likelihood (MLE) approach through the NLOPTR package (CITE **GARY INFO**). The gamma optimization search space was bound by extreme choice values. For example, someone with an extremely high OC would not accept even the most beneficial offers, and would reject a 20 cent offer when the handling time was 2 seconds (or 20/2). Conversely, a participant experiencing low opportunity costs would accept the non-beneficial offer of 4 cents for a 14 second delay (or 4/14). **We cross-validated each subject’s OC value using the pre-midpoint data for estimation, and post-midpoint choices for testing. The estimates from the training sample were used to predict acceptances in the testing sample, and the mean percent correctly predicted was reported for each group.** We then used a two-sided one-sample t-tests to compare each group's gammas to the average optimal earning rate (0.61, see Operationalization of Cost), allowing us to determine whether any demand boosted an offer's value. Finally, we compared gammas with an ANOVA (with condition as a factor), followed by post-hoc permutations (5000 iterations) comparing mean gammas per group. 

  The base formulation lacked the flexibility needed to capture the dynamics observed in the within-subjects experiment. We therefore tested a second model which allowed $\gamma$ to evolve over time, adapting a learning rule from @Constantino2015


$$\begin{aligned}
\gamma_{i+1} = (1 - (1 - \alpha) ^ {\tau_i}) \dfrac{R_{i} A_{i}}{\tau_i} +  (1 - \alpha) ^ {\tau_i} \gamma_{i}
\end{aligned}$$


  where $A_i$ is an acceptance indicator (0 or 1), and $\tau_i$ the amount of time spent in the trial. Higher values of the learning rate $\alpha$ (ranging from 0 to 1) prioritized updating from the current trial’s earning rate (left side of the equation), weighed by the amount of time experienced in the past trial. This time was given by 

$$\begin{aligned}
\tau_{i} = H_{i}  A_{i} + T_{i}
\end{aligned}$$

  such that the handling time was counted only for accepted trials. Gamma was initialized by the median earning rate achieved across individuals. These modifications allowed for the tracking of recent history that was part of our criteria, while mostly maintaining the same complexity (i.e. one free parameter) and structure of the base model


$$\begin{aligned}
Model_{adaptive} = R_i - \gamma_i H_i
\end{aligned}$$
  
  
  A final model considered the relationship between effortful engagement and the perceived length of time. Being immersed in a task can make time transpire subjectively faster [@Csikszentmihalyi2014], which might have made the cognitive task attractive. Time sensitivity has been previously used to improve fits on classic hyperbolic models of delay discounting [@Mckerchar2009]. It is possible that such subjective handling time was recalibrated in the within-subjects experiment, resulting in the observed convergance of acceptances across demands. We therefore added a time sensitivity parameter $S$ to the model, with a separate value associated with each type of demand. The values of $S_{demand}$ could then be recalibrated as participants experienced them, averaged according to the following formula [adapted from @Sutton2018]

  
$$\begin{aligned}
s_{i + 1} = s_i + A_i \left (\frac{1}{i}[S_{demand} - s_i] \right)
\end{aligned}$$


  in which the global value of $s$ was updated every time an individual completed a trial. We fitted the initial value of $S_{demand}$, which was bounded between 0 and 2 (values lower and greater than 1 indicating slower and faster perceived time, respectively). Even though the value of $s$ was updated equally across individuals, we theorized that each participant would incorporate $s$ into their decisions at different rates. This intuition was given by a weighted sum
  
  
$$\begin{aligned}
mS_i = s_i w_i + S_{demand} (1 - w_i)
\end{aligned}$$


  such that in every trial, the recalibrated length estimate of the handling time ($s$) was balanced against the actual experience of subjective time produced by the present demand ($S_{demand}$). The weight $w_i$ controlled how early participants started integrating the recalibrated time $s$ into their decisions, given by
  
  
$$\begin{aligned}
w_i = \left (\frac{i}{max(i)} \right ) ^{\alpha_w}
\end{aligned}$$
 
 
  in which high values of the free parmeter $\alpha_w$ denoted that decision makers incorporated the recalibrated subjective time early on in the experiment. Similar to @Mckerchar2009, the values of $mS_i$ exponentially modulated every instance of the handling time ($H_i$) from the adaptive model. In this way, subjective time perception produced by different demands could influence choice in two ways: retrospectively affecting the value of $\gamma$ through the perceived length of recent trials


$$\begin{aligned}
\tau_{i} = H_{i} ^ {mS_{i}}  A_{i} + T_{i}
\end{aligned}$$

and prospectively impacting the expected trial length at the time of a decision

$$\begin{aligned}
Model_{time} = R_i - \gamma_i H_i^{mS_i}
\end{aligned}$$

  This subjective time model was fitted differently across experiments, such that the fits from experiment 1 informed those of experiment 2. Since participants in the between-subjects experienced a single demand, the value of $s$ was reduced to the initial fit of $S_{demand}$, without any need to fit $\alpha_w$ (which was set to 1). For the within-subjects experiment, we assumed that the fitted values of $S_{demand}$ approximated the true time sensitivity produced by each demand. Therefore, for each individual in the within-subjects experiment, we set each value of $S_{demand}$ to be the median fitted value from the first experiment, and fitted $\alpha_w$ instead. In this way model complexities was fixed across experiments, even though different aspects of the model were emphasized.
  
  In summary, the free parameters from the subjective time model captured three features. First, how fast or slow a given demand felt ($S_{demand}$). Second, how early this time sensitivity was recalibrated and integrated into decisions based on the experience of intermixed demands ($\alpha_w$). Finally, whether this time sensitivity was used retrospectively (i.e. high $\alpha$), or prospectively (i.e. low $\alpha$). Note that every model could be reduced to the base formulation for values of $\alpha$ = 0, $\alpha_w$ = 1, and $S_{demand}$ = 1.
 
  In order to compare model fits, we used expectation maximization...(**Gary**)
  

(noise in the estimation of environmental richness [@Cash-Padgett2020])

## Results

### **Between-subjects: Tests of whether decision makers integrate delay and reward information**

``` {r BTW: 16.1.1. Individual logistic, fig.align="center", fig.width = 4, fig.height = 4, echo = FALSE}
# filter out errors and compute R + H logistic per subject, then get coefficients
logisRH <- dataBtw %>%
  filter(Choice < 2) %>%
  plyr::dlply("SubjID", identity) %>%
  lapply(function(data) {glm(Choice ~ Handling + Offer, data = data, family = "binomial")}) %>%
  sapply(coefficients)

# coefficient summaries
coeffRH_mean <- apply(logisRH, 1, mean)
coeffRH_se <- apply(logisRH, 1, sd) / sqrt(nSubjs_btw)

# Rank sum tests
rankHand <- wilcox.test(logisRH["Handling", ])
rankOffer <- wilcox.test(logisRH["Offer", ])

# Plot coeffs
temp <- melt(logisRH[c("Handling", "Offer"), ])
plot_handoffer <- qplot(data = temp, x = Var1, y = value, geom = "boxplot") + 
                    labs(x = "", y = "Coefficients") +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    theme(legend.position = c(0.9, 0.7),
                         panel.grid.major = element_blank(), 
                         panel.grid.minor = element_blank(), 
                         panel.background = element_blank(), 
                         axis.line = element_line(colour = "black"),
                         text = element_text(size = 22))
```

``` {r BTW: 16.1.2. AR sequential effects, echo = FALSE, include = FALSE}

# history
# Instead of quits, I'm now doing completions. This is because one would expect forced travels to also affect choice.
# First, iterate over the recent history to identify which sum length reduces the AIC the most
# tracing the history beyond 7 trials produces non-convergence
historySize_btw <- lapply(seq(2, 7), function(tail) {
  dat <- dataBtw %>%
    #filter(Cost != "Wait") %>%
    group_by(SubjID) %>%
    mutate(recentTravels = seqQuits(Completed),
           recentOffers = ifelse(TrialN < (tail + 1), Offer, Reduce(`+`, shift(Offer, 1:tail)))) 
  
  glmer(Choice ~ Handling + Offer + recentTravels + recentOffers + (1 | SubjID), data = dat, family = "binomial")
})

# then get the summary of the tail length that reduced the AIC the most
historyIndex_btw <- which(sapply(historySize_btw, function(iter) {AIC(iter)}) == min(sapply(historySize_btw, function(iter) {AIC(iter)})))

historyCoeffs_btw <- round(summary(historySize_btw[[historyIndex_btw]])$coefficients, digits = 2)

# # Idea: do AIC to compare models per participant
# # add an AR regressor to track recent quits
# logisRHAR <- dataBtw %>%
#               filter(Choice < 2) %>%
#               group_by(SubjID) %>%
#               mutate(AR = seqQuits(Choice)) %>%
#               plyr::dlply("SubjID", identity) %>%
#               lapply(function(data) {suppressWarnings(glm(Choice ~ Handling + Offer + AR, data = data, family = "binomial"))}) 
#   
# # get the AR CIs per participant 
# AR_CIs <- sapply(logisRHAR, function(x) {tryCatch(suppressWarnings(confint(x)[4, ]), error = function(e){c(0, 0)})}) %>%
#   replace_na(0) %>%
#   t() %>%
#   apply(1, function(x) (x[1] <= 0 & x[2] >= 0))
# 
# # coefficient summaries
# logisRHAR <- sapply(logisRHAR, coefficients)
# coeffRHAR_mean <- apply(logisRHAR, 1, mean, na.rm = T)
# coeffRHAR_se <- apply(logisRHAR, 1, sd, na.rm = T) / sqrt(nSubjs_btw)
# 
# # Rank sum test
# rankRHAR <- wilcox.test(logisRHAR["AR", ])
```

``` {r BTW: 16.1.3. Earnings, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4}
## calculate the proportions and earnings per subject, while keeping group info
temp <- dataBtw %>%
  filter(rawChoice < 2) %>%
  group_by(SubjID, Cost) %>%
  summarise(Earnings = sum(Offer[Choice == 1] / 100),
            pComplete = mean(Choice)) %>%
  mutate(pComplete2 = pComplete^2) %>%
  ungroup()


## linear fit of earnings x prop accept
lmEarn <- lm(Earnings ~ pComplete + pComplete2, data = temp)


## plot
earnFitplot_btw <- ggplot(data = temp, aes(pComplete, Earnings)) +
         scale_color_manual(values = colsBtw) +
         geom_point(aes(fill = Cost), pch = 21, color = "black", size = 3, show.legend = F) +
         stat_smooth(aes(y = Earnings), method = "lm", formula = y ~ x + I(x^2), color = "grey30") +
         labs(x = "Proportion Completed", y = "Total Earnings (dollars)") +
         scale_fill_manual(values = colsBtw) +
         theme(panel.grid.major = element_blank(),
           panel.grid.minor = element_blank(),
           panel.background = element_blank(),
           axis.line = element_line(colour = "black"),
           text = element_text(size = 12))

# Summary table for the linear model
# panderOptions("digits", 2)
# pander(lmEarn, style = "rmarkdown")

```

  In this experiment, groups faced with different behavioral costs (physical effort, cognitive effort, or delay) chose their preferred strategy to maximize rewards in fully-disclosed foraging environments of variable richness. Environmental richness was dictated by the time it took to obtain a reward (the handling time) and the time between trials (the travel time). We hypothesized 1) that participants would approximate reward-maximizing behavior by preferring higher rewards and shorter delays, but that 2) participants confronted with effortful demands would be more likely to accept trials than those faced with passive delay, regardless of handling time and reward amount.
  
  We performed a mixed effects logistic regression to address the first hypothesis. Since participants in the effort groups were forced to travel when they failed to perform above threshold (see Methods), we distinguish between acceptances (entering the handling period) and completions (successfully obtaining a reward). In line with our predictions, larger reward amounts significantly increased acceptance probabilities ($\beta$ = `r historyCoeffs_btw["Offer", "Estimate"]`, SE = `r historyCoeffs_btw["Offer", "Std. Error"]`, *p* `r report_p(historyCoeffs_btw["Offer", "Pr(>|z|)"])`), whereas longer delays decreased them ($\beta$ = `r historyCoeffs_btw["Handling", "Estimate"]`, SE = `r historyCoeffs_btw["Handling", "Std. Error"]`, *p* `r report_p(historyCoeffs_btw["Handling", "Pr(>|z|)"])`). Two additional regressors showed that having missed out on consecutive rewards decreased the probability of acceptance ($\beta$ = `r historyCoeffs_btw["recentTravels", "Estimate"]`, SE = `r historyCoeffs_btw["recentTravels", "Std. Error"]`, *p* `r report_p(historyCoeffs_btw["recentOffers", "Pr(>|z|)"])`), and that participants became more selective when recent offer history was richer ($\beta$ = `r historyCoeffs_btw["recentOffers", "Estimate"]`, SE = `r historyCoeffs_btw["recentOffers", "Std. Error"]`, *p* `r report_p(historyCoeffs_btw["recentOffers", "Pr(>|z|)"])`). The reduction in acceptances from recent misses was driven by the wait group (removing this group from the model yielded a *p* > 0.1), likely because this group accepted the least amount of offers. Comparing likelihood fits among lengths of offer history (e.g. sum over 2 or 3 past trials) suggested that participants were influenced by the `r historyIndex_btw + 1` most recent rewards (AIC = `r round(AIC(historySize_btw[[historyIndex_btw]]), digits = 2)`), which matched the offer presentation rotation (see Methods). 

### **Between-subjects: Comparisons among the four delay and effort conditions**

``` {r BTW: 16.1.4. Optimality (move to acceptance comparison eventually), echo = FALSE}
# in order
# clean data, 
# calculate each individual's prop. accept,
# calculate t.tests with optimality as null mean,
# correct pvalues with FDR
temp <- dataBtw %>%
  filter(Choice < 2) %>%
  group_by(SubjID, Handling, Offer, Cost) %>%
  summarise(pAccept = mean(Choice)) %>%
  mutate(optimal = case_when(
          (Handling == 10 & Offer < 8) ~ 0,
          (Handling == 14 & Offer < 20) ~ 0,
          TRUE ~ 1
        )) %>% 
  group_by(Handling, Offer, Cost) %>%
  summarise(meanAccept = mean(pAccept),
            sdAccept = sd(pAccept),
            optimal = unique(optimal),
            pvals = tryCatch(t.test(pAccept, mu = unique(optimal))$p.value, error = function(e) {1})) %>%
  mutate(FDR = p.adjust(pvals, method = "BY"),
         significant = FDR < 0.05)


# proportion of significant ones
# eventually find a way to plot this (or just place it further down)
# alternative per block: temp %>% group_by(Handling) %>% summarise(prop = mean(significant, na.rm = T))
optimalProp <- mean(temp$significant, na.rm = T) * 100

# proportion accepted per cost condition
optimalProp_cost <- temp %>% 
  group_by(Cost) %>% 
  summarise(propDeviations = round(mean(significant) * 100))


# ## if you want to replicate the cost 3 method
# # this says that all but easy deviated from optimality in the observed direction
# temp <- dataBtw %>%
#   group_by(Cost) %>%
#   filter(Offer < 20, Handling == 10) %>%
#   summarize(propAccept = mean(Choice),
#             accept = sum(Choice),
#             quit = sum(Choice == 0))
# 
# temp2 <- apply(temp, 1, function(row) {prop.test(as.numeric(row[3]), sum(as.numeric(row[3:4])), p = 0.5, conf.level = 0.95)})
#   
#   
# # store stats in a dataframe
# news <- data.frame(probability = sapply(temp2, "[[", "estimate"),
#                                    chisquared = sapply(temp2, "[[", "statistic"),
#                                    confint = t(sapply(temp2, "[[", "conf.int")),
#                                    pval = sapply(temp2, "[[", "p.value"))
# rownames(news) <- temp$Cost


```

``` {r BTW: 16.2.1. Overall Proportion/Earnings, fig.align = "center", fig.width = 8, fig.height = 4, echo = FALSE}  
## earnings and proportion accepted per cost type
# note that earnings are based on successful completions, not just acceptances
temp <- dataBtw %>%
        group_by(SubjID, Cost) %>%
        summarise(Earnings = sum(Offer[(Choice == 1) & (rawChoice < 2)] / 100),
                  pAccept = mean(Choice))


## summaries
overallSumms <- temp %>% 
  group_by(Cost) %>% 
  summarise (mProp = mean(pAccept), 
             sdProp = sd(pAccept), 
             mEarn = mean(Earnings), 
             sdEarn = sd(Earnings)) %>%
  mutate_if(is.numeric, round, digits = 2)

## formal testing 
# proportion completed
propAov <- summary(aov(pAccept ~ Cost, data = temp))

propTests <- temp %>% 
  plyr::dlply("Cost", identity) %>%
  sapply("[[", "pAccept") %>%
  as.data.frame() %>%
  combn(2, simplify = F) %>%
  sapply(function(x) {
    c(contrast = paste(colnames(x), collapse = ifelse(cohenD(x[, 1], x[, 2]) < 0, " < ", " > ")), 
      permPval = round(permute(x[, 1], x[, 2], simple = T), digits = 3),
      cohenD = abs(round(cohenD(x[, 1], x[, 2]), digits = 2)))
  }) %>% 
  t() %>%
  as_tibble()

# pairwise.wilcox.test(temp$pComplete, temp$Cost)

# earnings 
earnAov <- summary(aov(Earnings ~ Cost, data = temp))

earnTests <- temp %>% 
  plyr::dlply("Cost", identity) %>%
  sapply("[[", "Earnings") %>%
  as.data.frame() %>%
  combn(2, simplify = F) %>%
  sapply(function(x) {
    c(contrast = paste(colnames(x), collapse = ifelse(cohenD(x[, 1], x[, 2]) < 0, " < ", " > ")), 
      permPval = round(permute(x[, 1], x[, 2], simple = T), digits = 3),
      cohenD = abs(round(cohenD(x[, 1], x[, 2]), digits = 2)))
  }) %>% 
  t() %>%
  as_tibble()

## plots
# pAccepted
prop1 <- ggplot(temp, aes(Cost, pAccept, fill = Cost)) + 
        geom_boxplot(show.legend = F) +
        geom_jitter(width = 0.1, alpha = 0.3, show.legend = F) +
        scale_fill_manual(values = colsBtw) +
        ylim(c(0, 1)) +
        labs(y = "Proportion Accepted") +
        theme(panel.grid.major = element_blank(), 
            panel.grid.minor = element_blank(), 
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black"),
            text = element_text(size = 12))

# earnings
earnings1 <- ggplot(temp, aes(Cost, Earnings, fill = Cost)) + 
        geom_boxplot(show.legend = F) +
        geom_jitter(width = 0.1, alpha = 0.3, show.legend = F) +
        scale_fill_manual(values = colsBtw) +
        labs(y = "Total Earnings (dollars)") +
        theme(panel.grid.major = element_blank(), 
            panel.grid.minor = element_blank(), 
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black"),
            text = element_text(size = 12))


# Proportion of performance-based forced travels per subject and cost type
# move to 16.2.1.
propFails <- dataBtw %>% 
  filter(Cost != "Wait") %>% 
  group_by(SubjID, Cost) %>% 
  summarise(propFails = mean(rawChoice == 2)) %>%
  group_by(Cost) %>% 
  summarise(meanFT = mean(propFails),
            sdFT = sd(propFails))


## MAYBE BETTER WAY TO DO PAIRWISE COMPARISONS, USING VECTORIZE FROM BELOW
# OCanova$perms <- summaryOC_btw$all %>%
#   select(Cost, Gamma) %>%
#   plyr::dlply("Cost", identity) %>%
#   do.call(cbind, .) %>%
#   select_if(is.numeric) %>%
#   rename(Physical = Physical.Gamma,
#          Cognitive = Cognitive.Gamma,
#          Wait = Wait.Gamma,
#          Easy = Easy.Gamma) %>%
#   outer(., ., FUN = Vectorize(permute), simple = T)
# diag(OCanova$perms) <- 1

```

```{r General prop & earnings plots (Fig. 3), eval = !figsEnd, fig.width = 10, fig.height = 4, fig.align = "center", fig.cap = "Left: Proportion accepted per cost. Middle: Total number of dollars earned by each group by the end of the experiment. Right: The relationship between proportion accepted and total earned. Consistent with the foraging design, participants who over and under accepted earned the least."}
# plots generated in 16.2.1. and 16.1.3.
 prop1 | earnings1 | earnFitplot_btw
```

  The next set of analyses examined the hypothesis that effort group would uniformly accept more trials. Figure 3 (left) shows that the cognitive effort group consistently accepted more offers, which resulted in lower earnings (middle). One-way ANOVAs showed that differences among groups were significant both for overall acceptance rates (F(`r propAov[[1]]$Df[1]`, `r propAov[[1]]$Df[2]`) = `r round(propAov[[1]][1, 4], digits = 2)`, *p* `r report_p(propAov[[1]][1, 5])`), and total earnings (F(`r earnAov[[1]]$Df[1]`, `r earnAov[[1]]$Df[2]`) = `r round(earnAov[[1]][1, 4], digits = 2)`, *p* `r report_p(earnAov[[1]][1, 5])`). Post-hoc permutations (5000 iterations) comparing mean acceptance rates between every pair of costs confirmed that acceptance rates of the cognitive effort group (mean = `r filter(overallSumms, Cost == "Cognitive")$mProp`, SD = `r filter(overallSumms, Cost == "Cognitive")$sdProp`) were higher than those of the physical (mean = `r filter(overallSumms, Cost == "Physical")$mProp`, SD = `r filter(overallSumms, Cost == "Physical")$sdProp`; *p* `r report_p(filter(propTests, contrast == "Physical < Cognitive")$permPval)`, Cohen's D = `r filter(propTests, contrast == "Physical < Cognitive")$cohenD`) and wait groups (mean = `r filter(overallSumms, Cost == "Wait")$mProp`, SD = `r filter(overallSumms, Cost == "Wait")$sdProp`; *p* `r report_p(filter(propTests, contrast == "Cognitive > Wait")$permPval)`, Cohen's D = `r filter(propTests, contrast == "Cognitive > Wait")$cohenD`). It also showed that those faced with the easy task accepted more than those in the wait group, although responses from the easy group were more variable (mean Easy = `r filter(overallSumms, Cost == "Easy")$mProp`, SD = `r filter(overallSumms, Cost == "Easy")$sdProp`; *p* `r report_p(filter(propTests, contrast == "Wait < Easy")$permPval)`, Cohen's D = `r filter(propTests, contrast == "Wait < Easy")$cohenD`) (all other *p* > 0.05). The higher acceptance rates produced by the cognitive effort group resulted in significantly lower earnings (mean = `r filter(overallSumms, Cost == "Cognitive")$mEarn`, SD = `r filter(overallSumms, Cost == "Cognitive")$sdEarn`) than both physical (mean = `r filter(overallSumms, Cost == "Physical")$mEarn`, SD = `r filter(overallSumms, Cost == "Physical")$sdEarn`; *p* `r report_p(filter(earnTests, contrast == "Physical > Cognitive")$permPval)`, Cohen's D = `r filter(earnTests, contrast == "Physical > Cognitive")$cohenD`) and wait groups (mean = `r filter(overallSumms, Cost == "Wait")$mEarn`, SD = `r filter(overallSumms, Cost == "Wait")$sdEarn`; *p* `r report_p(filter(earnTests, contrast == "Cognitive < Wait")$permPval)`, Cohen's D = `r filter(earnTests, contrast == "Cognitive < Wait")$cohenD`). In addition, we found that the wait group earned more than the physical (*p* `r report_p(filter(earnTests, contrast == "Physical < Wait")$permPval)`, Cohen's D = `r filter(earnTests, contrast == "Physical < Wait")$cohenD`) and easy groups (mean Easy = `r filter(overallSumms, Cost == "Easy")$mEarn`, SD = `r filter(overallSumms, Cost == "Easy")$sdEarn`; *p* `r report_p(filter(earnTests, contrast == "Wait > Easy")$permPval)`, Cohen's D = `r filter(earnTests, contrast == "Wait > Easy")$cohenD`). The percentage of forced travels for cognitive effort was `r round(propFails[2, 2] * 100, digits = 2)`% across participants, and 0% for either physical or easy conditions, making performance an unlikely explanation for the observed differences in choice.
  
  We next examined the optimality of these decisions. The foraging task was configured so that there was a single reward-maximizing strategy per block type (see Operationalization of Cost in Methods), and participants were warned that accepting everything would not maximize their gains. Accordingly, participants who over- or under-accepted earned the least (Figure 3, right; general linear model with quadratic term, F = `r round(summary(lmEarn)$fstatistic[1], digits = 2)`, Beta = `r round(summary(lmEarn)$coefficients[3,1], digits = 2)`, SE = `r round(summary(lmEarn)$coefficients[3, 2], digits = 2)`, $R^2$ = `r round(summary(lmEarn)$r.squared, digits = 2)`, *p* `r report_p(summary(lmEarn)$coefficients[3, 4])`). Two-sided, one-sample t-tests against the optimal proportion of acceptances (i.e. $\mu$ = 0 or 1) for each combination of cost type, handling time, and offer showed that participants significantly deviated from optimality in `r round(optimalProp)`% of combinations (*p* < 0.05, FDR corrected), with the percentage of deviations being lowest for the wait group (Wait = `r filter(optimalProp_cost, Cost == "Wait")[2]`%, physical = `r filter(optimalProp_cost, Cost == "Physical")[2]`%, Cognitive = `r filter(optimalProp_cost, Cost == "Cognitive")[2]`%, Easy = `r filter(optimalProp_cost, Cost == "Easy")[2]`%). Together, these findings show that even though participants did not always perform optimally, they adequately integrated the statistics of the environment as they tracked their recent experience. Further, they suggest that cognitive effort can boost the value of an offer, as reflected by higher acceptances at the expense of earnings.

``` {r BTW: 16.2.2. rmANOVA, echo=FALSE,fig.align="center",fig.width=5,fig.height=4, results = 'asis'}
## This section was removed because it was deemed redundant. The interaction is addressed once the a priori mixed effects logistic model is evaluated against multiple forms of interaction effects (16.2.4.).
# temp <- dataBtw %>%
#         filter(Choice < 2) %>%
#         group_by(SubjID, Cost, Handling, Offer) %>%
#         summarise(pAccept = mean(Choice))
# 
# ## REPEATED MEASURES ANOVA
# # I think I like this the most
# prop.aov <- with(temp, aov(pAccept ~ factor(Cost) * Handling * Offer + 
#                                  Error(SubjID / (Handling * Offer))))
# 
# ## NOTES ON RESULTS
# # According to the logistic fit, there are main effects for all predictors, but no interactions. It also says that the overall odds for the wait group to accept versus the effort is around 16%. Similarly, increasing the handling time results in a decrease in the multiplicative odds of ~25%. Increments in reward increase the odds of acceptance by ~40%.
```

``` {r BTW: 16.2.2. rmANOVA plot, echo = FALSE, fig.align = "center", fig.width = 10, fig.height = 6}
# # code to calculate the rwd/sec rate for each acceptance threshold per handling (calculate the optimal solution)
# rwd <- c(4, 8, 20)
# handling <- c(14, 10, 2) + 2 # add 2 to H and T to account for offer and reward disclosure windows
# travel <- c(2, 6, 14) + 2
# rwdRates <- sapply(3:1, function(thresh) {(sum(tail(rwd, n = thresh))) / ((handling * thresh) + (travel * 3))}) 
# mean(apply(rwdRates, 1, max))
# 
# as_tibble(rwdRates) %>% 
#   mutate(Handling = handling) %>%
#   rename(Five = V1,
#          Eight = V2,
#          Twenty = V3) %>%
#   gather(Reward, Rate, -Handling) %>%
#   mutate(Reward = factor(Reward, levels = list("Five", "Eight", "Twenty")),
#          Handling = as.character(Handling)) %>%
#   group_by(Handling) %>%
#   mutate(Optimal = ifelse(Rate == max(Rate), max(Rate), -1)) %>%
#   ungroup() %>%
#   ggplot(aes(Reward, Rate, group = Handling, color = Handling)) +
#     geom_point(size = 3) +
#     geom_line(size = 1.5) +
#     geom_point(aes(Reward, Optimal), pch = 21, color = "black", fill = NA, size = 7, show.legend = F) +
#     scale_color_manual(values = c("purple", "grey50", "darkgoldenrod3")) +
#     ylim(0, 1.2) +
#     labs(y = "Expected reward per sec.", x = "Reward acceptance threshold") +
#     theme(legend.position = c(0.85, 0.2),
#           legend.key = element_blank(),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 22))

# and plot
hrPlot_btw <- dataBtw %>%
        filter(Choice < 2, Cost != "Easy") %>%
        group_by(SubjID, Cost, Handling, Offer, optimal) %>%
        summarise(pAccept = mean(Choice)) %>%
        group_by(Cost, Handling, Offer, optimal) %>%
        summarise(meanComplete = mean(pAccept),
                  SE = sd(pAccept) / sqrt(length(pAccept))) %>%
   ggplot(aes(interaction(Offer, Handling), meanComplete, color = Cost)) + 
        geom_point(size = 3) + 
        geom_errorbar(aes(ymin = meanComplete - SE, ymax = meanComplete + SE), width = 0.2, size = 1) +
        geom_line(aes(group = interaction(Handling, Cost)), size = 1) +
        geom_point(aes(y = round(optimal)), shape = 21, fill = "grey75", color = "grey30", size = 3, show.legend = F) +
        labs(x = "Offer.Handling", y = "Proportion Accepted") +
        scale_color_manual(values = colsBtw) +
        theme(legend.key = element_blank(),
              panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line = element_line(colour = "black"),
              text = element_text(size = 12))

# snippet on rmANOVA
# These intuitions were formally tested using a repeated measures ANOVA, whose results are presented in the table below. Overall, the analysis partially confirmed our predictions. While all main effects were significant, there was an unexpected significant condition-by-reward interaction, which could be due to the performance of the "easy" group relative to their peers. Importantly, we found that the interaction between all three main parameters was not significant, thus suggesting that the effects of handling and reward on choices were not different across groups.

```

``` {r BTW: 16.2.2. rmANOVA table, pander}
# # anova table
# x <- summary(prop.aov)   
# x <- x$`Error: Within`
# panderOptions("digits", 2)
# pander(x, style = "rmarkdown", split.table = 110)
```

``` {r BTW: 16.2.3. Mixed Effects Model, echo = FALSE, fig.align = "center", fig.width = 5, fig.height = 4, include = FALSE}
# clean the data so the total acceptances and quits per subj x handling x reward are ready for modeling
mixLogis_main_btw <- list()
mixData <- dataBtw %>%
  #filter(Choice < 2) %>% analysis performed on acceptances, not completions
  group_by(SubjID, Cost, Handling, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice)) %>%
  ungroup()


# model with a random intercept, then relevel the cost to get all pairwise comparisons
# the correct way to model a logistic with proportions as dependent vars is by providing n of hits and quits as a matrix
# the pre-registered version has random intercepts per subject: (1 | SubjID) - So, for the SubjID grouping, get a random intercept (1)
# or, for each subject, random intercept and slopes for handling and offer: (1 + Handling + Offer | SubjID)
# one can add random H + R slopes for each Cost group with: (0 + Handling + Offer | Cost)
# so far, the best qualitative and AIC-based fit comes from random HR slopes + intercept per subject, plus random HR slopes per cost group:  (1 + Handling + Offer | SubjID) + (0 + Handling + Offer | Cost)
# though that produces warnings too
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait"))
mixLogis_main_btw$Wait <- glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Cognitive"))
mixLogis_main_btw$Cognitive <- glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Physical"))
mixLogis_main_btw$Physical <- glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Easy"))
mixLogis_main_btw$Easy <- glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)

# matrix of coefficients and pvalues
rearrange <- c("Cognitive", "Easy", "Wait", "Physical") # to kinda match the order in Cost 3
mixSummary_btw <- betaMatrix(mixLogis_main_btw, rearrange = rearrange)
diag(mixSummary_btw$Pvals) <- NA


# plot coefficients and pvalues in a matrix
# invert colors
col2 <- colorRampPalette(c("#053061", "#2166AC", "#4393C3", "#92C5DE", "#D1E5F0", "#FFFFFF", "#FDDBC7", "#F4A582", "#D6604D", "#B2182B", "#67001F"))
# 
# corrplot(mixSummary_btw$Betas, 
#          is.corr = F, 
#          p.mat = mixSummary_btw$Pvals, 
#          type = "lower",
#          outline = T,
#          #insig = "p-value",
#          sig.level = 0.05, 
#          na.label = "square", 
#          na.label.col = "grey",
#          method = "color", 
#          tl.col = "black",
#          tl.cex = 0.8,
#          col = col2(200))

```

``` {r BTW: 16.2.3. Mixed Effects Model plot rand intercepts, echo = FALSE, fig.align = "center", fig.width = 5, fig.height = 4}
# # random intercepts for each subject, with cost group and overall acceptances
# RI <- ranef(mixLogis_main_btw$Wait)$SubjID[, 1]
# RI <- dataBtw %>% 
#   group_by(SubjID, Cost) %>% 
#   summarise(pAccept = mean(Choice)) %>% 
#   ungroup() %>%
#   mutate(RI = RI)
# 
# # plot relationship between random intercept (RI) and proportion accepted per cost group
# RI1 <- ggplot(data = RI, aes(RI, pAccept, color = Cost)) + 
#   geom_vline(xintercept = 0, linetype = "dashed") +
#   geom_smooth() + 
#   geom_point(alpha = 0.5) + 
#   scale_color_manual(values = colsBtw) +
#   theme_classic()
```

``` {r BTW: 16.2.4. Model comparison, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4}
# relevel, since the wait-reference model is used from the previous section
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait"))

## model fitting
mixLogis_compare_btw <- list()
mixLogis_compare_btw$Base <- glmer(cbind(totalAccepted, totalQuits) ~ 1 + (1 | SubjID), family = "binomial", data = mixData) # intercept only
mixLogis_compare_btw$Cost <- glmer(cbind(totalAccepted, totalQuits) ~ Cost + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare_btw$Handling <- glmer(cbind(totalAccepted, totalQuits) ~ Handling + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare_btw$Offer <- glmer(cbind(totalAccepted, totalQuits) ~ Offer + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare_btw$HR <- glmer(cbind(totalAccepted, totalQuits) ~ Offer + Handling + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare_btw$AllMain <- mixLogis_main_btw$Wait
mixLogis_compare_btw$HR_interaction <- glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer * Handling + (1 | SubjID), family = "binomial", data = mixData, control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))
mixLogis_compare_btw$Interaction <- glmer(cbind(totalAccepted, totalQuits) ~ (Cost + Handling + Offer)^2 + (1 | SubjID), family = "binomial", data = mixData)


## Model selection 
# AIC/BIC
ME_evals <- list()
ME_evals$abicLogis_compare <- data.frame(Model = names(mixLogis_compare_btw),
                                AIC = sapply(mixLogis_compare_btw, AIC),
                                BIC = sapply(mixLogis_compare_btw, BIC)) %>%
                                  mutate_at(vars(AIC:BIC), round)
  
# analysis of deviance for models with similar AIC
ME_evals$HRvMain <- anova(mixLogis_compare_btw$HR, mixLogis_compare_btw$AllMain)
ME_evals$MainvHRint <- anova(mixLogis_compare_btw$AllMain, mixLogis_compare_btw$HR_interaction)
ME_evals$MainvAllint <- anova(mixLogis_compare_btw$AllMain, mixLogis_compare_btw$Interaction)

# effect size for the main model
mixLogis_compare_btw$AllMainRsq <- 1 - (deviance(mixLogis_compare_btw$AllMain)/deviance(mixLogis_compare_btw$Base)) # denominator is the null deviance
```

``` {r BTW: 16.2.4. Plot Deviance comparison, echo = FALSE, fig.align = "center", fig.width = 6,fig.height = 4}
# plot AIC and BIC
ABIC_plot_btw <- ME_evals$abicLogis_compare %>% 
  gather(Comparison, Value, AIC:BIC) %>%
  mutate(Model = factor(Model, levels = names(mixLogis_compare_btw))) %>%
  ggplot(aes(Model, Value, fill = Comparison)) +
    geom_point(color = "black", pch = 21, size = 3) +
      theme(legend.position = c(0.8, 0.7),
            axis.text.x = element_text(angle = 45, hjust = 1),
            panel.grid.major = element_blank(),
            panel.grid.minor = element_blank(),
            panel.background = element_blank(),
            axis.line = element_line(colour = "black"),
            text = element_text(size = 22))
    
```

```{r mixed effects plots (Fig. 4), eval = !figsEnd, out.width = "80%", fig.align = "center", fig.cap = "Comparing acceptance rates among costs for each reward and handling combination. A: Proportion accepted by each group for every combination of handling time and reward. Grey dots indicate the reward-maximizing acceptance for each combination. B: Matrix portraying the coefficients (color scale) and significance that resulted from switching the reference cost condition (crosses mark non-significant comparisons). Each entry shows how much more likely a group was to accept compared to the reference condition (row). C: AIC and BIC values for a forward mixed-effects model comparison, ranging from a base intercept-only to a full-interaction model."}
#hrPlot_btw / (ABIC_plot_btw | ME_matrix_btw) # doesn't work with corrplot.. oh well
include_graphics("./Images/FIG4.png")
```

  We next probed whether these results held uniformly across all parameter combinations. Figure 4A shows the mean proportion of acceptances (± SEM) per combination of handling time, reward, and cost, with optimal acceptance rates depicted by the gray circles. Qualitatively, the figure confirms two important predictions. First, participants adapted to the richness of the each timing block, gravitating towards optimality regardless of the cost they faced. Second, differences in cost were consistent across blocks, and acceptances were uniformly highest for the cognitive effort group. We tested these observations with a mixed-effects logistic regression, computing the probability of accepting an offer as a function of handling time, reward amount, and demand type (see Methods for details). As before, the model showed significant main effects of handling time (global intercept = `r round(fixef(mixLogis_main_btw$Wait)["(Intercept)"], digits = 2)`, $\beta$ = `r round(fixef(mixLogis_main_btw$Wait)["Handling"], digits = 2)`, SE = `r round(summary(mixLogis_main_btw$Wait)[[10]]["Handling", "Std. Error"], digits = 2)`, *p* `r report_p(summary(mixLogis_main_btw$Wait)[[10]]["Handling", "Pr(>|z|)"])`) and offer amount ($\beta$ = `r round(fixef(mixLogis_main_btw$Wait)["Offer"], digits = 2)`, SE = `r round(summary(mixLogis_main_btw$Wait)[[10]]["Offer", "Std. Error"], digits = 2)`, *p* `r report_p(summary(mixLogis_main_btw$Wait)[[10]]["Offer", "Pr(>|z|)"])`). Comparisons among all conditions are shown on Figure 4B. The matrix of coefficients conveys that the cognitive effort group was significantly more likely to accept any offer than the physical ($\beta$ = `r round(mixSummary_btw$Betas["Physical", "Cognitive"], digits = 2)`, *p* `r report_p(mixSummary_btw$Pvals["Physical", "Cognitive"])`) and wait groups ($\beta$ = `r round(mixSummary_btw$Betas["Wait", "Cognitive"], digits = 2)`, *p* `r report_p(mixSummary_btw$Pvals["Wait", "Cognitive"])`), but not the easy group.
  
  These observations could not be better explained by simpler or more complex regression models. Figure 4C shows AIC and BIC values for models of increasing complexity, starting with an intercept-only configuration (see Methods for details). Here we focus on AIC, as both metrics yielded comparable results. As predicted, the a priori model with all main effects performed better than those relying on a single parameter (a priori model $R^2$ = `r round(mixLogis_compare_btw$AllMainRsq, digits = 2)`), as well as the model with main effects for handling time and reward amount (AIC~a-priori~ = `r filter(ME_evals$abicLogis_compare, Model == "AllMain")$AIC`, AIC~handling/reward~ = `r filter(ME_evals$abicLogis_compare, Model == "HR")$AIC`, $\chi^2$(`r ME_evals$HRvMain[[7]][2]`) = `r round(ME_evals$HRvMain$Chisq[2], digits = 2)`, *p* `r report_p(ME_evals$HRvMain[[8]][2])`). However, against our expectations, the a priori model was outperformed by models that added an interaction between handling and reward (AIC~HR_interaction~ = `r filter(ME_evals$abicLogis_compare, Model == "HR_interaction")$AIC`, $\chi^2$(`r ME_evals$MainvHRint[[7]][2]`) = `r round(ME_evals$MainvHRint$Chisq[2], digits = 2)`, *p* `r report_p(ME_evals$MainvHRint[[8]][2])`) as well as one that considered all possible interactions (AIC~all_interactions~ = `r filter(ME_evals$abicLogis_compare, Model == "Interaction")$AIC`, $\chi^2$(`r ME_evals$MainvAllint[[7]][2]`) = `r round(ME_evals$MainvAllint$Chisq[2], digits = 2)`, *p* `r report_p(ME_evals$MainvAllint[[8]][2])`). These improvements were driven by the fact that differences in offer acceptances among costs were never present for 20 cents (all *p* values for interactions containing offer amount < 0.001). As the highest reward should always be accepted in prey foraging environments, we conclude that the most parsimonious explanation of choice behavior came from the a priori model containing all main effects. These results further support the hypotheses that handling and reward amounts were similarly integrated across groups, and that the absolute rate of acceptances was affected by the demand faced. However, they offer partial support for the hypothesis that effort would induce higher acceptance rates, which was only true for those confronted with cognitive effort.

### **Between-subjects: Consistency and stability**


``` {r BTW: 16.3.1. surv and RTs, echo = FALSE, warning = FALSE, fig.align = "center", fig.width = 10, fig.height = 4, include = FALSE}
# qualitative survival analysis
# this version of a survival curve forces all successful acceptances to have an RT of 14 (i.e. the longest handling time)
# that's to avoid jumps from 2s to 10s to 14s
# 1 - Choice is to reverse the coding of choices, since the survival package assumes 0 = censor?
temp <- dataBtw %>%
    #filter(rawChoice < 2) %>% 
    mutate(#RT = ifelse(Choice == 1, 14, RT),
           Choice = 1 - Choice) 

survData <- survfit(Surv(RT, Choice) ~ Cost, data = temp)

survPlot_btw <- autoplot(survData) +
  geom_vline(xintercept = 1, lty = 2) + # end of the grace period for physical trials
  scale_color_manual(values = colsBtw) +
  scale_fill_manual(values = colsBtw) +
  scale_x_continuous(breaks = seq(14)) +
  ylim(0, 1) +
  labs(x = "Time in Trial", y = "Proportion Accepted", fill = "Cost", color = "Cost") +
  theme(legend.position = "none",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        text = element_text(size = 14))

# supplemental
# RT ecdfs for the whole group across blocks, with the option to subdivide by cost group
RTplot_btw <- dataBtw %>%
  filter(Choice == 0,
         RT < 1) %>%
  ggplot(aes(RT, group = Block, color = Block)) +
    stat_ecdf() +
    scale_x_continuous(breaks = seq(0, 1, length.out = 3), labels = c(0, 0.5, 1)) +
    labs(y = "Proportion of Quits Before Time X", x = "Quitting Time (seconds)") +
    facet_wrap(vars(Cost)) +
    theme(#legend.position = c(0.9, 0.7),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      panel.background = element_blank(),
      axis.line = element_line(colour = "black"),
      text = element_text(size = 14))

```

``` {r BTW: 16.3.2. Pre/Post Break, echo = FALSE, fig.align = "center", fig.width = 5, fig.height = 5}
# # divide the data into proportion accepted before/after the break (one per column)
# temp <- dataBtw %>%
#   filter(Choice < 2) %>%
#   group_by(SubjID, Cost, Half) %>%
#   summarise(pAccepted = mean(Choice)) %>%
#   ungroup() %>%
#   spread(Half, pAccepted)
#
# # separate by cost type and compute a linear model to predict half 2 from 1
# sep <- temp %>% plyr::dlply("Cost", identity)
# prepost <- list()
# prepost$LM <- lapply(sep, function(data) {lm(data$Half_2 ~ data$Half_1)})
# 
# # get the coefficients and confidence intervals
# # and concatenate
# prepost$summary <- as.data.frame(cbind(t(sapply(prepost$LM, coefficients)), t(sapply(prepost$LM, confint))))
# colnames(prepost$summary) <- c("Intercept", "Coefficient", "Int_CI-low", "Int_CI-high", "Beta_CI-low", "Beta_CI-high")

## PRE/POST PAIRED PERMUTATIONS
temp <- dataBtw %>%
  group_by(SubjID, Cost) %>%
  summarise(propAccept_1 = mean(Choice[Half == "Half_1"]),
            propAccept_2 = mean(Choice[Half == "Half_2"])) %>%
  plyr::dlply("Cost", identity) 


# paired perms and effect sizes
prepost_btw <- list()
prepost_btw$CohenD <- sapply(temp, function(data) {DescTools::CohenD(data$propAccept_1, data$propAccept_2)})
prepost_btw$Perms <- sapply(temp, function(data) {permute(data$propAccept_1, data$propAccept_2, paired = T, simple = T)})
```

``` {r BTW: 16.3.2. Plot Pre/Post, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 4}
# scatter plot of pre/post acceptances
# first calculate the mean acceptances per cost/half to draw horizontal and vertical lines on the plot
df_mean <- dataBtw %>%
            group_by(SubjID, Cost, Block, Half) %>%
            summarise(propAccept = mean(Choice)) %>%
            group_by(SubjID, Cost, Half) %>%
            summarise(propAccept = mean(propAccept)) %>%
            spread(Half, propAccept) %>%
            group_by(Cost) %>%
            summarise(propAccept_1 = mean(Half_1),
                      propAccept_2 = mean(Half_2))


# and plot
prepost_btw$scatterplot <- dataBtw %>%
                        filter(Choice < 2) %>%
                        group_by(SubjID, Cost) %>%
                        summarise(propAccept_1 = mean(Choice[Half == "Half_1"]),
                                  propAccept_2 = mean(Choice[Half == "Half_2"])) %>%
                        ggplot(aes(propAccept_1, propAccept_2, fill = Cost)) +
                          geom_point(size = 3, pch = 21, color = "black", show.legend = F) +
                          scale_fill_manual(values = colsBtw) +
                          scale_color_manual(values = colsBtw) +
                          xlim(0, 1) +
                          ylim(0, 1) +
                          labs(x = "Proportion Accepted - 1st Half", y = "Proportion Accepted - 2nd Half") +
                          geom_abline(slope = 1, intercept = 0, lty = 2) +
                          geom_hline(data = df_mean, aes(yintercept = propAccept_2, color = Cost), linetype="dashed", show.legend = F) +
                          geom_vline(data = df_mean, aes(xintercept = propAccept_1, color = Cost), linetype="dashed", show.legend = F) +
                          theme(panel.grid.major = element_blank(),
                                panel.grid.minor = element_blank(),
                                panel.background = element_blank(),
                                axis.line = element_line(colour = "black"),
                                text = element_text(size = 14))

# acceptances per cost across blocks
# note that because the same handling/travel time combos repeat in the same order pre/post, 1/4-2/4-3/6 are direct comparisons
prepost_btw$propBlocks <- dataBtw %>%
                         filter(Choice < 2) %>%
                         group_by(SubjID, Cost, Block) %>%
                         summarise(propAccept_subj = mean(Choice)) %>%
                         group_by(Cost, Block) %>%
                         summarize(propAccept = mean(propAccept_subj),
                                   seAccept = sd(propAccept_subj)/sqrt(nSubjs_btw)) %>%
                         ggplot(aes(Block, propAccept, color = Cost)) +
                           geom_vline(xintercept = 3.5, linetype = "dashed") +
                           geom_point(size = 2) +
                           geom_errorbar(aes(ymin = propAccept - seAccept, ymax = propAccept + seAccept, color = Cost), width = 0.1) +
                           geom_line() +
                           ylim(0, 1) +
                           labs(y = "Proportion Acccepted") +
                           scale_x_continuous(breaks = seq(6)) +
                           scale_color_manual(values = colsBtw) +
                           scale_fill_manual(values = colsBtw) +
                           theme(legend.key = element_blank(),
                                 legend.position = c(0.8, 0.3),
                                 panel.grid.major = element_blank(),
                                 panel.grid.minor = element_blank(),
                                 panel.background = element_blank(),
                                 axis.line = element_line(colour = "black"),
                                 text = element_text(size = 14))


# RTs for the cognitive tasks as a function of handling time, offer, and experimental half
prepost_btw$cogtaskRTs <- dataBtw_coglogs %>%
  filter(Choice == 1,
         Handling > 2) %>%
  group_by(SubjID, Handling, Offer, Trial_Time, Half) %>%
  summarise(mRT = mean(Task.RT)) %>%
  group_by(Handling, Offer, Trial_Time, Half) %>%
  summarise(meanRT = mean(mRT),
            seRT = sd(mRT, na.rm = T) / sqrt(21)) %>%
  ungroup() %>%
  mutate(Offer = factor(Offer, levels = c(4, 8, 20))) %>% 
  ggplot(aes(Trial_Time, meanRT, group = Offer, color = Offer)) +
    geom_line() +
    geom_line(aes(Trial_Time, meanRT + seRT, group = Offer, color = Offer), linetype = "dashed") +
    geom_line(aes(Trial_Time, meanRT - seRT, group = Offer, color = Offer), linetype = "dashed") +
    geom_point(size = 2) +
    ylim(0.2, 1) +
    facet_wrap(vars(Handling, Half)) +
    theme_classic()


```

``` {r BTW: 16.3.2. prepost lm table, pander}
# panderOptions("digits", 2)
# pander(prepost$summary, style = "rmarkdown", split.table = 110)
```

  Next, we tested the hypothesis that foragers would display consistent and stable choices. Participants were deemed consistent if they did not quit after engaging in a trial. To examine this, we plotted survival curves of trial quits across all participants (Figure 5, top-left), with black crosses signaling either completed trials (at 2 s, 10 s, and 14 s) or forced travels. These curves show that most decisions to quit happened within the first second into the handling time, and participants rarely quit during the handling time. Physical and easy groups showed slightly lagged responses because the experiment allowed a one-second grace period for participants to begin gripping (marked by the dashed line). This resulted in some individuals opting to wait for that second to indicate an offer rejection (the censor at 4s in the physical group was a completion during a 2 s handling trial that was incorrectly recorded as 4 s by the experimental code). Plotting the quitting time distributions across blocks for the first second of the handling time (ECDFs on Figure 5, top-right) showed that participants in the wait and cognitive groups responded faster as the experiment progressed. These response time observations suggested clarity in participant choices
  
```{r Survival and prepost plots (Fig. 5), eval = !figsEnd, fig.width = 8, fig.height = 7, fig.align = "center", fig.cap = "Top: Survival curve (left), and distribution of times when participants quit a trial over time. Bottom: Proportion accepted in the second half as a function of first half acceptances (left). Dashed lines indicate the mean for each group. On the right, the proportion accepted over time per group. The order of blocks was repeated in the second half (indicated by the black dashed line)."}
# plots generated in 16.3.1. and 16.3.2.
(suppressWarnings(survPlot_btw) | RTplot_btw) / (prepost_btw$scatterplot | prepost_btw$propBlocks)
```

  Stability was defined as a persistence in acceptance patterns throughout the experimental session. The scatter plot on Figure 5 (bottom-left) shows the overall proportion accepted by each participant before and after the mid-point break. Paired permutations showed no differences in acceptances for wait and easy groups (*p* > 0.05), but we found significantly lower acceptances for physical (*p* `r report_p(prepost_btw$Perms[1])`, Cohen's D = `r round(prepost_btw$CohenD[1], digits = 2)`) and cognitive (*p* `r report_p(prepost_btw$Perms[2])`, Cohen's D = `r round(prepost_btw$CohenD[2], digits = 2)`) groups. Figure 6 (left) shows that the reduction acceptances in effort groups over time was steeper for longer handling times, a hallmark of effortful demands [@Treadway2009]. We then replicated the mixed-effects logistic regression from the previous section separately on each experimental half, which confirmed that differences among demands were stable (Figure 6, right). Together, the consistency and stability displayed by participants suggest that decision makers had a clear representation of the environment.

``` {r BTW: 16.3.2. Plot Pre/Post reward x handling, echo=FALSE, fig.align = "center", fig.width = 8, fig.height = 4}
# this plot divides the proportion of acceptances for each reward/handling combination per half of the experiment
hrprepostPlot_btw <-dataBtw %>%
  group_by(SubjID) %>%
  mutate(Half = ifelse(Half == "Half_1", "First Half", "Second Half")) %>%
  group_by(SubjID, Half, Handling, Cost, Offer) %>%
  summarise(pAccept = mean(Choice)) %>%
  group_by(Half, Cost, Handling, Offer) %>%
  summarise(propAccept = mean(pAccept),
            SE = sd(pAccept) / sqrt(nSubjs_btw)) %>%
  ungroup() %>%
  mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
  ggplot(aes(Offer, propAccept, color = Cost)) +
    geom_point(size = 3, show.legend = T) +
    geom_line(size = 1, show.legend = F) +
    scale_color_manual(values = colsBtw) +
    scale_fill_manual(values = colsBtw) +
    geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.3, size = 1, show.legend = T) +
    scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1)) +
    scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
    labs(y = "Proportion Accepted") +
    facet_wrap(vars(Handling, Half), ncol = 2) +
    theme(legend.position = "bottom",
          strip.text.x = element_blank(),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          panel.background = element_blank(), 
          axis.line = element_line(colour = "black"),
          text = element_text(size = 16))


```

``` {r BTW: 16.3.2. mix pre/post separately, echo = FALSE, fig.align="center", fig.width = 5, fig.height = 4}
## compute the same mixed effects logistic for each half
# mixed logistic for the first half
mixLogis_pre_btw <- list()
mixData <- dataBtw %>%
  filter(Half == "Half_1") %>%
  group_by(SubjID, Cost, Handling, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_pre_btw$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait"))
mixLogis_pre_btw$Wait <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Easy"))
mixLogis_pre_btw$Easy <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Cognitive"))
mixLogis_pre_btw$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)

# mixed logistic for the second half
mixLogis_post_btw <- list()
mixData <- dataBtw %>%
  filter(Half == "Half_2") %>%
  group_by(SubjID, Cost, Handling, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_post_btw$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait"))
mixLogis_post_btw$Wait <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Easy"))
mixLogis_post_btw$Easy <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Cognitive"))
mixLogis_post_btw$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (1 | SubjID), family = "binomial", data = mixData)


## now produce a summary matrix
# get the beta and pvalue matrices 
betasPre <- betaMatrix(mixLogis_pre_btw, rearrange = rearrange)
betasPost <- betaMatrix(mixLogis_post_btw, rearrange = rearrange)

# and combine them
betaMat <- matrix(NA, nrow = nrow(betasPre$Betas), ncol = nrow(betasPre$Betas))
betaMat[lower.tri(betaMat)] <- betasPre$Betas[lower.tri(betasPre$Betas)]
betaMat[upper.tri(betaMat)] <- betasPost$Betas[lower.tri(betasPost$Betas)]
dimnames(betaMat) <- list(rearrange, rearrange)

pvalMat <- matrix(NA, nrow = nrow(betasPre$Pvals), ncol = nrow(betasPre$Pvals))
pvalMat[lower.tri(pvalMat)] <- betasPre$Pvals[lower.tri(betasPre$Pvals)]
pvalMat[upper.tri(pvalMat)] <- betasPost$Pvals[lower.tri(betasPost$Pvals)]
dimnames(pvalMat) <- list(rearrange, rearrange)

# remove uninteresting comparisons 
diag(betaMat) <- 0

# # aand plot
# corrplot(betaMat, 
#          is.corr = F, 
#          p.mat = pvalMat, 
#          outline = T,
#          #insig = "p-value",
#          sig.level = 0.05, 
#          na.label = "square", 
#          na.label.col = "grey",
#          method = "color", 
#          tl.col = "black",
#          tl.cex = 0.8,
#          col = col2(200))
````

```{r Prepost mixed effects plot (Fig. 6), eval = !figsEnd, fig.align = "center", fig.cap = "Distilling acceptance behavior before and after the break. Left: Proportion of acceptances per handling, reward, and half, showing that effortful demands became increasingly aversive over time. Right: Mixed-effects coefficients denoting the comparison among costs acceptances for each half separately (mirrored along the diagonal). Even though acceptances steadily decreased in the effort groups, the relative preferences were preserved."}
include_graphics("./Images/FIG6.png")

```




### **Within-subjects: Tests of whether decision makers integrate reward information**

  If the cost or value of a demand were intrinsic, the relative preferences displayed so far should have been robust to the ability to obtain the same rewards through different demands. We investigated this possibility by modifying the foraging task, such that every participant experienced every type of demand in blocks that interleaved wait trials with either physical or cognitive effort (fixed at 10 s handling and 6 s travel times, to allow for comparisons between experiments). We refer to observations from each block's wait trials by their effort pair (i.e. "Wait-C" for cognitive, and "Wait-P" for physical). We hypothesized that 1) reward decision patterns would match those of the previous experiment; and 2) that interleaving effort and delay would produce persistent preferences for wait trials.
	
``` {r WTH: 16.1.1. reward coefficients, fig.align = "center", fig.width = 4, fig.height = 4, echo = FALSE}
# adapt the data so the fits are only performed on choices, not failed attempts (revisit?)
dataList <- dataWth %>% 
  plyr::dlply("SubjID", identity)

# fits
logisticFits_base <- lapply(dataList, function(data) {suppressWarnings(glm(Choice ~ Offer, data = data, family = "binomial"))})

# Summarize
baseCoeffs <- as.data.frame(t(sapply(logisticFits_base, "[[", "coefficients")))
baseCoeffs$ConvIters <- sapply(logisticFits_base, function(x) {summary(x)$iter})
baseCoeffs$R2 <- sapply(logisticFits_base, function(x) {1 - (summary(x)$deviance / summary(x)$null.deviance)})
baseCoeffs$aic <- sapply(logisticFits_base, function(x) {summary(x)$aic})
baseCoeffs$propAccept_total <- sapply(dataList, function(data) {mean(data$Choice)})
baseCoeffs$propAccept_totalSQRD <- baseCoeffs$propAccept_total^2
baseCoeffs$totalEarned <- (dataWth %>% group_by(SubjID) %>% summarize(totalEarned = sum(Offer[Choice == 1])))$totalEarned
baseCoeffs$SubjID <- subjList_wth # add subject list column to join the demographics by it below

# # plot to match the one in Cost 2, even though we only have offer as an influence
# ggplot(data = baseCoeffs, aes(x = "Offer", y = Offer)) +
#   geom_boxplot() +
#   ylim(-10, 10) +
#   labs(x = "", y = "Coefficients") +
#   geom_hline(yintercept = 0, linetype = "dashed") +
#   theme(legend.position = c(0.9, 0.7),
#         panel.grid.major = element_blank(), 
#         panel.grid.minor = element_blank(), 
#         panel.background = element_blank(), 
#         axis.line = element_line(colour = "black"),
#         text = element_text(size = 16))

```

``` {r WTH: 16.1.2. AR + sequential effects, echo = FALSE, include = FALSE}
# tracing the history beyond 7 trials produces non-convergence
historySize_wth <- lapply(seq(2, 7), function(tail) {
  dat <- dataWth %>%
    group_by(SubjID) %>%
    mutate(recentTravels = seqQuits(Completed),
           recentOffers = ifelse(TrialN < (tail + 1), Offer, Reduce(`+`, shift(Offer, 1:tail)))) 
  
  glmer(Choice ~ Offer + recentTravels + recentOffers + (1 | SubjID), data = dat, family = "binomial")
})

# then get the summary of the tail length that reduced the AIC the most
historyIndex_wth <- which(sapply(historySize_wth, function(iter) {AIC(iter)}) == min(sapply(historySize_wth, function(iter) {AIC(iter)})))

historyCoeffs_wth <- round(summary(historySize_wth[[historyIndex_wth]])$coefficients, digits = 3)


# old
# # Create an autoregressive predictor based on previous consecutive quits
# dataList <- lapply(dataList, function(data) {data %>% mutate(AR = seqQuits(Choice))})
# 
# # fits per participant
# logisticFits_base <- lapply(dataList, function(data) {suppressWarnings(glm(Choice ~ Offer + AR, data = data, family = "binomial"))}) 
# 
# # get the low/high 95% CIs for AR
# baseCoeffs <- as_tibble(t(sapply(logisticFits_base, function(data) {suppressWarnings(confint(data, "AR"))}))) %>% 
#   dplyr::rename(AR_CILow = `2.5 %`, AR_CIHigh = `97.5 %`) %>%
#   mutate(SubjID = subjList_wth) %>%
#   left_join(baseCoeffs, by = "SubjID")
# 
# # reorder so that SubjID comes first
# #baseCoeffs <- baseCoeffs[ , c(3,4,5,6,7,8,9,1,2)]
# 
# # create a vector indicating whether 0 is included or not (i.e. if there is 0, no sequential effect)
# # but first change NA to 0 (visual inspection showed that NAs were tied to p's with full or near full p(accept), meaning they weren't influenced by history)
# baseCoeffs <- baseCoeffs %>% 
#   mutate(AR_CILow = ifelse(is.na(AR_CILow), 0, AR_CILow),
#          AR_CIHigh = ifelse(is.na(AR_CIHigh), 0, AR_CIHigh))
# baseCoeffs$AR_effect <- apply(baseCoeffs, 1, function(x) (!(as.numeric(x["AR_CILow"]) <= 0 & as.numeric(x["AR_CIHigh"]) >= 0)))
```

``` {r WTH: 16.1.3. prop accept x earnings, echo = FALSE, include = T, fig.align="center",fig.width = 4, fig.height = 4}
# Notes: I also performed this comparing halves and block types x half, and it doesn't look like participants were accepting less to make more.
# calculate the proportions and earnings per subject, while keeping group info
temp <- dataWth %>%
  filter(rawChoice < 2) %>%
  group_by(SubjID) %>%
  summarise(Earnings = sum(Offer[Choice == 1] / 100),
            pComplete = mean(Choice)) %>%
  mutate(pComplete2 = pComplete^2) %>%
  ungroup()

# linear fit of earnings x prop accept
# adding a regressor for half shows no diffs in earnings
lmEarn <- lm(Earnings ~ pComplete + pComplete2, data = temp)

# plot
earnFitplot_wth <- ggplot(data = temp, aes(pComplete, Earnings)) + 
         scale_color_manual(values = colsWth) + 
         geom_point(pch = 21, color = "black", size = 3, show.legend = T) + 
         stat_smooth(aes(y = Earnings), method = "lm", formula = y ~ x + I(x^2), color = "grey30") +
         labs(x = "Proportion Completed", y = "Total Earnings (dollars)") +
         scale_fill_manual(values = colsWth) +
         ylim(12, 16) +
         theme(panel.grid.major = element_blank(), 
           panel.grid.minor = element_blank(), 
           panel.background = element_blank(), 
           axis.line = element_line(colour = "black"),
           text = element_text(size = 16))

# extra analyses
# a pre-post paired permutation on earnigs confirms no differences (more evidence in section 16.3.3--prepost)
# this means that, at least in general, participants are not changing their behavior due to monetary incentives
# but because of something else, maybe like fatigue (this pattern holds even at finer subdivisions)
# PPearn <- dataWth %>%
#           #filter(Block %in% c(1,2,5,6)) %>%
#           mutate(Choice = ifelse(Choice > 1, 0, Choice)) %>%
#           group_by(SubjID, Half) %>%
#           summarise(Earnings = sum(Offer[Choice == 1] / 100),
#                     pComplete = mean(Choice)) %>%
#           ungroup() %>% 
#           select(SubjID, Half, Earnings) %>% 
#           spread(Half, Earnings)
# 
# permute(PPearn$Half_1, PPearn$Half_2, paired = TRUE)
```

``` {r WTH: 16.1.4. optimality, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4}
# get quits and acceptances per cost type 
temp <- dataWth %>%
  group_by(Cost) %>%
  filter(Offer < 20) %>%
  summarize(propAccept = mean(Choice),
            accept = sum(Choice),
            quit = sum(Choice == 0))

# perform a prop.test based on the group's acceptances and quits per cost
temp2 <- apply(temp, 1, function(row) {prop.test(as.numeric(row[3]), sum(as.numeric(row[3:4])), p = 0.5, conf.level = 0.95)})
  
  
# store stats in a dataframe and round up to the second digits for reporting
summary_costvsreward <- data.frame(probability = sapply(temp2, "[[", "estimate"),
                                   chisquared = sapply(temp2, "[[", "statistic"),
                                   confint = t(sapply(temp2, "[[", "conf.int")),
                                   pval = sapply(temp2, "[[", "p.value")) %>% round(., digits = 2)
rownames(summary_costvsreward) <- temp$Cost

rm(temp2)


## no differences in earnings
earnings_within <- dataWth %>%
  group_by(SubjID, Cost) %>%
  filter(rawChoice == 1) %>%
  summarise(earning = sum(Offer) / 100) %>%
  group_by(Cost) %>%
  summarise(mEarn = mean(earning),
            sdEarn = sd(earning)) %>%
  mutate_if(is.numeric, round, digits = 2)


```

  Analyses from the previous experiment were repurposed to examine cost differences within individuals. We started by testing the hypothesis that participants would successfully integrate the statistics of the environment. A mixed-effects logistic regression once again showed that larger reward amounts significantly increased acceptance probabilities ($\beta$ = `r historyCoeffs_wth["Offer", "Estimate"]`, SE = `r historyCoeffs_wth["Offer", "Std. Error"]`, *p* `r report_p(historyCoeffs_wth["Offer", "Pr(>|z|)"])`), and that participants became more selective if they had recently observed large offers ($\beta$ = `r historyCoeffs_wth["recentOffers", "Estimate"]`, SE = `r historyCoeffs_wth["recentOffers", "Std. Error"]`, *p* `r report_p(historyCoeffs_wth["recentOffers", "Pr(>|z|)"])`). Model comparisons suggested that participants were influenced by the `r historyIndex_wth + 1` most recent rewards (AIC = `r round(AIC(historySize_wth[[historyIndex_wth]]), digits = 2)`). However, this time participants were unaffected by recent reward misses (*p* > 0.05).
  As before, a linear model predicting earnings showed that participants who completed too few or too many trials earned the least, in line with the experimental design (Figure 7A; general linear model with quadratic term, F = `r round(summary(lmEarn)$fstatistic[1], digits = 2)`, $\beta$ = `r round(summary(lmEarn)$coefficients[3,1], digits= 2)`, SE = `r round(summary(lmEarn)$coefficients[3, 2], digits = 2)`, $R^2$ = `r round(summary(lmEarn)$r.squared, digits = 2)`, *p* `r report_p(summary(lmEarn)$coefficients[3, 4])`). When assessing the optimality of acceptances for 4 and 8 cents (99% of 20 cent offers were correctly accepted), chi-squared tests against the optimal rate of 50% showed that participants under-accepted physical effort trials (proportion accepted = `r round(summary_costvsreward["Physical", "probability"], digits = 2)`, $\chi^2$ = `r round(summary_costvsreward["Physical", "chisquared"], digits = 2)`, *p* `r report_p(summary_costvsreward["Physical", "pval"])`; all other *p* > 0.05). Despite this deviation, participants earned a similar total amount per cost on average (mean Cognitive = `r filter(earnings_within, Cost == "Cognitive")$mEarn`, SD = `r filter(earnings_within, Cost == "Cognitive")$sdEarn`; mean Wait-C = `r filter(earnings_within, Cost == "Wait-C")$mEarn`, SD = `r filter(earnings_within, Cost == "Wait-C")$sdEarn`; mean Physical = `r filter(earnings_within, Cost == "Physical")$mEarn`, SD = `r filter(earnings_within, Cost == "Physical")$sdEarn`; mean Wait-P = `r filter(earnings_within, Cost == "Wait-P")$mEarn`, SD = `r filter(earnings_within, Cost == "Wait-P")$sdEarn`). These results convey that participants in both experiments were similarly influenced by the characteristics of the environment, although participants in the present experiment gravitated more towards optimality.
  


### **Within-subjects: Comparisons among the four delay and effort conditions**

``` {r WTH: 16.2.1. Overall Proportion diffs, fig.align = "center", fig.width = 6, fig.height = 4, echo = FALSE}  
# ## ECDF of proportion completed per cost type
# temp <- dataWth %>%
#         filter(Choice < 2) %>%
#         #mutate(Cost = factor(Cost, levels = list("Physical", "Cognitive", "Wait_0", "Wait_1"))) %>%
#         group_by(SubjID, Cost) %>%
#         summarise(pComplete = mean(Choice))
# 
# 
# # plots
# (p2 <- ggplot(temp, aes(Cost, pComplete, fill = Cost)) + 
#         geom_boxplot(show.legend = F) +
#         scale_fill_manual(values = colsWth) +
#         ylim(c(0, 1)) +
#         labs(y = "Proportion Completed") +
#         theme(panel.grid.major = element_blank(), 
#             panel.grid.minor = element_blank(), 
#             panel.background = element_blank(), 
#             axis.line = element_line(colour = "black"),
#             text = element_text(size = 16)))

```

``` {r WTH: 16.2.1. mixed effect models, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 5}
# both methods yield similar outputs, at least coefficients and pvalues
# but the likelihoods will differ of course. The second one might be better in terms of computational needs.
# method 1
# mixLogis_main_wth <- list()
# mixLogis_main_wth$Cog <- glmer(Choice ~ Cost + Offer + (1 | SubjID), family = "binomial", data = data)

# mixed effects logistics, rotating the reference cost condition
mixLogis_main_wth <- list()
mixData <- dataWth %>%
  group_by(SubjID, Cost, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_main_wth$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait-C"))
mixLogis_main_wth$`Wait-C` <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait-P"))
mixLogis_main_wth$`Wait-P` <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Physical"))
mixLogis_main_wth$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)


# plot proportion accepted per trial/offer combo
hrPlot_wth <- dataWth %>%
        filter(Choice < 2) %>%
        group_by(Cost, Offer) %>%
        summarise(propAccept = mean(Choice),
                  SE = sd(Choice) / sqrt(nSubjs_wth)) %>%
        ungroup() %>%
        mutate(Optimal = rep(c(0, 1, 1), length(unique(dataWth$Cost))),
               #BlockType = as.factor(rep(c(0,1,0,1), each = 3)),
               Offer = ifelse(Offer == 20, 12, Offer)) %>%
        ggplot(aes(Offer, propAccept, color = Cost)) +
          geom_point(size = 3, show.legend = T) +
          geom_point(aes(y = Optimal), size = 3, pch = 21, fill = "grey75", color = "grey30") +
          geom_line(lwd = 1, show.legend = T) +
          scale_color_manual(values = colsWth) +
          scale_fill_manual(values = colsWth) +
          geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.4, size = 1) +
          scale_y_continuous(limits = c(0, 1), breaks = c(0,0.5,1)) +
          scale_x_continuous(breaks = c(4, 8, 12), labels = c("4", "8", "20")) +
          labs(y = "Proportion Accepted") +
          theme(legend.position = c(0.8, 0.25),
                legend.key = element_blank(),
                panel.grid.major = element_blank(),
                panel.grid.minor = element_blank(),
                panel.background = element_blank(),
                axis.line = element_line(colour = "black"),
                text = element_text(size = 22))

```

``` {r WTH: 16.2.1. mixed effects matrix, echo = FALSE, fig.align = "center", fig.width = 5,  fig.height = 4}
# get summary of coeffs
rearrange <- c("Cognitive", "Wait-C", "Wait-P", "Physical")
mixSummary_wth <- betaMatrix(mixLogis_main_wth, rearrange = rearrange)
diag(mixSummary_wth$Pvals) <- NA

# remove uninteresting comparisons
mixSummary_wth$Betas[rbind(c(3, 1), c(4,2))] <- NA
mixSummary_wth$Pvals[rbind(c(3, 1), c(4,2))] <- NA

# corrplot makes it difficult to use this in complex layouts, so the code has been commented out to save resources

# # invert colors
# col2 <- colorRampPalette(c("#053061", "#2166AC", "#4393C3", "#92C5DE", "#D1E5F0", "#FFFFFF", "#FDDBC7", "#F4A582", "#D6604D", "#B2182B", "#67001F"))

# corrplot(mixSummary$Betas, 
#          is.corr = F, 
#          p.mat = mixSummary$Pvals, 
#          type = "lower",
#          outline = T,
#          #insig = "p-value",
#          sig.level = 0.05, 
#          na.label = "square", 
#          na.label.col = "grey",
#          method = "color", 
#          tl.col = "black",
#          tl.cex = 0.8,
#          cl.length = 5,
#          cl.lim = c(-2.0001, 2.0001),
#          col = col2(200))
         
```

``` {r WTH: 16.2.2. ME model comparison, echo = FALSE, fig.align="center", fig.width = 6, fig.height = 4}
# fit ME logistic at increasing model complexities
mixLogis_compare_wth <- list()
mixLogis_compare_wth$Base <- glmer(cbind(totalAccepted, totalQuits) ~ 1 + (1 | SubjID), family = "binomial", data = mixData) # intercept only
mixLogis_compare_wth$Cost <- glmer(cbind(totalAccepted, totalQuits) ~ Cost + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare_wth$Reward <- glmer(cbind(totalAccepted, totalQuits) ~ Offer + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare_wth$AllMain <- mixLogis_main_wth$Cognitive
mixLogis_compare_wth$Interaction <- glmer(cbind(totalAccepted, totalQuits) ~ Cost * Offer + (1 | SubjID), family = "binomial", data = mixData)

# Model selection 
abicLogis_compare <- data.frame(Model = names(mixLogis_compare_wth),
                                AIC = sapply(mixLogis_compare_wth, AIC),
                                BIC = sapply(mixLogis_compare_wth, BIC)) %>%
                                  mutate_at(vars(AIC:BIC), round)


# plot 
mixLogis_compare_wth$plot <- abicLogis_compare %>% 
  gather(Comparison, Value, AIC:BIC) %>%
  mutate(Model = factor(Model, levels = names(mixLogis_compare_wth))) %>%
  ggplot(aes(Model, Value, fill = Comparison)) +
    geom_point(color = "black", pch = 21, size = 3) +
    theme(legend.position = c(0.8, 0.8),
      legend.key = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      panel.background = element_blank(),
      axis.line = element_line(colour = "black"),
      text = element_text(size = 16))

# analysis of deviance
mixLogis_compare_wth$aod <- anova(mixLogis_compare_wth$Reward, mixLogis_compare_wth$AllMain, mixLogis_compare_wth$Interaction)

# effect size
mixLogis_compare_wth$Rsq <- 1 - (deviance(mixLogis_compare_wth$AllMain)/deviance(mixLogis_compare_wth$Base)) # denominator is the null deviance
```

```{r Earnings/prop completed/ME plots (Fig. 7), eval = !figsEnd, fig.width = 8, fig.height = 7, fig.align = "center", fig.cap = "A: Quadratic relationship between trial completions and total earnings. B: proportion of acceptances of each reward per cost condition. C: Matrix of mixed-effects coefficients showing comparisons among costs (while controlling for reward amount). Comparisons between effort and wait trials from different blocks are omitted, as these are not considered in our original predictions. Crosses mark non-significant comparisons. D: AIC and BIC values for models of increasing complexity (a priori = AllMain)."}

include_graphics("./Images/FIG8.png")
```

  Next, we addressed the hypothesis that passive waiting trials would be less costly than effortful ones. Even though the reward acceptance pattern at 10 s handling was similar to the between-subjects experiment (Figure 7B), differences among cost conditions were less evident. Moreover, these differences contradicted our predictions, as cognitive effort remained the least costly among demands. A mixed effects logistic regression confirmed once again that increases in offer amount made acceptances significantly more likely (global intercept = `r round(fixef(mixLogis_main_wth$Cognitive)["(Intercept)"], digits = 2)`, $\beta$ = `r round(fixef(mixLogis_main_wth$Cognitive)["Offer"], digits = 2)`, SE = `r round(summary(mixLogis_main_wth$Cognitive)[[10]]["Offer", "Std. Error"], digits = 2)`, *p* `r report_p(summary(mixLogis_main_wth$Cognitive)[[10]]["Offer", "Pr(>|z|)"])`), but that the only significant difference was the higher likelihood to accept cognitive offers compared to physical ones (Figure 7C; $\beta$ = `r round(mixSummary_wth$Betas["Physical", "Cognitive"], digits = 2)`, *p* `r report_p(mixSummary_wth$Pvals["Physical", "Cognitive"])`). Model comparisons suggested that even though a model with all main effects explained a large portion of the variance ($R^2$ = `r round(mixLogis_compare_wth$Rsq, digits = 2)`), there were similar AIC and BIC values for any model including reward amount (Figure 7D). An analysis of deviance among the three models showed that adding cost condition to reward amount significantly improved fit (AIC~reward~ = `r filter(abicLogis_compare, Model == "Reward")$AIC`, AIC~a-priori~ = `r filter(abicLogis_compare, Model == "AllMain")$AIC`, $\chi^2$(`r mixLogis_compare_wth$aod[[7]][2]`) = `r round(mixLogis_compare_wth$aod$Chisq[2], digits = 2)`, *p* `r report_p(mixLogis_compare_wth$aod[[8]][2])`). However, probing the interaction between these features significantly improved fits from the main effects model (AIC~Interaction~ = `r filter(abicLogis_compare, Model == "Interaction")$AIC`, $\chi^2$(`r mixLogis_compare_wth$aod[[7]][3]`) = `r round(mixLogis_compare_wth$aod$Chisq[3], digits = 2)`, *p* `r report_p(mixLogis_compare_wth$aod[[8]][3])`), albeit at the cost of interpretability due to choice fluctuations over time (see Consistency and Stability). These initial results defied our predictions, suggesting that the availability of alternative demands erases preferences among them (rather than inverting them from what was observed in experiment 1).


### **Consistency and stability**

``` {r WTH: 16.3.1. surv and RT, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4, include = FALSE}
# Survival curve for quits and response time ECDFs across blocks
# 1 - Choice is to reverse the coding of choices, since the survival package assumes 0 = censor
# to note re: x-axis. In this exp, p's could quit within the 2s offer window. Cost 2 didn't allow that, so it counts from when trial begins.
temp <- dataWth %>%
    mutate(RT = ifelse(ResponseType != "OfferQuit", RT + 2, RT)) %>%
    #filter(rawChoice < 2) %>% 
    mutate(#RT = ifelse(Choice == 1, 14, RT),
           Choice = 1 - Choice) 

survData <- survfit(Surv(RT, Choice) ~ Cost, data = temp)

survPlot_wth <- autoplot(survData) +
  geom_vline(xintercept = 2, lty = 2) + # end of the grace period for physical trials
  scale_color_manual(values = colsWth) +
  scale_fill_manual(values = colsWth) +
  scale_x_continuous(breaks = seq(12)) +
  ylim(0, 1) +
  scale_x_continuous(breaks = seq(0, 12), labels = seq(-2, 10)) +
  labs(x = "Time in Trial", y = "Proportion Accepted", fill = "Cost", color = "Cost") +
  theme(legend.position = "none",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        text = element_text(size = 16))

# ecdf of RTs across blocks for all subjects combined
RTplot_wth <- dataWth %>%
  mutate(RT = ifelse(ResponseType != "OfferQuit", RT + 2, RT)) %>%
  mutate(Choice = 1 - Choice)  %>%
  filter(ResponseType == "OfferQuit") %>%
  ggplot(aes(RT, group = Block, color = Block)) +
    stat_ecdf() +
    labs(y = "Proportion of Quits Before Time X", x = "Quitting Time") +
    #facet_wrap(vars(Cost)) +
    theme(legend.position = c(0.8, 0.3),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      panel.background = element_blank(),
      axis.line = element_line(colour = "black"),
      text = element_text(size = 16))

```

``` {r WTH: 16.3.2. quits in offer vs handling, echo = FALSE}
# Proportion of forced travels per subject and cost type
propFails_wth <- dataWth %>%
  filter(!Cost %in% c("Wait-C", "Wait-P")) %>%
  group_by(SubjID, Cost) %>%
  summarise(propFails = mean(rawChoice == 2) * 100) 


# Proportion of quits once the trial started per subject
# ResponseType distinguished between offer quits and handling quits
propQuits_wth <- dataWth %>%
  filter(!(ResponseType %in% c("Forced Travel", "Forced travel", "Reward"))) %>%
  group_by(SubjID) %>%
  summarise(propAccept = mean(ResponseType == "Quit") * 100)

```

  Similar to the between-subject results, a survival analysis displayed that participants made most of their choices quickly within the offer window (Figure 8A). The median percentage of quits during the handling time was `r round(median(propQuits_wth$propAccept), digits = 2)`% (SD = `r round(sd(propQuits_wth$propAccept), digits = 2)`), with only `r sum(propQuits_wth$propAccept > 20)` participants quitting over 20% of trials during the handling time. Moreover, the cumulative quitting time distributions show that participant responses during the offer window became faster over time (Figure 8B). Once in the handling time, the overall percentage (**shift to percentage in exp 1**) of forced travels in the effortful trials was low for both cognitive (median = `r round(median(filter(propFails_wth, Cost == "Cognitive")$propFails), digits = 2)`%, SD = `r round(sd(filter(propFails_wth, Cost == "Cognitive")$propFails), digits = 2)`) and physical (median = `r round(median(filter(propFails_wth, Cost == "Physical")$propFails), digits = 2)`%, SD = `r round(sd(filter(propFails_wth, Cost == "Physical")$propFails), digits = 2)`) conditions. Similar to the between-subjects experiment, these results suggest that decision makers were consistent in their adopted strategies, and were able to perform the tasks well.

``` {r WTH: 16.3.3. pre-post, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 4, include = F}
# # restructure the data
# temp <- dataWth %>%
#   filter(!(Block %in% c(3, 4))) %>%
#   group_by(SubjID, Cost, Half) %>%
#   summarise(propAccepted = mean(Choice)) %>%
#   ungroup() %>%
#   spread(Half, propAccepted)
# 
# # compute the mixed effects model (linear for this)
# mixLogis_prepost <- lmer(Half_2 ~ Half_1 + Cost -1 + (1 | SubjID), data = temp)
# 
# # get the coefficients and confidence intervals
# t1 <- coefficients(mixLogis_prepost)$SubjID[1, 2:6]
# t2 <- as.data.frame(t(confint(mixLogis_prepost)[3:7,]))
# 
# # and concatenate them
# summary_prepost <- t(bind_rows(t1, t2))
# colnames(summary_prepost) <- c("Coefficient", "CI-low", "CI-high")
#
# # prepost half paired perms
# temp <- temp %>% plyr::dlply("Cost", identity) 
# prepost <- list()
# prepost$cohen <- sapply(temp, function(data) {DescTools::CohenD(data$Half_1, data$Half_2)})
# prepost$perm <- sapply(temp, function(data) {permute(data$Half_1, data$Half_2, paired = T, simple = T)})

## paired tests for first versus last block per cost type
## PRE/POST PAIRED PERMUTATIONS
temp <- dataWth %>%
  filter(!(Block %in% c(3, 4))) %>%
  group_by(SubjID, Cost) %>%
  summarise(propAccept_1 = mean(Choice[Half == "Half_1"]),
            propAccept_2 = mean(Choice[Half == "Half_2"])) %>%
  plyr::dlply("Cost", identity) 


# paired perms and effect sizes
prepost_wth <- list()
prepost_wth$CohenD <- sapply(temp, function(data) {DescTools::CohenD(data$propAccept_1, data$propAccept_2)})
prepost_wth$Perms <- sapply(temp, function(data) {permute(data$propAccept_1, data$propAccept_2, paired = T, simple = T)})


# correlation between the difference in acceptance rates and mistakes
accDiff <- dataWth %>%
  filter(!(Block %in% c(3, 4)), 
         Cost %in% c("Cognitive", "Physical")) %>%
  group_by(SubjID, Cost) %>%
  summarise(propAccept_1 = mean(Choice[Half == "Half_1"]),
            propAccept_2 = mean(Choice[Half == "Half_2"])) %>%
  mutate(Diff = propAccept_2 - propAccept_1)
  
corDiff <- cor(accDiff$Diff, propFails_wth$propFails)

```

``` {r WTH: 16.3.3. pre-post plots, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 4}
# various plots
# acceptances across blocks
prepost_wth$propBlocks <- dataWth %>%
    filter(Choice < 2) %>%
    group_by(SubjID) %>%
    mutate(Block = case_when(
      Block %in% c(1, 2) ~ 1,
      Block %in% c(3, 4) ~ 2,
      Block %in% c(5, 6) ~ 3
    )) %>%
    group_by(SubjID, Cost, Block) %>%
    summarise(propAccept_subj = mean(Choice)) %>%
    group_by(Cost, Block) %>%
    summarize(propAccept = mean(propAccept_subj),
              seAccept = sd(propAccept_subj)/sqrt(nSubjs_wth)) %>%
    ggplot(aes(Block, propAccept, color = Cost)) +
      geom_point(size = 2) +
      geom_errorbar(aes(ymin = propAccept - seAccept, ymax = propAccept + seAccept, color = Cost), width = 0.1) +
      geom_line() +
      ylim(0, 1) +
      labs(y = "Proportion Accepted") +
      scale_x_continuous(breaks = c(1, 2, 3)) +
      scale_color_manual(values = colsWth) +
      scale_fill_manual(values = colsWth) +
      theme(legend.key = element_blank(),
         legend.position = c(0.8, 0.3),
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank(),
         axis.line = element_line(colour = "black"),
         text = element_text(size = 16))

# Pre-post midpoint
# calculate mean per trial type to draw horizontal/vertical lines pre/post
df_mean <- dataWth %>%
            filter(Choice < 2, !(Block %in% c(3,4))) %>%
            group_by(SubjID, Cost) %>%
            summarise(pAccept_1 = mean(Choice[Half=="Half_1"]),
                      pAccept_2 = mean(Choice[Half=="Half_2"])) %>%
            group_by(Cost) %>%
            summarise(propAccept_1 = round(mean(pAccept_1), digits = 2),
                      sdAccept_1 = round(sd(pAccept_1), digits = 2),
                      propAccept_2 = round(mean(pAccept_2), digits = 2),
                      sdAccept_2 = round(sd(pAccept_2), digits = 2))

prepost_wth$scatterplot <- dataWth %>%
  filter(Choice < 2, !(Block %in% c(3,4))) %>%
  group_by(SubjID, Cost) %>%
  summarise(propAccept_1 = mean(Choice[Half=="Half_1"]),
            propAccept_2 = mean(Choice[Half=="Half_2"])) %>%
  ggplot(aes(propAccept_1, propAccept_2, fill = Cost)) +
    geom_point(size = 3, pch = 21, color = "black", show.legend = F) +
    labs(x = "Proportion Accepted - First Block", y = "Proportion Accepted - Last Block") +
    scale_fill_manual(values = colsWth) +
    scale_color_manual(values = colsWth) +
    xlim(0, 1) +
    ylim(0, 1) +
    geom_abline(slope = 1, intercept = 0, lty = 2) +
    geom_hline(data = df_mean, aes(yintercept = propAccept_2, color = Cost), linetype="dashed", show.legend = F) +
    geom_vline(data = df_mean, aes(xintercept = propAccept_1, color = Cost), linetype="dashed", show.legend = F) +
    theme(legend.key = element_blank(),
       legend.position = c(0.8, 0.25),
       panel.grid.major = element_blank(),
       panel.grid.minor = element_blank(),
       panel.background = element_blank(),
       axis.line = element_line(colour = "black"),
       text = element_text(size = 16))


prepost_wth$cogtaskRTs <- dataWth_coglogs %>%
  filter(Outcome > 0) %>%
  mutate(Outcome = ifelse(Outcome == 2, 0, Outcome)) %>%
  group_by(SubjID, Offer, Trial_Time, Half) %>%
  summarise(mRT = mean(RT),
            pComplete = mean(Outcome)) %>%
  group_by(Offer, Trial_Time, Half) %>%
  summarise(meanRT = mean(mRT),
            seRT = sd(mRT, na.rm = T) / sqrt(48),
            meanProp = mean(pComplete),
            seProp = sd(pComplete) / sqrt(48)) %>%
  ungroup() %>%
  mutate(Offer = factor(Offer, levels = c(4, 8, 20))) %>%
  ggplot(aes(Trial_Time, meanRT, group = Offer, color = Offer)) +
    geom_line() +
    geom_line(aes(Trial_Time, meanRT + seRT, group = Offer, color = Offer), linetype = "dashed") +
    geom_line(aes(Trial_Time, meanRT - seRT, group = Offer, color = Offer), linetype = "dashed") +
    geom_point(size = 2) +
    ylim(0.2, 1) +
    facet_wrap(vars(Half)) +
    theme_classic()

# ## PROPORTION OF COGNITIVE ACCEPTANCES NOT DIFFERENT BETWEEN FIRST HALF OF WITHIN AND BETWEEN EXPERIMENTS
# btw <- dataBtw %>%
#   filter(Handling == 10) %>%
#   group_by(SubjID, Cost) %>%
#   summarise(pAccept = mean(Choice)) %>%
#   plyr::dlply("Cost", identity)
# 
# wth <- dataWth %>%
#   filter(Block < 3) %>%
#   mutate(Cost2 = ifelse(Cost %in% c("Wait-P", "Wait-C"), "Wait", Cost)) %>%
#   group_by(SubjID, Cost2) %>%
#   summarise(pAccept = mean(Choice)) %>%
#   plyr::dlply("Cost2", identity)
# 
# 
# permute(btw$Cognitive$pAccept, wth$`1`$pAccept, simple = T)
# 
# permute(btw$Physical$pAccept, wth$`2`$pAccept, simple = T)
# 
# permute(btw$Wait$pAccept, wth$Wait$pAccept, simple = T)

# # wait vs effort across effort conditions
# pAll <- dataWth %>%
#   filter(Choice < 2) %>%
#   mutate(Cost2 = ifelse(!(Cost %in%  c("Wait-C", "Wait-P")), "EFFORT", "WAIT"),
#          BlockType = ifelse(gsub(".*_", "", Cost) == 1, "Physical", "Cognitive")) %>%
#   group_by(SubjID, BlockType) %>%
#   summarise(Wait_accept = mean(Choice[Cost2 == "WAIT"]),
#             Effort_accept = mean(Choice[Cost2 == "EFFORT"])) %>%
#   ggplot(aes(Wait_accept, Effort_accept, color = SubjID, shape = BlockType)) +
#     geom_point(size = 5, show.legend = T) +
#     xlim(0,1) +
#     ylim(0,1) +
#     geom_abline(slope = 1, intercept = 0, lty = 2) +
#     scale_color_manual(values = colsWth) +
#     scale_fill_manual(values = colsWth) +
#     theme_classic()

# # load the plots
# grid.arrange(mid, propBlocks, ncol = 2)


# # plot showing the difference in proportion of acceptances between first and last block, effort vs delay for each block type
# # lines connect the same subject between the blocks
# dataWth %>%
#   group_by(SubjID) %>%
#   mutate(Block = case_when(
#     Block %in% c(1, 2) ~ 1,
#     Block %in% c(3, 4) ~ 2,
#     Block %in% c(5, 6) ~ 3
#   )) %>%
#   #filter(Half == "Half_1", Offer < 20) %>%
#   filter(Block != 2) %>%
#   mutate(Block = ifelse(Block == 1, "First Block", "Last Block")) %>%
#   group_by(SubjID, Cost, Block, Offer) %>%
#   summarise(propAccept = mean(Choice)) %>%
#   ungroup() %>%
#   #mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
#   spread(Block, propAccept) %>%
#   mutate(Difference = `Last Block` - `First Block`) %>%
#   group_by(SubjID, Cost) %>%
#   summarise(Diffscore = sum(Difference)/3) %>%
#   mutate(Type = ifelse(Cost %in% c("Cognitive", "Physical"), "Effort", "Delay"),
#          Cost = ifelse(Cost %in% c("Cognitive", "Wait-C"), "Cognitive", "Physical")) %>%
#   spread(Type, Diffscore) %>%
#   ggplot(aes(Effort, Delay, group = SubjID, fill = Cost)) +
#     geom_hline(yintercept = 0) +
#     geom_vline(xintercept = 0) +
#     geom_path(show.legend = F, alpha = 0.5, linetype = "dashed") +
#     geom_point(show.legend = T, pch = 21, color = "black", size = 4) +
#     scale_fill_manual(values = colsWth) +
#     annotate("text", x = -0.4, y = 0.6, label = "Increased preference for delay over time", size = 5, color = "grey30") +
#     annotate("text", x = -0.5, y = -0.6, label = "All acceptances decreased", size = 5, color = "grey30") +
#     annotate("text", x = 0.5, y = 0.6, label = "All acceptances increased", size = 5, color = "grey30") +
#     annotate("text", x = 0.4, y = -0.6, label = "Increased preference for effort over time", size = 5, color = "grey30") +
#     labs(x = "P(Effort): Last - First Block", y = "P(Delay): Last - First Block", fill = "Block type") +
#     ylim(-0.7, 0.7) +
#     xlim(-0.7, 0.7) +
#     theme(legend.key = element_blank(),
#           legend.position = c(0.9, 0.35),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 20))


### CODE TO LOOK AT EARNINGS PER BLOCK AND CALCULATE OPTIMAL EARNINGS
# # code to calculate the rwd/sec rate for each acceptance threshold per handling
# rwd <- c(4, 8, 20)
# handling <- c(14, 10, 2) + 2 # add 2 to H and T to account for offer and reward disclosure windows
# travel <- c(2, 6, 14) + 2
# rwdRates <- sapply(3:1, function(thresh) {(sum(tail(rwd, n = thresh))) / ((handling * thresh) + (travel * 3))})
# 
# # the highest reward amount for a typical 7-min block per handling is:
# apply(rwdRates, 1, max) * (7 * 60)

# # this helps make sense of the toal earnings
# # plotting mean earnings per cost and block
# dataWth %>%
#   filter(rawChoice == 1) %>%
#   group_by(SubjID) %>%
#   mutate(Block = case_when(
#     Block %in% c(1, 2) ~ 1,
#     Block %in% c(3, 4) ~ 2,
#     Block %in% c(5, 6) ~ 3
#   )) %>%
#   group_by(SubjID, Cost, Block) %>%
#   summarise(subEarnings = sum(Offer)) %>%
#   group_by(Cost, Block) %>%
#   summarise(meanEarned = mean(subEarnings),
#             seEarned = sd(subEarnings) / sqrt(nSubjs_wth)) %>%
#   ggplot(aes(Block, meanEarned, color = Cost)) +
#     geom_point(size = 2) +
#     geom_errorbar(aes(ymin = meanEarned - seEarned, ymax = meanEarned + seEarned, color = Cost), width = 0.1, size = 1.5) +
#     geom_line(size = 1.5) +
#     labs(y = "Mean Earnings +- SE (cents)") +
#     scale_x_continuous(breaks = c(1, 2, 3)) +
#     scale_color_manual(values = colsWth) +
#     scale_fill_manual(values = colsWth) +
#     theme(legend.key = element_blank(),
#           #legend.position = c(0.1, 0.8),
#           panel.grid.major = element_blank(),
#           panel.grid.minor = element_blank(),
#           panel.background = element_blank(),
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 22))

```

``` {r WTH: 16.3.3. Plot Pre/Post reward x handling, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 4}
# # first block
# t1 <- data %>%
#   group_by(SubjID) %>%
#   mutate(Block = do.call(c, lapply(c(2, 4, 6), function(x) {rep(x / 2, sum(Block %in% c(x - 1, x)))}))) %>%
#   #filter(Half == "Half_1", Offer < 20) %>%
#   filter(Block == 1) %>%
#   group_by(Cost, Offer) %>%
#   summarise(propAccept = mean(Choice),
#             SE = sd(Choice) / sqrt(nSubjs_wth)) %>%
#   ungroup() %>%
#   mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
#   ggplot(aes(Offer, propAccept, color = Cost)) +
#     geom_point(size = 3, show.legend = F) +
#     #geom_point(aes(y = Optimal), size = 3, pch = 21, color = "darkgrey") +
#     geom_line(show.legend = F) +
#     scale_color_manual(values = colsWth) +
#     scale_fill_manual(values = colsWth) +
#     geom_errorbar(aes(ymin=propAccept-SE, ymax=propAccept+SE), width=0.2, size=1, show.legend = T) +
#     scale_y_continuous(limits = c(-0.1,1.1), breaks = c(0,0.5,1)) +
#     scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
#     labs(x = "Reward", y = "Proportion Accepted", title = "First Block") +
#     theme(legend.key = element_blank(),
#           legend.position = c(0.8, 0.25),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))
# 
# 
# # last block
# t3 <- data %>%
#     group_by(SubjID) %>%
#     mutate(Block = do.call(c, lapply(c(2, 4, 6), function(x) {rep(x / 2, sum(Block %in% c(x - 1, x)))}))) %>%
#     #filter(Half == "Half_2", Offer < 20) %>%
#     filter(Block == 3) %>%
#     group_by(Cost, Offer) %>%
#     summarise(propAccept = mean(Choice),
#               SE = sd(Choice) / sqrt(nSubjs_wth)) %>%
#     ungroup() %>%
#     mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
#     ggplot(aes(Offer, propAccept, color = Cost)) +
#       geom_point(size = 3, show.legend = F) +
#       geom_line(show.legend = F) +
#       scale_color_manual(values = colsWth) +
#       scale_fill_manual(values = colsWth) +
#       geom_errorbar(aes(ymin=propAccept-SE, ymax=propAccept+SE), width=0.2, size=1, show.legend = F) +
#       scale_y_continuous(limits = c(-0.1,1.1), breaks = c(0,0.5,1)) +
#       scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
#       labs(x = "Reward", y = "", title = "Last Block") +
#       theme(legend.position = c(0.1, 0.7),
#             panel.grid.major = element_blank(), 
#             panel.grid.minor = element_blank(), 
#             panel.background = element_blank(), 
#             axis.line = element_line(colour = "black"),
#             text = element_text(size = 16))
# 
# grid.arrange(t1, t3, ncol = 2)

hrprepostPlot_wth <- dataWth %>%
  group_by(SubjID) %>%
  mutate(Block = case_when(
    Block %in% c(1, 2) ~ 1,
    Block %in% c(3, 4) ~ 2,
    Block %in% c(5, 6) ~ 3
  )) %>%
  #filter(Half == "Half_1", Offer < 20) %>%
  filter(Block != 2) %>%
  mutate(Block = ifelse(Block == 1, "First Two Blocks", "Last Two Blocks")) %>%
  group_by(Cost, Block, Offer) %>%
  summarise(propAccept = mean(Choice),
            SE = sd(Choice) / sqrt(nSubjs_wth)) %>%
  ungroup() %>%
  mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
  ggplot(aes(Offer, propAccept, color = Cost)) +
    geom_point(size = 3, show.legend = T) +
    #geom_point(aes(y = Optimal), size = 3, pch = 21, color = "darkgrey") +
    geom_line(size = 1, show.legend = F) +
    scale_color_manual(values = colsWth) +
    scale_fill_manual(values = colsWth) +
    geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.3, size = 1, show.legend = T) +
    scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1)) +
    scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
    labs(x = "Reward", y = "Proportion Accepted") +
    facet_wrap(vars(Block)) +
    theme(legend.key = element_blank(),
          legend.position = c(0.9, 0.25),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          panel.background = element_blank(), 
          axis.line = element_line(colour = "black"),
          text = element_text(size = 16))

```

``` {r WTH: 16.3.3. mix pre/post separately, echo = FALSE, fig.align="center", fig.width=5, fig.height=4}
## compute the same mixed effects logistic for each half
# mixed logistic for the first half
mixLogis_pre_wth <- list()
mixData <- dataWth %>%
  filter(Half == "Half_1") %>%
  group_by(SubjID, Cost, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_pre_wth$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait-C"))
mixLogis_pre_wth$`Wait-C` <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait-P"))
mixLogis_pre_wth$`Wait-P` <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Physical"))
mixLogis_pre_wth$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)


# mixed logistic for the second half
mixLogis_post_wth <- list()
mixData <- dataWth %>%
  filter(Half == "Half_2") %>%
  group_by(SubjID, Cost, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_post_wth$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait-C"))
mixLogis_post_wth$`Wait-C` <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait-P"))
mixLogis_post_wth$`Wait-P` <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Physical"))
mixLogis_post_wth$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)

## now produce a summary matrix
# get the beta and pvalue matrices 
betasPre <- betaMatrix(mixLogis_pre_wth)
betasPost <- betaMatrix(mixLogis_post_wth)

# and combine them
betaMat <- matrix(NA, nrow = nrow(betasPre$Betas), ncol = nrow(betasPre$Betas))
betaMat[lower.tri(betaMat)] <- betasPre$Betas[lower.tri(betasPre$Betas)]
betaMat[upper.tri(betaMat)] <- betasPost$Betas[upper.tri(betasPost$Betas)]
dimnames(betaMat) <- list(names(mixLogis_pre_wth), names(mixLogis_pre_wth))

pvalMat <- matrix(NA, nrow = nrow(betasPre$Pvals), ncol = nrow(betasPre$Pvals))
pvalMat[lower.tri(pvalMat)] <- betasPre$Pvals[lower.tri(betasPre$Pvals)]
pvalMat[upper.tri(pvalMat)] <- betasPost$Pvals[upper.tri(betasPost$Pvals)]
dimnames(pvalMat) <- list(names(mixLogis_pre_wth), names(mixLogis_pre_wth))

# remove uninteresting comparisons 
diag(betaMat) <- 0
betaMat[rbind(c(1, 3), c(2, 4), c(3, 1), c(4,2))] <- NA
pvalMat[rbind(c(1, 3), c(2, 4), c(3, 1), c(4,2))] <- NA

# aand plot
# corrplot(betaMat, 
#          is.corr = F, 
#          p.mat = pvalMat, 
#          outline = T,
#          #insig = "p-value",
#          sig.level = 0.05, 
#          na.label = "square", 
#          na.label.col = "grey",
#          method = "color", 
#          tl.col = "black",
#          tl.cex = 0.8,
#          col = col2(200))
```

``` {r WTH: 16.3.3 temp plot, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 4}
# cw <- dataWth %>%
#   group_by(SubjID) %>%
#   mutate(Block = case_when(
#     Block %in% c(1, 2) ~ 1,
#     Block %in% c(3, 4) ~ 2,
#     Block %in% c(5, 6) ~ 3
#   )) %>%
#   filter(Offer < 20) %>%
#   filter(Block != 2) %>%
#   group_by(SubjID, Block, Cost, Offer) %>%
#   summarise(pAccept = mean(Choice)) %>%
#   spread(Block, pAccept) %>%
#   mutate(diff = `3` - `1`) %>%
#   ungroup() %>%
#   select(-c(`1`, `3`)) %>%
#   spread(Cost, diff) %>%
#   ggplot(aes(Cognitive, Wait-C, fill = as.character(Offer))) +
#     geom_point(pch = 21, color = "black", size = 3) +
#     geom_hline(yintercept = 0) +
#     geom_vline(xintercept = 0) +
#     geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
#     ylim(-1, 1) +
#     xlim(-1, 1) +
#     labs(#title = "P(Accepted): Last - First Block",
#          x = "P(Cognitive): Last - First",
#          y = "P(Wait-C): Last - First",
#          fill = "Offer") +
#     theme(legend.key = element_blank(),
#           legend.background = element_rect(fill = "grey80"),
#           legend.position = c(0.8, 0.8),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))
# 
# gw <- dataWth %>%
#   group_by(SubjID) %>%
#   mutate(Block = case_when(
#     Block %in% c(1, 2) ~ 1,
#     Block %in% c(3, 4) ~ 2,
#     Block %in% c(5, 6) ~ 3
#   )) %>%
#   filter(Offer < 20) %>%
#   filter(Block != 2) %>%
#   group_by(SubjID, Block, Cost, Offer) %>%
#   summarise(pAccept = mean(Choice)) %>%
#   spread(Block, pAccept) %>%
#   mutate(diff = `3` - `1`) %>%
#   ungroup() %>%
#   select(-c(`1`, `3`)) %>%
#   spread(Cost, diff) %>%
#   ggplot(aes(Physical, Wait-P, fill = as.character(Offer))) +
#     geom_point(pch = 21, color = "black", size = 3, show.legend = F) +
#     geom_hline(yintercept = 0) +
#     geom_vline(xintercept = 0) +
#     geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
#     ylim(-1, 1) +
#     xlim(-1, 1) +
#     labs(#title = "P(Accepted): Last - First Block",
#          x = "P(Physical): Last - First",
#          y = "P(Wait-P): Last - First",
#          fill = "Offer") +
#     theme(legend.position = c(0.8, 0.8),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))
# 
# 
# grid.arrange(cw, gw, ncol = 2)
# 
# # more prepost tests
# # diffscore prepost for effort vs wait
# dataWth %>%
#   group_by(SubjID) %>%
#   mutate(Block = case_when(
#     Block %in% c(1, 2) ~ 1,
#     Block %in% c(3, 4) ~ 2,
#     Block %in% c(5, 6) ~ 3
#   )) %>%
#   #filter(Half == "Half_1", Offer < 20) %>%
#   filter(Block != 2) %>%
#   mutate(Block = ifelse(Block == 1, "First Block", "Last Block")) %>%
#   group_by(SubjID, Cost, Block, Offer) %>%
#   summarise(propAccept = mean(Choice)) %>%
#   ungroup() %>%
#   #mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
#   spread(Block, propAccept) %>%
#   mutate(Difference = `Last Block` - `First Block`) %>%
#   group_by(SubjID, Cost) %>%
#   summarise(Diffscore = sum(Difference)/3) %>%
#   spread(Cost, Diffscore) %>%
#   ggplot() +
#     geom_point(aes(Cognitive, Wait-C), color = colsWth[1]) +
#     geom_point(aes(Physical, Wait-P), color = colsWth[2]) +
#     geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
#     ylim(-0.5, 0.5) +
#     xlim(-0.5, 0.5) +
#     #geom_hline(yintercept = 0, linetype = "dashed", size = 1) +
#     #geom_boxplot(show.legend = F, size = 1.5) +
#     #geom_jitter(width=0.1, alpha = 0.3, show.legend = F) +
#     #scale_color_manual(values = colsWth) +
#     #scale_fill_manual(values = colsWth) +
#     theme(legend.key = element_blank(),
#           legend.position = c(0.9, 0.25),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 22))
# 
# # diffscore per cost type (you don't see if specific subjects are prone to increasing/decreasing acceptances across them)
# dataWth %>%
#   group_by(SubjID) %>%
#   mutate(Block = case_when(
#     Block %in% c(1, 2) ~ 1,
#     Block %in% c(3, 4) ~ 2,
#     Block %in% c(5, 6) ~ 3
#   )) %>%
#   #filter(Half == "Half_1", Offer < 20) %>%
#   filter(Block != 2) %>%
#   mutate(Block = ifelse(Block == 1, "First Block", "Last Block")) %>%
#   group_by(SubjID, Cost, Block, Offer) %>%
#   summarise(propAccept = mean(Choice)) %>%
#   ungroup() %>%
#   #mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
#   spread(Block, propAccept) %>%
#   mutate(Difference = `Last Block` - `First Block`) %>%
#   group_by(SubjID, Cost) %>%
#   summarise(Diffscore = sum(Difference)/3) %>%
#   ggplot(aes(Cost, Diffscore, fill = Cost)) +
#     geom_hline(yintercept = 0, linetype = "dashed", size = 1) +
#     geom_boxplot(show.legend = F, size = 1.5) +
#     geom_jitter(width=0.1, alpha = 0.3, show.legend = F) +
#     scale_color_manual(values = colsWth) +
#     scale_fill_manual(values = colsWth) +
#     theme(legend.key = element_blank(),
#           legend.position = c(0.9, 0.25),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 22))
```

```{r Survival and prepost plots (Fig. 8), eval = !figsEnd, fig.width = 8, fig.height = 7, fig.align = "center", fig.cap = "Top: Survival curve (left), and distribution of quitting times over time (right). Bottom: Proportion accepted in the second half as a function of first half acceptances (left). Dashed lines indicate the mean for each cost. On the right, the proportion accepted every time participants experienced a given block type."}
# plots generated in 16.3.1. and 16.3.3.
(suppressWarnings(survPlot_wth) | suppressWarnings(RTplot_wth)) / (prepost_wth$scatterplot | prepost_wth$propBlocks)
```

  However, against our predictions, participants accepted significantly fewer trials on the second half of the experiment, regardless of cost (paired permutations; Cognitive: mean~pre~ = `r filter(df_mean, Cost == "Cognitive")$propAccept_1`, SD~pre~ = `r filter(df_mean, Cost == "Cognitive")$sdAccept_1`, mean~post~ = `r filter(df_mean, Cost == "Cognitive")$propAccept_2`, SD~post~ = `r filter(df_mean, Cost == "Cognitive")$sdAccept_2`, *p* `r report_p(prepost_wth$Perms[["Cognitive"]])`, Cohen's D = `r round(prepost_wth$CohenD[["Cognitive"]], digits = 2)`; Physical: mean~pre~ = `r filter(df_mean, Cost == "Physical")$propAccept_1`, SD~pre~ = `r filter(df_mean, Cost == "Physical")$sdAccept_1`, mean~post~ = `r filter(df_mean, Cost == "Physical")$propAccept_2`, SD~post~ = `r filter(df_mean, Cost == "Physical")$sdAccept_2`, *p* `r report_p(prepost_wth$Perms[["Physical"]])`, Cohen's D = `r round(prepost_wth$CohenD[["Physical"]], digits = 2)`; Wait-C:  mean~pre~ = `r filter(df_mean, Cost == "Wait-C")$propAccept_1`, SD~pre~ = `r filter(df_mean, Cost == "Wait-C")$sdAccept_1`, mean~post~ = `r filter(df_mean, Cost == "Wait-C")$propAccept_2`, SD~post~ = `r filter(df_mean, Cost == "Wait-C")$sdAccept_2`, *p* `r report_p(prepost_wth$Perms[["Wait-C"]])`, Cohen's D = `r round(prepost_wth$CohenD[["Wait-C"]], digits = 2)`; Wait-P: mean~pre~ = `r filter(df_mean, Cost == "Wait-P")$propAccept_1`, SD~pre~ = `r filter(df_mean, Cost == "Wait-P")$sdAccept_1`, mean~post~ = `r filter(df_mean, Cost == "Wait-P")$propAccept_2`, SD~post~ = `r filter(df_mean, Cost == "Wait-P")$sdAccept_2`, *p* `r report_p(prepost_wth$Perms[["Wait-P"]])`, Cohen's D = `r round(prepost_wth$CohenD[["Wait-P"]], digits = 2)`; Figure 9). The reduction in acceptances was not driven by a performance decline, as the difference in acceptance rates was not significantly correlated with the proportion of forced travels across individuals (Pearson's r = `r round(corDiff, digits = 2)`, *p* > 0.05). Notably, the pattern of choices from the first half of the experiment resembled that from the matching handling/travel block from the between-subject experiment (Figure 4A). Mixed effects logistic regressions performed on each half separately confirmed that the relative preference for cognitive effort seen previously was preserved at the beginning of the experiment (Figure 9, right, lower triangle), but that it disappeared over time amid an overall decline in acceptance rates as participants gained experience with all three conditions (upper triangle). 
  
  All together, our findings show that the stable preferences for cognitive effort observed during single-demand foraging slowly fade if alternative demands are introduced. This points towards task-specific and outcome-independent features from cognitive tasks that can be recalibrated by new experiences.
  
```{r Prepost mixed effects plots (Fig. 9), eval = !figsEnd, fig.align = "center", fig.cap = "Acceptance rates over time. Left: Proportion accepted in the first two and last two blocks of the experiment. An initial preference for cognitive effort trials fades amid a global decrease in acceptance rates. Right: Mixed effects coefficient matrix for each half separately. The pattern from the first half resembles what was observed in the between-subjects experiment (Figure 4)."}
include_graphics("./Images/FIG10.png")

```


### **Computational modeling of foraging behavior**

``` {r BTW: 16.3.3. OC modeling, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4}
## fit the OC model
summaryOC_btw <- list()
summaryOC_btw$all <- dataBtw %>%
                 group_by(Cost, SubjID) %>%
                 do(optimizeOCModel(., simplify = T)) %>%
                 ungroup()


## plot
summaryOC_btw$plot <- ggplot(summaryOC_btw$all, aes(Cost, Gamma, fill = Cost)) +
  # geom_hline(yintercept = 0.64, alpha = 0.9, color = "gray20", size = 1) +
  # geom_hline(yintercept = 0.7, alpha = 0.9, color = "gray40", size = 1) +
  # geom_hline(yintercept = 0.9, alpha = 0.9, color = "gray60", size = 1) +
  geom_hline(yintercept = 0.74, alpha = 0.9, size = 1, linetype = "dashed") + # mean of highest earning rates across blocks
  #geom_dotplot(stackdir = 'center', binaxis = 'y', show.legend = F) +
  geom_boxplot(show.legend = F) +
  geom_jitter(width = 0.1, alpha = 0.5, show.legend = F, pch = 21, size = 3) +
  ylim(0,1.5) +
  labs(x = "") +
  scale_fill_manual(values = colsBtw) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        text = element_text(size = 16))


## one sample t against optimal strategy
summaryOC_btw$t.test <- summaryOC_btw$all %>%
  select(Cost, Gamma) %>%
  plyr::dlply("Cost", identity) %>%
  sapply(., function(data) {t.test(data$Gamma, mu = 0.74)})


## descriptive stats
summaryOC_btw$descriptives <- summaryOC_btw$all %>%
  group_by(Cost) %>%
  summarise(mGamma = round(mean(Gamma), digits = 2),
            sdGamma = round(sd(Gamma), digits = 2))


# summaryOC$all %>%
#   ggplot(aes(Gamma, percentAccept, fill = Cost, size = temperature)) +
#   geom_point(pch = 21)


# # gamma parameter recovery tests
# # test a single subject against their own choices
# (subj <- sample(subjList_btw, 1))
# subjData <- dataBtw %>% 
#   filter(SubjID == subj) %>% 
#   group_by(Cost, Handling, Offer) %>% 
#   summarise(propAccept = mean(Choice)) %>%
#   ungroup()
# 
# params <- filter(summaryOC$all, SubjID == subj) %>% 
#   select(Gamma, temperature)
# 
# test_params(params,
#             model = expr(params$Gamma * Handling), 
#             subjData, 
#             plot = T)


# # test multiple values of gamma and plot the comparison
# lowGamma <- 0
# upperGamma <- 1.7
# nGamma <- 6
# testys <- lapply(seq(lowGamma, upperGamma, length.out = nGamma), test_params)
# (testys <- do.call(rbind, testys))
# testys %>%
#   ggplot(aes(interaction(Offer, Handling), Choice, color = Gamma)) +
#   geom_point(size = 3) +
#   geom_line(aes(group = interaction(Handling, Gamma)), size = 1) +
#   labs(x = "Offer.Handling", y = "Proportion Accepted") +
#   facet_wrap(vars(Gamma), ncol = 2) +
#   theme(legend.key = element_blank(),
#         panel.grid.major = element_blank(),
#         panel.grid.minor = element_blank(),
#         panel.background = element_blank(),
#         axis.line = element_line(colour = "black"),
#         text = element_text(size = 16))

```

``` {r BTW: 16.3.5. OC comparisons, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4}
# ANOVA
OCanova <- list()
OCanova$aov <- summary(aov(Gamma ~ Cost, data = summaryOC_btw$all))
Fval <- round(OCanova$aov[[1]]$`F value`[1], digits = 2) # doing separate variables because I can't easily use the `` in markdown.
Pval <- OCanova$aov[[1]]$`Pr(>F)`[1]
Rsquared_aov <- round(OCanova$aov[[1]]$`Sum Sq`[1] / sum(OCanova$aov[[1]]$`Sum Sq`), digits = 2)
  
# Post-hoc pairwise tests
OCanova$perms <- summaryOC_btw$all %>%
  select(Cost, Gamma) %>%
  plyr::dlply("Cost", identity) %>%
  do.call(cbind, .) %>%
  select_if(is.numeric) %>%
  rename(Physical = Physical.Gamma,
         Cognitive = Cognitive.Gamma,
         Wait = Wait.Gamma,
         Easy = Easy.Gamma) %>%
  outer(., ., FUN = Vectorize(permute), simple = T)
diag(OCanova$perms) <- 1

OCanova$ES <- summaryOC_btw$all %>%
  select(Cost, Gamma) %>%
  plyr::dlply("Cost", identity) %>%
  do.call(cbind, .) %>%
  select_if(is.numeric) %>%
  rename(Physical = Physical.Gamma,
         Cognitive = Cognitive.Gamma,
         Wait = Wait.Gamma,
         Easy = Easy.Gamma) %>%
  outer(., ., FUN = Vectorize(cohenD)) %>%
  abs()
```

``` {r BTW: 16.3.5. corrplot, echo = FALSE, fig.align = "center", fig.width = 5, fig.height = 5}
# # plot all pairwise comparisons
# corrplot(OCanova$ES,
#          is.corr = F, 
#          p.mat = OCanova$perms,
#          type = "upper",
#          insig = "p-value",
#          sig.level = -1,
#          na.label = "square",
#          na.label.col = "grey",
#          method = "color", 
#          tl.col = "black", 
#          tl.cex = 0.8,
#          outline = T)
```

``` {r BTW: 16.3.4. OC cross-validation, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4}
# Compute the OC for pre-midpoint
summaryOC_btw$pre <- dataBtw %>%
                 filter(Half == "Half_1") %>%
                 group_by(Cost, SubjID) %>%
                 do(optimizeOCModel(., simplify = T)) %>%
                 ungroup() %>%
                 select(SubjID, Gamma)

# let's add the gammas to the second half data and estimate prop predicted on the second half
summaryOC_btw$predicted <- dataBtw %>%
                 filter(Half == "Half_2") %>%
                 group_by(Cost, SubjID) %>%
                 left_join(summaryOC_btw$pre, by = "SubjID") %>%
                 mutate(OC = Handling * Gamma,
                        predicted = Offer > OC) %>% 
                 summarise(percentPredicted = mean(predicted == Choice) * 100) %>%
                 group_by(Cost) %>%
                 summarise(mPredicted = round(mean(percentPredicted), digits = 2),
                           sdPredicted = round(sd(percentPredicted), digits = 2))

# # plot
# (summaryOC_btw$predictPlot <- ggplot(summaryOC_btw$predicted, aes(Cost, percentPredicted, fill = Cost)) +
#                             labs(x = "") +
#                             geom_boxplot(show.legend = F) +
#                             scale_fill_manual(values = colsBtw) +
#                             labs(y = "Percent Predicted") +
#                             ylim(0,100) +
#                             theme(panel.grid.major = element_blank(),
#                             panel.grid.minor = element_blank(),
#                             panel.background = element_blank(),
#                             axis.line = element_line(colour = "black"),
#                             text = element_text(size = 16)))

```

  We tested three nested models based on a set of criteria derived from observed choice behavior (see Methods for details). The first one, a base formulation, estimated a subjective opportunity cost for each individual and demand. The second one allowed the OC to gradually adapt using a reinforcement learning rule. The final one added a time sensitivity parameter that controlled how each demand affected the perceived length of the handling time, which could be recalibrated based on the experience of multiple types of demands.
  
  In the first model, gamma reflected cost by indexing how much money per second a participant needed to accept a trial. For example, the aversion produced by a given cost could prompt an individual to spend their time and effort in the most profitable offers, resulting in a high gamma value. Conversely, low gamma values represented low selectivity when evaluating offers. Results from this model apply only to the between-subjects experiment, as this single parameterization was not flexible enough to account for the dynamics of experiment two. This model was able to explain a considerable degree of choice variance per individual ($R^2$: mean = `r round(mean(summaryOC_btw$all$Rsquared), digits = 2)`, SD = `r round(sd(summaryOC_btw$all$Rsquared), digits = 2)`). Figure 10 displays the distribution of fitted gammas, which were significantly different across groups (F(`r OCanova$aov[[1]]$Df[1]`, `r OCanova$aov[[1]]$Df[2]`) = `r Fval`, p = `r report_p(Pval)`, $R^2$ = `r Rsquared_aov`). Pairwise post-hoc permutations indicated that the mean gamma values for the cognitive group were significantly lower than for the wait (*p* `r report_p(OCanova$perms["Wait", "Cognitive"])`, Cohen's D = `r round(OCanova$ES["Wait", "Cognitive"], digits = 2)`) and physical effort (*p* `r report_p(OCanova$perms["Physical", "Cognitive"])`, Cohen's D = `r round(OCanova$ES["Physical", "Cognitive"], digits = 2)`), and marginally lower than the easy group (*p* `r report_p(OCanova$perms["Easy", "Cognitive"])`, Cohen's D = `r round(OCanova$ES["Easy", "Cognitive"], digits = 2)`). In addition, the easy group displayed significantly lower gamma values than the wait group (*p* `r report_p(OCanova$perms["Wait", "Easy"])`, Cohen's D = `r round(OCanova$ES["Wait", "Easy"], digits = 2)`). We cross-validated individual OCs by using the pre-midpoint data for estimation, and post-midpoint choices for testing. This procedure revealed high mean predictive accuracies for all individuals (mean accuracy for Cognitive = `r filter(summaryOC_btw$predicted, Cost == "Cognitive")$mPredicted`%, SD = `r filter(summaryOC_btw$predicted, Cost == "Cognitive")$sdPredicted`; mean Physical = `r filter(summaryOC_btw$predicted, Cost == "Physical")$mPredicted`%, SD = `r filter(summaryOC_btw$predicted, Cost == "Physical")$sdPredicted`; mean Wait = `r filter(summaryOC_btw$predicted, Cost == "Wait")$mPredicted`%, SD = `r filter(summaryOC_btw$predicted, Cost == "Wait")$sdPredicted`; mean Easy = `r filter(summaryOC_btw$predicted, Cost == "Easy")$mPredicted`%, SD = `r filter(summaryOC_btw$predicted, Cost == "Easy")$sdPredicted`), despite the reduction in acceptances produced by the effortful demands. We further interpreted these fitted values by comparing them to the optimal gamma (dashed line on Figure 10; see Operationalization of Cost). Two-sided one-sample t-tests against this optimal magnitude resulted in significantly lower gamma values for the cognitive group (mean = `r filter(summaryOC_btw$descriptives, Cost == "Cognitive")$mGamma`, SD = `r filter(summaryOC_btw$descriptives, Cost == "Cognitive")$sdGamma`; T(20) = `r round(summaryOC_btw$t.test["statistic", "Cognitive"][[1]], digits = 2)`, *p* `r report_p(summaryOC_btw$t.test["p.value", "Cognitive"][[1]])`), and marginally higher values for the wait group (mean = `r filter(summaryOC_btw$descriptives, Cost == "Wait")$mGamma`, SD = `r filter(summaryOC_btw$descriptives, Cost == "Wait")$sdGamma`; T(20) = `r round(summaryOC_btw$t.test["statistic", "Wait"][[1]], digits = 2)`, *p* `r report_p(summaryOC_btw$t.test["p.value", "Wait"][[1]])`). Neither physical effort (mean = `r filter(summaryOC_btw$descriptives, Cost == "Physical")$mGamma`, SD = `r filter(summaryOC_btw$descriptives, Cost == "Physical")$sdGamma`) nor the easy condition (mean = `r filter(summaryOC_btw$descriptives, Cost == "Easy")$mGamma`, SD = `r filter(summaryOC_btw$descriptives, Cost == "Easy")$sdGamma`) produced significant differences (all *p* > 0.05). These results suggest that the cognitive effort group acted as if they perceived the trials as attractive.
  
```{r Gamma per cost group (Fig. 10), eval = !figsEnd, fig.align = "center", fig.height = 3, fig.width = 5, fig.cap = "Gamma values for each group. The dashed line indicates the average optimal earnings per second across blocks, and provides a reference for how costly (higher) or valuable (lower) each condition was. Wait and physical effort groups tended to perceive demands as costly, while the cognitive effort group seemed to value their task instead."}
  
summaryOC_btw$plot

```

  The second model asked whether interleaved demands gradually altered the OC over time, based on the derivation by Constantino & Daw (2015). This model behaved similarly to the base version at low enough values of alpha.


``` {r WTH: 16.3.4., echo = FALSE, fig.align="center", fig.width=6, fig.height=4}
# Here it might be best to estimate gamma for the wait trials only at first
# once the wait OCs are estimated, then they can be modulated by effort
# Estimate gamma per subject for each group separately
summaryOC_wth <- list()
summaryOC_wth$all <- dataWth %>%
  mutate(Block = case_when(
    Block %in% c(1, 2) ~ 1,
    Block %in% c(3, 4) ~ 2,
    Block %in% c(5, 6) ~ 3
  )) %>%
  #filter(Choice < 2) %>%
  group_by(SubjID, Cost) %>% #group_by(SubjID, Cost, Block) %>%
  do(optimizeOCModel(., simplify = T)) %>%
  ungroup()

# plot
summaryOC_wth$plot <- ggplot(summaryOC_wth$all, aes(Cost, Gamma, fill = Cost)) +
    geom_boxplot(show.legend = F) +
    geom_hline(yintercept = 0.64, alpha = 0.9, color = "gray20", size = 1) +
    geom_hline(yintercept = 0.7, alpha = 0.9, color = "gray40", size = 1) +
    geom_hline(yintercept = 0.9, alpha = 0.9, color = "gray60", size = 1) +
    ylim(0,1.5) +
    labs(x = "") +
    scale_fill_manual(values = colsWth) +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.background = element_blank(),
          axis.line = element_line(colour = "black"),
          text = element_text(size = 16))

# try to replicate the group-level proportion of acceptances from 16.2
# this looks pretty much like the empirical proportion data, so the stochasticity is playing a major part in the estimation
# the estimate probability even matches actual proportions at the single subject level 
# logis <- mapply(function(g, t) {1 / (1 + exp((-1 * t) * (c(4, 8, 20) - (g * 10))))}, summaryOC_wth$all$Gamma, summaryOC_wth$all$Temperature)
# colnames(logis) <- unite(summaryOC_wth, test, SubjID, Cost)$test
# logis2 <- as.data.frame(logis) %>% 
#   gather(Cost, Probability) %>%
#   mutate(SubjID = substr(Cost, 1,3), 
#          Cost = substr(Cost, 5,20),
#          Reward = rep(c(4,8,20), nrow(.)/3)) %>%
#   group_by(Cost, Reward) %>%
#   summarize(propAccept = mean(Probability)) %>%
#   ggplot(logis2, aes(Reward, propAccept, color = Cost)) +
#     geom_point(size = 3, show.legend = T) +
#     geom_line(lwd = 1, show.legend = T) +
#     scale_color_manual(values = colsWth) +
#     scale_fill_manual(values = colsWth) +
#     #geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.4, size = 1) +
#     scale_y_continuous(limits = c(-0.1,1.1), breaks = c(0,0.5,1)) +
#     scale_x_continuous(breaks = c(4, 8, 12), labels = c("4", "8", "20")) +
#     labs(y = "Proportion Accepted") +
#     theme(legend.key = element_blank(),
#           panel.grid.major = element_blank(),
#           panel.grid.minor = element_blank(),
#           panel.background = element_blank(),
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))
```

``` {r WTH: 16.3.5., echo = FALSE, fig.width=5, fig.height=4, fig.align="center"}
# subdivide data
dataTrain <- dataWth %>%
   filter(Block < 5)

dataTest <- dataWth %>%
  filter(Block > 4)

# estimate
# per-cost, but consider the idea above
OCs_validation <- list()
temp <- dataTrain %>% filter(Cost == "Wait-C") %>% plyr::dlply("SubjID", identity)
OCs_validation$`Wait-C` <- sapply(temp, function(Data) {optimizeOCModel(Data)$Gamma})
temp <- dataTrain %>% filter(Cost == "Wait-P") %>% plyr::dlply("SubjID", identity)
OCs_validation$`Wait-P` <- sapply(temp, function(Data) {optimizeOCModel(Data)$Gamma})
temp <- dataTrain %>% filter(Cost == "Cognitive") %>% plyr::dlply("SubjID", identity)
OCs_validation$Cognitive <- sapply(temp, function(Data) {optimizeOCModel(Data)$Gamma})
temp <- dataTrain %>% filter(Cost == "Physical") %>% plyr::dlply("SubjID", identity)
OCs_validation$Physical <- sapply(temp, function(Data) {optimizeOCModel(Data)$Gamma})

# reshape data
temp <- bind_rows(OCs_validation) %>% 
  mutate(SubjID = subjList_wth) %>%
  gather(Cost, gamma, `Wait-C`:Physical) %>%
  left_join(dataTest, ., by = c("SubjID", "Cost")) %>% # assign OCs to each subject per cost type
  mutate(prediction = Offer > Handling * gamma) %>%
  # plyr::dlply("SubjID", identity)
  group_by(SubjID, Cost) %>%
  summarize(predicted = mean(Choice == prediction))

# plot prediction %
# qplot(Cost, predicted, geom = "boxplot", data = temp) + 
#   ylim(0,1) +
#   theme_classic()
```

``` {r WTH: 16.3.6., echo = FALSE}
# repeated measures anova
OCs_anova <- list()
OCs_anova$aov <- summary(aov(Gamma ~ Cost + Error(SubjID / Cost), data = summaryOC_wth$all))

# table
#pander(OCs_anova$aov)

# do all pairwise comparisons
OCs_anova$gammas <- summaryOC_wth$all %>% 
  reshape2::dcast(SubjID ~ Cost, value.var = "Gamma") %>%
  select(-SubjID)

# perform a paired permutation among every pair of cost types
OCs_anova$perms <- outer(OCs_anova$gammas, OCs_anova$gammas, FUN = Vectorize(permute), simple = T, paired = T)
OCs_anova$perms[rbind(c(1,4),c(2,3),c(3,2),c(4,1))] <- NA

# and the effect sizes
OCs_anova$ES <- abs(outer(OCs_anova$gammas, OCs_anova$gammas, FUN = Vectorize(DescTools::CohenD)))
OCs_anova$ES[rbind(c(1,4),c(2,3),c(3,2),c(4,1))] <- NA
```

``` {r WTH: 16.3.6. corrplot, echo = FALSE, fig.align="center", fig.width=5, fig.height=5}
# corrplot(OCs_anova$ES,
#          is.corr = F, 
#          p.mat = OCs_anova$perms,
#          type = "upper",
#          insig = "p-value",
#          sig.level = -1,
#          na.label = "square",
#          na.label.col = "grey",
#          method = "color", 
#          tl.col = "black", 
#          tl.cex = 0.8,
#          outline = T)
```



## Discussion

  Understanding the sporadic attractiveness of effort remains elusive. In this study, we tested the hypothesis that the perceived cost or value of a demand could depend on the ability to achieve the same outcome through different means. Using two foraging experiments that exposed individuals to a single or intermixed types of demands, we found that 1) preferences for cognitive effort were stable when individuals faced a single form of demand, but faded over time when other costs were also experienced; and 2) the phenomenon can be explained by a model in which preferences depend on how participants construe the amount of time a task takes, which could hypothetically be recalibrated when tasks are interleaved.
  
  Our findings provided partial support for our pre-registered hypotheses (https://osf.io/2rsgm/registrations). In accord with our predictions, decision makers in both experiments successfully approximated reward-maximizing strategies, as evidenced by acceptance rates near optimal points. This is in line with previous foraging experiments, where the main deviation from optimality has been a tendency to overharvest [@Constantino2015; @Wikenheiser2013]. This successful integration of the task was further evidenced by the ability of individuals to describe near-optimal strategies after participation, discarding the possibility of our results being driven by noise or bias in the estimation of environmental richness [@Cash-Padgett2020; @Garrett2020; @Dundon2020]. In contrast, our prediction that effort requirements would produce more acceptances only held for the cognitive group, and this tendency for higher acceptances did not switch to passive waiting as predicted for the within-subjects experiment. These hypotheses originally stemmed from prevalent proposals that incentivized work should become more appealing [@Eisenberger1992; @Inzlicht2018], but that people would opt for the easiest available way to achieve desired outcomes [@Hull1943; @Olivola2013]. Instead, our novel use of foraging paradigms, in addition to our inclusion of types of effort that are often studied separately, highlighted task-specific attributes contributing to the attractive or aversive nature of demands. The fact that predicted stable preferences for cognitive effort carried over onto the beginning of the within-subjects experiment further supported a task-specific modulation. However, the unexpected gradual convergence in acceptances in the second experiment emphasized that these features could be contextually molded.
  
  These choice dynamics ruled out a number of common explanatory candidates. For instance, differences in choice stability between experiments suggest that a demand's cost is not intrinsic. The gradual choice convergence in the second experiment also discards the effects of boredom, which would have accentuated the appeal of active tasks over time, or the possibility that our population was mostly composed of demand seekers [@Cacioppo1996; @Sayali2019]. Even though fatigue could explain the decrease in effort acceptances across experiments, we would have expected tired participants to turn to wait trials over time in order to preserve earning rates. We also ruled out the sunk cost effects used to explain overharvesting [@Wikenheiser2013; @Sweis2018], as participants expressed their choices early and rarely quit during the handling time. Finally, the observed differences in acceptances between effort types cannot be explained by reward alone, as demands were rewarded at the same rate. 
  
  (**hold off on this paragraph until modeling is done**) We do not discard the influence of these elements in daily effort evaluations. Instead, we computationally highlight that effort can became valuable by shortening the subjective elapsed time of trials, a perception that could be recalibrated as individuals experienced multiple forms of demands (Cognitive effort gained value by making trials feel shorter). Alterations in subjective time sensitivity have been shown to play a role in temporal discounting [@Mckerchar2009], and are often experienced when we are fully immersed in a task [@Csikszentmihalyi2014]. The shorter time sensitivity of cognitive effort could be due to its constant recruitment of working memory, impeding estimations of the nominal time, or because of the discrete nature of the task (versus a continuous configuration of the physical and wait tasks). These results expand our understanding of the attractiveness of effort, bringing forth outcome-independent features that can be leveraged to motivate effort engagement in diverse everyday scenarios. ....This could be because: 1) discrete nature of cog taska llowed for counting; 2) flow; 3) persistent recruitment of working memory (Buhi and Gershman). Talk about future experiments: if this is subjective time, changing the pacing of the tasks should change the acceptance rates for cognitive effort. Maybe use this example: if I ask you to perform an activity until you think a minute has elapsed, that estimate will be noisy, but also different for different tasks. That's how it is in real life.

  Relying on foraging paradigms helped us to control for common shortfalls in the existing literature. For example, even though engaging with a demanding offer encompasses evaluating, pursuing, and receiving its outcome, investigations of the pursuit and receipt stages have been limited. Many studies have sought to isolate the evaluation stage by presenting individuals with offers that do not come into effect until the end of the experimental session (with delays spanning up to months), deliberately setting aside the question of how the ongoing experience of cost and outcome affects subsequent choices [@Kable2007; @Chong2016; @Massar2015; @Apps2015a]. However, discrepancies between expected and experienced value are well known [@Kahneman2006], and choosing immediate outcomes could be due to factors other than time preference, such as the risk of being unable to redeem the future rewards, or hoping to find better offers in the near future [@Fawcett2012; @Frederick2002; @Sozou1998]. We circumvented these pitfalls by providing participants with an opportunity to actively experience the demands they accepted, and gaving them full knowledge of the available options. Another challenge in comparing effort and delay is that increments in the required effort are often paired with a longer execution time. This issue is often avoided by comparing discounting functions, which yield points at which individuals faced with increasing costs become indifferent to large rewards [@Massar2015; @Prevost2010]. We eliminated this possibility by fixing the handling time for all demands, allowing us to probe people's preferences directly. Moreover, the between-subjects experiment, which resembled the traditional way costs are compared, provided a baseline of foraging behavior that contextualized the findings of the second experiment (a direct test of our hypothesis). Finally, the existence of optimal strategies helped us identify when demands could be attractive, a central motivation for this study. The apparent attractiveness of cognitive effort was reflected both by a higher proportion of acceptances at the expense of earnings, and by comparisons of fitted opportunity costs against the optimal earning rate. Of note is that the optimality of the strategies induced by each demand depended on the richness of the block. This can help explain why the wait group earned the most despite having similar acceptance rates to the physical group, as the former was closer to optimality in the richest environment.
  
  It is possible that participants gravitated towards cognitive demands because the task configuration was not effortful. This could be due to differences in how we calibrated cognitive and physical tasks. For physical demands, we adjusted the gripping threshold to people's maximum strength, whereas we trained participants in the cognitive tasks until they reached criterion. Training might have improved response automaticity and reduce error-related risk. In addition, attributes such as the efficacy to complete a task, or the diversity of its components, have been reported to promote engagement [@Bandura2010; @Eisenberger1996]. Nevertheless, we believe our manipulations were successful in inducing feelings of effort. Hand grips are regularly used to probe physical effort [@Chong2017; @Hogan2018; @Prevost2010], and task switching has been shown to feel cognitively effortful by recruiting cognitive control [@Kool2010; @Shenhav2017]. Furthermore, we observed a steeper decrease in acceptances over time at longer handling times, a hallmark of effortful exhertion [@Treadway2009]. 
  
  The present findings have important implications for daily and clinical scenarios. Excessive aversion to delay and effort can manifest as impulsivity and apathy, respectively [@Ainslie1975; @Bonnelle2016; @Chong2016; @Cummings1993], fluctuations in which are hallmark symptoms of various mental illnesses and substance use disorders [@Paulus2007]. Lack of motivation to exert effort or wait for prospects is a distinguishing factor among mental afflictions like depression and schizophrenia [@Barch2019]. Within the former, medicated patients in remission are more sensitive to effort than controls, and this sensitivity is predictive of relapse [@Berwian2020]. Encouraging patients to engage in activities has been found to be a helpful treatment for depression [@Dimidjian2006]. Tailored reward schedules can successfully achieve this [@Eisenberger1996]. However, since depressed patients also show insensitivity to rewards [@Huys2013], identifying what makes effort appealing in its own right can help breach treatment-resistant cases. Our work suggests that subdividing a task can promote its pursual either by taxing working memory and/or allowing individuals to count completed tasks instead of continuous time. More importantly, it highlights that decision scenarios should be configured to promote pursual of desired activities, perhaps by removing alternative uses of time. These same principles can potentially be applied to increasing productivity at work or adherence to physical exercise routines.
  
  
### *Points to cover* 

- Regardless, our focus here was not on the amount of effort exherted, but on what makes people pursue the task in spite of its toll.

  . Talk clinical and practical. Practical could be motivating people to pursue challenges that produce desirable outcomes, like work, exercise etc.  Much of the literature on the value of effort has suggested that it can be triggered by tailored reward schedules. This doesn't always do (Eisenberger, Inzlicht). There is also the idea that training in certain tasks will have generalizing effects, which are not always true (Kable). We highlight an outcome-independent feature that is phenomenologically common (time sensitivity), and potentially easy to implement. 


- we have all felt the need to populate empty time 


- It's maybe relevant that waiting is goal oriented (you don't sit around for its own sake), but activities are pursued for their own appeal. *like, waiting is a goal oriented activity, but effort can be the goal itself.* in this sense it makes sense to study delays in terms of their modulation of the value of prospects, but we have unnecessarily adopted this view for ethe study of ffort, thinking of it mostly as a cost. Even if their discounting properties are similar, their resulting behavior can be contrasting (apathy vs impulsivity), and they're impacted differently by mental illness (adhd amygdala paper). Also, it's not just that we would rather to fill in empty periods of waiting, but how.

- it's maybe relevant that waiting is goal oriented (you don't sit around for its own sake), but activities are pursued for their own appeal. like, waiting is a goal oriented activity, but effort can be the goal itself. In this sense it makes sense to study delays in terms of their modulation of the value of prospects, but we have unnecessarily adopted this view for the study of effort, thinking of it mostly as a cost. Even if their discounting properties are similar, their resulting behavior can be contrasting (apathy vs impulsivity), and they're impacted differently by mental illness (adhd amygdala paper).

- Theories enumerated by Eisenberger and Cameron (1996) focus on how people see their behavior to explain why tasks are intrinsically attractive. We, on the other hand, point to task-specific features that promote appeal, which are amenable to configuration in everyday decision contexts. In other words, self-explanatory motivation versus task-specific ones.



 
  
  
\newpage

## Figures

```{r New Trial plot (Fig. 1), eval = figsEnd, out.width = "80%", fig.align = "center", fig.cap= "General foraging trial structure. On each trial, participants were offered to earn money (4, 8, or 20 cents) by sustaining effort or waiting during the handling time (2, 10, or 14 s). The end of a trial was followed by a travel time (handling and travel times always added up to 16 s). Participants could skip unfavorable trials and immediately start traveling to a potentially better offer. In the between-subject experiment, cost was fixed per participant and handling time varied per block. In the within-subject version, handling time was fixed at 10 s, but a combination of effort and delay trials changed per block. Possible costs, handling times, and rewards were fully disclosed in order to avoid experience-dependent learning."}

include_graphics("./Images/FIG1_short.png")
```

\newpage

```{r New Optimal plot (Fig. 2), eval = figsEnd, out.width = "80%", fig.align = "center", fig.cap= "Possible earnings per second for each acceptance threshold (i.e. the smallest amount accepted) for each handling time. Circles denote the reward-maximizing threshold for each block, which is described in the table. Experiments shared the 10 s handling block, in which it was optimal to skip all 4 cent offers."}

include_graphics("./Images/FIG2.png")
```

\newpage

```{r New General prop & earnings plots (Fig. 3), eval = figsEnd, fig.width = 10, fig.height = 4, fig.align = "center", fig.cap = "Left: Proportion accepted per cost. Middle: Total number of dollars earned by each group by the end of the experiment. Right: The relationship between proportion accepted and total earned. Consistent with the foraging design, participants who over and under accepted earned the least."}
# plots generated in 16.2.1. and 16.1.3.
 prop1 | earnings1 | earnFitplot_btw
```

\newpage

```{r New mixed effects plots (Fig. 4), eval = figsEnd, out.width = "80%", fig.align = "center", fig.cap = "Comparing acceptance rates among costs for each reward and handling combination. A: Proportion accepted by each group for every combination of handling time and reward. Grey dots indicate the reward-maximizing acceptance for each combination. B: Matrix portraying the coefficients (color scale) and significance that resulted from switching the reference cost condition (crosses mark non-significant comparisons). Each entry shows how much more likely a group was to accept compared to the reference condition (row). C: AIC and BIC values for a forward mixed-effects model comparison, ranging from a base intercept-only to a full-interaction model."}
#hrPlot_btw / (ABIC_plot_btw | ME_matrix_btw) # doesn't work with corrplot.. oh well
include_graphics("./Images/FIG4.png")
```

\newpage

```{r New Survival and prepost plots (Fig. 5), eval = figsEnd, fig.width = 8, fig.height = 7, fig.align = "center", fig.cap = "Top: Survival curve (left), and distribution of times when participants quit a trial over time. Bottom: Proportion accepted in the second half as a function of first half acceptances (left). Dashed lines indicate the mean for each group. On the right, the proportion accepted over time per group. The order of blocks was repeated in the second half (indicated by the black dashed line)."}
# plots generated in 16.3.1. and 16.3.2.
(suppressWarnings(survPlot_btw) | RTplot_btw) / (prepost_btw$scatterplot | prepost_btw$propBlocks)
```

\newpage

```{r New Prepost mixed effects plot (Fig. 6), eval = figsEnd, fig.align = "center", fig.cap = "Distilling acceptance behavior before and after the break. Left: Proportion of acceptances per handling, reward, and half, showing that effortful demands became increasingly aversive over time. Right: Mixed-effects coefficients denoting the comparison among costs acceptances for each half separately (mirrored along the diagonal). Even though acceptances steadily decreased in the effort groups, the relative preferences were preserved."}
include_graphics("./Images/FIG6.png")
```

\newpage

```{r New Earnings/prop completed/ME plots (Fig. 7), eval = figsEnd, fig.width = 8, fig.height = 7, fig.align = "center", fig.cap = "A: Quadratic relationship between trial completions and total earnings. B: proportion of acceptances of each reward per cost condition. C: Matrix of mixed-effects coefficients showing comparisons among costs (while controlling for reward amount). Comparisons between effort and wait trials from different blocks are omitted, as these are not considered in our original predictions. Crosses mark non-significant comparisons. D: AIC and BIC values for models of increasing complexity (a priori = AllMain)."}

include_graphics("./Images/FIG8.png")
```

\newpage

```{r New Survival and prepost plots (Fig. 8), eval = figsEnd, fig.width = 8, fig.height = 7, fig.align = "center", fig.cap = "Top: Survival curve (left), and distribution of quitting times over time (right). Bottom: Proportion accepted in the second half as a function of first half acceptances (left). Dashed lines indicate the mean for each cost. On the right, the proportion accepted every time participants experienced a given block type."}
# plots generated in 16.3.1. and 16.3.3.
(suppressWarnings(survPlot_wth) | suppressWarnings(RTplot_wth)) / (prepost_wth$scatterplot | prepost_wth$propBlocks)
```

\newpage
  
```{r New Prepost mixed effects plots (Fig. 9), eval = figsEnd, fig.align = "center", fig.cap = "Acceptance rates over time. Left: Proportion accepted in the first two and last two blocks of the experiment. An initial preference for cognitive effort trials fades amid a global decrease in acceptance rates. Right: Mixed effects coefficient matrix for each half separately. The pattern from the first half resembles what was observed in the between-subjects experiment (Figure 4)."}
include_graphics("./Images/FIG10.png")
```

\newpage

```{r New Gamma per cost group (Fig. 10), eval = figsEnd, fig.align = "center", fig.height = 3, fig.width = 5, fig.cap = "Gamma values for each group. The dashed line indicates the average optimal earnings per second across blocks, and provides a reference for how costly (higher) or valuable (lower) each condition was. Wait and physical effort groups tended to perceive demands as costly, while the cognitive effort group seemed to value their task instead."}
  
summaryOC_btw$plot

```


\newpage

## References

