---
title: Apparent preferences for cognitive effort fade when multiple forms of effort and delay are interleaved in a foraging environment
author: "Claudio Toro-Serey^1^2 & Joseph T. McGuire^1^2"
csl: apa.csl
output:
  word_document: default #reference_docx: Style_reference_draft.docx
  pdf_document:
    highlight: haddock
    keep_tex: no
header-includes:
- \usepackage{setspace}\doublespacing
- \usepackage[left]{lineno}
- \linenumbers
- \usepackage{float}
- \newcommand{\beginsupplement}{ \setcounter{table}{0} \renewcommand{\thetable}{S\arabic{table}}
  \setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}} }
#subtitle: 'Abbreviated Title: Heterogeneity in the functional topography of mPFC'
#bibliography: mPFC_paper.bib
---

```{r Global options, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, fig.pos = 'H')
```

```{r Setup, include = FALSE}
## libraries
library(tidyverse)
library(ggfortify)
library(knitr)
library(pander) 
library(nloptr)
library(pwr)
library(lme4)
#library(lmerTest)
library(gridExtra)
library(reshape2)
library(corrplot)

## aesthetic options
lbls <- c("Wait","Cognitive","Physical","Easy") # between subj
colsBtw = c("#78AB05","#D9541A","deepskyblue4", "darkgoldenrod2") # plot colors (wait, effort)
colsWth <- c("#D9541A", "#78AB05", "dodgerblue4", "deepskyblue3")#"grey30", "grey70") # plot colors (wait, effort)
lthick = 2 # line thickness for plots

## functions
# get the number of continuous quits before current trial
seqQuits <- function(subjData = 1){
  
    # get the lag
    tempSeq <- sequence(rle(subjData)$lengths) # sequence
    tempSeq[subjData==1] <- 0 # not interested in completed runs, just quits
    tempSeq <- c(0,tempSeq) # lag it so it can be used as a t-1 regressor

    # return data so that the length is equivalent
    out <- tempSeq[1:length(subjData)]
    
    return(out)
    
}

# Cohen's D for 2 groups (could just use DesctTools...)
# for a more flexible approach, make the data input to be a list with entries for n groups, 
# then do length(list) for the number of groups. 
cohenD <- function(group1 = 1, group2 = 2){
  
      # means
      mean1 <- mean(group1, na.rm = T)
      mean2 <- mean(group2, na.rm = T)
      
      # variance
      var1 <- var(group1, na.rm = T)
      var2 <- var(group2, na.rm = T)
      
      # equation
      out <- (mean1 - mean2) / sqrt((var1 + var2)/2)
      
      return(out)
  
}


# generic permutation for 2 groups
permute <- function(group1 = 1, group2 = 2, statType = mean, nPerms = 5000, paired = FALSE, simple = FALSE){
  
    # prep data
    summaryPerm <- list()
    lOne <- length(group1)
    lTwo <- length(group2)
    bigSample <- c(group1,group2)  
    
    if (paired == FALSE) {
      
        
        for (i in 1:nPerms){
          
            # relabel samples
            tempBig <- sample(bigSample)
            tempOne <- tempBig[1:lOne]
            tempTwo <- tempBig[(lOne+1):length(bigSample)]
            
            # stats
            tempDiffs <- statType(tempOne,na.rm=T) - statType(tempTwo,na.rm=T)
            summaryPerm$jointDist[i] <- tempDiffs # statType(tempDiffs, na.rm = T) 
          
        }  
        
    } else {
      
        for (i in 1:nPerms){
          
            # shift labels in a pairwise fashion
            tempDiffs <- statType((-1)^rbinom(lOne,1,0.5) * (group1 - group2))
            summaryPerm$jointDist[i] <- tempDiffs
          
        }
      
    }
  
    # get the observed difference
    diffs <- statType(group1,na.rm=T) - statType(group2,na.rm=T)
    observedAbs <- abs(diffs) # maybe leave it as means here
    observed <- diffs
    summaryPerm$Pval <- 2 * (1 - ecdf(summaryPerm$jointDist)(observedAbs))
    if (length(unique(abs(summaryPerm$jointDist))) == 1) {summaryPerm$Pval <- 1} # if the difference is always the same, then p = 1
    summaryPerm$Observed <- observed
    
    # return the results
    if (simple) {
      
      return(summaryPerm$Pval)
      
    } else if (!simple) {
      
      return(summaryPerm)
      
    }
    
}

# Non-parametric Bootstrap for a single group
bootstrap <- function(group = 1, statType = mean, B = 5000){
  
    # prep param
    bootStats <- rep(0,B)
    
    # iterate
    for(b in 1:B){
      
      # wait group
      x <- sample(group,length(group),replace=T)  
      bootStats[b] <- statType(x,na.rm = T)
      
    }
    
    return(bootStats)
    
}

# basic power calculation for a one way ANOVA
powerCalc <- function(d = 1.2, Za = 1.96, Zb = 0.842){
  
  out <- 2*((Za + Zb)/d)^2 + (0.25*(Za^2))
  
  return(round(out))
  
}

# OC-specific computation of the negative log likelihood for model optimization
negLogLik <- function(params, choice, handling, reward) {
  gamma <- exp(params[2]) * handling
  model <- exp(params[1]) * (reward - gamma)
  p = 1 / (1 + exp(-model))
  p[p == 1] <- 0.999
  p[p == 0] <- 0.001
  tempChoice <- rep(NA, length(choice))
  tempChoice[choice == 1] <- log(p[choice == 1])
  tempChoice[choice == 0] <- log(1 - p[choice == 0]) # log of probability of choice 1 when choice 0 occurred
  negLL <- -sum(tempChoice) 
  return(negLL)
}

# optimize the OC model
optimizeOCModel <- function(Data, Algorithm = "NLOPT_LN_NEWUOA", simplify = F, optfun = negLogLik) {
  
  # Data: The participant's log
  # Algorithm: probably let be
  # optfun: an external function to minimize (in this case OC, separately defined as negloglik)
  
  # Prep data
  handling <- Data$Handling
  reward <- Data$Offer
  choice <- Data$Choice
  
  # Prep list of results to be returned
  out <- list()
  out$percentQuit <- mean(choice == 0) * 100
  out$percentAccept <- mean(choice== 1) * 100 
  miss <- (choice != 1) & (choice != 0)
  out$percentMiss <- mean(miss)  * 100
  choice <- choice[!miss]
  reward <- as.numeric(reward[!miss])
  handling <- as.numeric(handling[!miss])
  
  # Establish lower and upper bounds
  LB <- round(log((min(reward)/max(handling)) * 0.99), digits = 4)
  UB <- round(log((max(reward)/min(handling)) * 1.01), digits = 4) # in reality this should be the second largest, since no one would reject the highest val
  
  # Begin defining parameters
  # If choices are one-sided (i.e. all accepted)
  if ((sum(choice) == length(choice)) | (sum(choice) == 0)) {
    ifelse(sum(choice) == length(choice), out$Gamma <- exp(LB), out$Gamma <- exp(UB)) 
    out$Scale <- 1 # it was NA, but in theory a temperature of 1 also indicates noiseless estimates, and allows for easier fit computations
    out$LL <- 0
  } else {
    # Create a feasible region (search space)
    params <- as.matrix(expand.grid(scale = c(-1, 1), gamma = seq(LB, UB, length = 3)))
    # Create a list to check the minimization of the negative log lik.
    info <- list()
    info$negLL <- Inf
    # Define the options to be used during optimization
    # Consider looking into other optimization algorithms and global minima
    opts <- list("algorithm" = Algorithm,
               "xtol_rel" = 1.0e-8)
    # Optimize the sOC over all possible combinations of starting points
    for (i in seq(nrow(params))) {
      tempInfo <- nloptr(x0 = params[i, ], 
                   eval_f = optfun, 
                   lb = c(log(0.001), LB),
                   ub = c(-log(0.001), UB),
                   opts = opts,
                   choice = choice, 
                   handling = handling,
                   reward = reward)
      if (tempInfo$objective < info$negLL) {
        #print("Minimized")
        info$negLL <- tempInfo$objective
        info$params <- tempInfo$solution
      }
    }
    out$Gamma <- exp(info$params[2])
    out$Scale <- exp(info$params[1])
    out$LL <- -info$negLL
  }
  
  # Summarize the outputs
  out$LL0 <- log(0.5) * length(choice)
  out$Rsquared <- 1 - (out$LL/out$LL0)
  out$subjOC <- out$Gamma * handling
  out$p <- 1 / (1 + exp(-(out$Scale*(reward - out$subjOC))))
  out$predicted <- reward > out$subjOC
  out$predicted[out$predicted == TRUE] <- 1
  out$percentPredicted <- mean(out$predicted == choice) 
  
  # adjust the probabilities in case of extreme gammas
  if (out$Gamma <= exp(LB)) {
    out$Gamma <- exp(LB) # temporary condition because Nlopt is not respecting the lower bound
    out$p <- rep(1, length(choice))
  } else if (out$Gamma == exp(UB)) {
    out$p <- rep(0, length(choice))
  } 
  
  # if doing this with dplyr::do(), return a simplified data.frame instead with the important parameters
  if (simplify) {
    out <- data.frame(out[c(seq(8), 12)])
  }   
  
  return(out)

}

# summary matrices for refrence-changing models
betaMatrix <- function(model, rearrange = NA) {
# get a similarity matrix of the resulting coefficient pairings for the cost conditions
# first, do a full_join based on column names on the list of coefficient vectors from each dummy code relevel
# then match the names of columns and rows so NAs are in the diagonal
  
  # get the names of the reference group per model iteration
  refnames <- names(model)
  
  # coefficient matrix
  temp <- lapply(model, function(data) {coefficients(data)$SubjID[1, 2:4]})
  mixCoeffs <- bind_rows(temp) 
  preln <- ifelse("Cost" %in% substr(names(mixCoeffs), 1, 4), 4, 5) # count how many characters precede the name of each cost (diff across studies)
  dimnames(mixCoeffs) <- list(refnames, substr(names(mixCoeffs), preln + 1, 20))
  mixCoeffs <- as.matrix(mixCoeffs[, match(rownames(mixCoeffs), colnames(mixCoeffs))])
  mixCoeffs[is.na(mixCoeffs)] <- 0
  
  # now the pvals
  temp <- lapply(model, function(data) {as.list(summary(data)$coefficients[2:4, 4])})
  mixPvals <- as.matrix(bind_rows(temp)) 
  dimnames(mixPvals) <- list(refnames, substr(colnames(mixPvals), preln + 1, 20))
  mixPvals <- as.matrix(mixPvals[, match(rownames(mixPvals), colnames(mixPvals))])
  mixPvals[is.na(mixPvals)] <- 1

  # if you would like to re-arrange the coefficient order, supply a vector with the desired sequence
  if (length(rearrange) > 1) {
    mixCoeffs <- mixCoeffs[rearrange, rearrange]
    dimnames(mixCoeffs) <- list(rearrange, rearrange)
    mixPvals <- mixPvals[rearrange, rearrange]
    dimnames(mixPvals) <- list(rearrange, rearrange)
  }
  
  # combine matrices into list to return
  out <- list(Betas = round(mixCoeffs, digits = 2),
              Pvals = round(mixPvals, digits = 5))
  
  return(out)
  
}

# gather basic data (this is better as a function, then rbind)
summarizeData <- function(Data = waitData, nSubs = nSubsW, type = "Wait"){
  
  tempSummary <- data.frame(group = rep(type, nSubs))
  
  if (type == "Cognitive") {
    
    for (subj in seq(nSubs)){
      
        # Choice index
        tempChoice <- Data[[subj]]$Choice
        
        # Proportion complete
        tempSummary[subj,2] <- mean(tempChoice)
      
        # earnings
        tempSummary[subj,3] <- sum(Data[[subj]]$Offer[tempChoice==1])
        
        # RTs
        tempSummary[subj,4] <- median(Data[[subj]]$`Choice RT`[tempChoice==0])
    
        # Mistakes (only for cognitive effort group)
        tempMistakes <- sum(cogDataLong[[subj]]$`Trial Outcome`=="Forced travel") # how many forced travel trials
        tempTrialnum <- length(Data[[subj]]$Choice) # how many decision trials
        tempSummary[subj,5] <- tempMistakes / (tempMistakes + tempTrialnum) # proportion of forced travels throughout the exp        
    
    }    
    
  } else {
    
    for (subj in seq(nSubs)){
        
        # choice index
        tempChoice <- Data[[subj]]$Choice
      
        # propotion complete
        tempSummary[subj,2] <- mean(tempChoice)
        
        # earnings
        tempSummary[subj,3] <- sum(Data[[subj]]$Offer[tempChoice==1])
        
        # RTs
        tempSummary[subj,4] <- median(Data[[subj]]$RT[tempChoice==0])
        
        # Mistakes (to allow for mistakes on the cognitive group)
        tempSummary[subj,5] <- NA
        
    }
  
  }
  
  return(tempSummary)
  
}

```

```{r Load data for between-subject experiment, echo = FALSE}
### data loading and cleaning up
# forced travels are switched to acceptances, as they reflect a preference for that trial
# rawChoice will be used to compute the # of mistakes
# The RT is upper-bounded because a glitch in the code made two 14s trials last longer (among all p's) 
setwd("./Cost2/data")
files <- dir(pattern = '_log.csv')

# load data
dataBtw <- tibble(SubjID = files) %>%
  mutate(contents = map(SubjID, ~ suppressWarnings(read_csv(., col_types = cols()))))  %>%
  mutate(Cost = substring(SubjID, 5, 8),
         Cost = case_when(Cost == "wait" ~ "Wait",
                          Cost == "cogT" ~ "Cognitive",
                          Cost == "phys" ~ "Physical",
                          Cost == "phea" ~ "Easy"),
         SubjID = as.integer(substring(SubjID, 0, 3))) %>%
  unnest() %>%
  rename(TrialN = X1) %>%
  mutate(rawChoice = Choice, 
         RT = ifelse(RT > 14.1, 14, RT),
         Choice = ifelse(Choice == 2, 1, Choice), # forced travels (2) become acceptances (1)
         Half = ifelse(Block < 4, "Half_1", "Half_2"),
         Cost = factor(Cost, levels = c("Physical", "Cognitive", "Wait", "Easy")),
         optimal = case_when(
          (Handling == 10 & Offer < 8) ~ 0,
          (Handling == 14 & Offer < 20) ~ 0,
          TRUE ~ 1
          )
         ) 

# get a simple subject list and the number of subjects
subjList_btw <- unique(dataBtw$SubjID)
nSubjs_btw <- length(subjList_btw)

```

```{r Load data for within-subject experiment, echo = FALSE}
# First looks at the new data
# The RT is upper-bounded because a glitch in the code made one 10s last 14s
setwd('./Cost3/data/')
files <- dir(pattern = 'main_log.csv')

# load the data and remove extreme subjects
dataWth <- data_frame(SubjID = files) %>% 
  mutate(contents = map(SubjID, ~ suppressWarnings(read_csv(., col_types = cols()))))  %>%
  mutate(SubjID = substring(SubjID, 0, 3)) %>%
  unnest() %>%
  mutate(RT = ifelse(RT > 10.1, 10, RT),
         Half = ifelse(Block < 4, "Half_1", "Half_2"),
         Btype = BlockType) %>%
  unite(Cost, Cost, BlockType) %>%
  rename(TrialN = X1) %>%
  mutate(rawChoice = Choice,
         Choice = ifelse(Choice == 2, 1, Choice),
         Cost = case_when(Cost == "WAIT_0" ~ "Wait_C",
                           Cost == "COGNITIVE_0" ~ "Cognitive",
                           Cost == "GRIP_1" ~ "Physical",
                           Cost == "WAIT_1" ~ "Wait_P")) %>%
  group_by(SubjID) %>%
  mutate(BlockOrder = ifelse(Btype[1] == 0, "Cognitive1st", "Physical1st"))

# Get just the subject list and number of subjects
subjList_wth <- unique(dataWth$SubjID)
nSubjs_wth <- length(subjList_wth)
```

1.  Department of Psychological and Brain Sciences, Boston University, Boston, USA

2.  Center for Systems Neuroscience, Boston University, Boston, USA

\bigskip

Corresponding authors: Claudio Toro-Serey (ctoro@bu.edu) & Joseph T. McGuire (jtmcg@bu.edu)


\newpage

## Abstract

Cognitive and physical effort are typically regarded as costly, but recent findings have suggested that exerting effort can boost the value of prospects under certain conditions. Here we embedded mental and physical effort in a "diet choice" foraging task, which required decision makers not only to evaluate the magnitude and delay of a focal prospective reward, but also to estimate the general opportunity cost of time. In two experiments, independent sets of participants collected rewards that required equivalent periods of cognitive effort, physical effort, or unfilled delay. Monetary offers varied per trial, and the two experiments differed in whether the type of effort or delay cost was the same on every trial (between-subjects, n=21 per condition), or varied across trials (within-subjects, n=48). Participants were free either to accept or reject the cost/reward prospect offered on each trial. All participants were more likely to accept offers when rewards were higher and delays shorter, in line with a reward-maximizing strategy. Participants almost never reversed their acceptance decisions, and error rates in completing the effort requirement were low and decreased over time. When participants faced only one type of cost, cognitive effort persistently produced the highest acceptance rate compared to trials with an equivalent period of either physical effort or unfilled delay. This could be because cognitive effort was directly rewarding, or because it modulated foraging-related factors such as the perceived duration of delays or the estimated richness of the environment. We theorized that if cognitive effort were intrinsically rewarding, we would observe the same pattern of preferences when participants foraged for varying cost types in addition to rewards. In the within-subject experiment, an initially higher acceptance rate for cognitive effort trials disappeared over time amid an overall decline in acceptance rates as participants gained experience with all three conditions. Our results extend the view that cognitive demands may reduce the discounting effect of delays, but also suggest that differences in cost can eventually fade if individuals actively experience alternative forms of demand. Rather than assigning intrinsic value to cognitive effort, our findings support the idea that a cognitive effort requirement might influence contextual factors such as subjective delay durations or the perceived opportunity cost of time. Such altered estimations can be recalibrated if multiple forms of demand are interleaved.


## Introduction


## Methods


## Results

### **Between-subjects: Tests of whether decision makers integrate delay and reward information**

### 16.1.1.

  *To address hypothesis 4.1., A logistic regression will be fit for each participant in order to predict trial-wise acceptances, using handling time and reward amount as predictors. The resulting beta coefficients for handling time and reward will be pooled across all participants, and we will perform a one-sample rank-sum test on each set of coefficients to examine whether the they are significantly positive or negative (compared to 0). If the group coefficients are significantly positive, it would mean that a predictor reliably increases the likelihood of acceptance. This will allow us to determine whether increments in handling time and reward amounts increased and decreased the likelihood of acceptance for each participant, respectively.* 
  
``` {r 16.1.1., fig.align="center", fig.width = 4, fig.height = 4, echo = FALSE}
# filterout errors and compute R + H logistic per subject, then get coefficients
logisRH <- dataBtw %>%
  filter(Choice < 2) %>%
  plyr::dlply("SubjID", identity) %>%
  lapply(function(data) {glm(Choice ~ Handling + Offer, data = data, family = "binomial")}) %>%
  sapply(coefficients)

# Rank sum tests
rankHand <- wilcox.test(logisRH["Handling", ])
rankOffer <- wilcox.test(logisRH["Offer", ])

# Plot coeffs
temp <- melt(logisRH[c("Handling", "Offer"), ])
qplot(data = temp, x = Var1, y = value, geom = "boxplot") + 
  labs(x = "", y = "Coefficients") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme(legend.position = c(0.9, 0.7),
       panel.grid.major = element_blank(), 
       panel.grid.minor = element_blank(), 
       panel.background = element_blank(), 
       axis.line = element_line(colour = "black"),
       text = element_text(size = 16))
```

The plot shows the pooled coefficients across subjects for the effect of each predictor. The results show the expected discounting effect of increasing the handling time, as well as the increase in acceptance likelihood as a function of reward increments. We found that these coefficients were significantly different from zero for both handling (V = `r rankHand$statistic`, p `r ifelse(rankHand$p.value < 0.001, "< 0.001", paste("=", round(rankHand$p.value, digits = 2)))`) and reward (V = `r rankOffer$statistic`, p `r ifelse(rankOffer$p.value < 0.001, "< 0.001", paste("=", round(rankHand$p.value, digits = 2)))`) regardless of cost condition.

\bigskip

### 16.1.2.	

*We will perform an extension of the logistic regression from 16.1.1., this time adding an autoregressive covariate containing the number of consecutive quits prior to a given trial. In this way, we will examine the possibility that participant choices were governed by recent quitting history rather than the experimental parameters (see 11.1.3.). Coefficients not significantly different from 0 will denote that a participant did not rely on recent quitting history.*
  
``` {r 16.1.2., echo = FALSE, include = FALSE}
# add an AR regressor to track recent quits
logisRHAR <- dataBtw %>%
              filter(Choice < 2) %>%
              group_by(SubjID) %>%
              mutate(AR = seqQuits(Choice)) %>%
              plyr::dlply("SubjID", identity) %>%
              lapply(function(data) {glm(Choice ~ Handling + Offer + AR, data = data, family = "binomial")}) 
  
# Rank sums
rankAR <- logisRHAR %>% 
  sapply(coefficients) %>%
  wilcox.test(.["AR", ])

AR_CIs <- sapply(logisRHAR, function(x) {tryCatch(confint(x)[4, ], error = function(e){c(0, 0)})}) %>%
  replace_na(0) %>%
  t() %>% 
  apply(1, function(x) !(x[1] <= 0 & x[2] >= 0))

```

To measure the significance of each subject's autoregressive coefficient, we computed its CI and checked how many contained zero. By this measure, `r sum(AR_CIs)` out of `r nSubjs_btw` of our participants seemed to have been influenced by recent quitting. However, the autoregressive predictor did not preclude the effect of the remaininig experimental parameters.

\bigskip

### 16.1.3.	

*A general linear model with constant, linear, and quadratic terms will be used to estimate the correspondence between proportion accepted (independent variable) and total earnings (dependent variable). No other covariates will be used, as this analysis is to confirm that over and under accepting are detrimental to total earnings. The quadratic term will be defined as the squared deviation from the optimal overall acceptance rate.*
  
``` {r 16.1.3. Earnings, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4}
# calculate the proportions and earnings per subject, while keeping group info
temp <- dataBtw %>%
  filter(rawChoice < 2) %>%
  group_by(SubjID, Cost) %>%
  summarise(Earnings = sum(Offer[Choice == 1] / 100),
            pComplete = mean(Choice)) %>%
  mutate(pComplete2 = pComplete^2) %>%
  ungroup()

# linear fit of earnings x prop accept
lmEarn <- lm(Earnings ~ pComplete + pComplete2, data = temp)

# plot
(earnFitplot <- ggplot(data = temp, aes(pComplete, Earnings)) + 
         scale_color_manual(values = colsBtw) + 
         geom_point(aes(fill = Cost), pch = 21, color = "black", size = 3, show.legend = T) + 
         stat_smooth(aes(y = Earnings), method = "lm", formula = y ~ x + I(x^2), color = "grey30") +
         labs(x = "Proportion Completed", y = "Total Earnings (dollars)") +
         scale_fill_manual(values = colsBtw) +
         theme(panel.grid.major = element_blank(), 
           panel.grid.minor = element_blank(), 
           panel.background = element_blank(), 
           axis.line = element_line(colour = "black"),
           text = element_text(size = 16)))

```

The figure shows that participants that over and unceraccepted earned less money overall, as predicted. This is supported by a significant quadratic term from the linear model (F = `r round(summary(lmEarn)$fstatistic[1], digits = 2)`, Beta = `r summary(lmEarn)$coefficients[3,1]`, SE = `r round(summary(lmEarn)$coefficients[3,2], digits = 2)`, R-squared = `r round(summary(lmEarn)$r.squared, digits = 2)`; black line shows the fit). The following table shows all the results from the model.

``` {r 16.1.3. Earnings table, echo=FALSE,fig.align="center",fig.width=9,fig.height=5}
# Summary table for the linear model
panderOptions("digits", 2)
pander(lmEarn, style = "rmarkdown")
```  
  
\bigskip
  
### 16.1.4.	

*To determine the optimality of each group’s decisions, we will perform two-sided one-sample t-tests (with mu being the optimal proporion of acceptances--either 0 or 1) to see if the proportion of acceptances for each time/reward combination was significantly different from the optimal rate (see 11.2.). This will result in 36 independent tests (3 rewards amounts, 3 handling/travel time combinations, and 4 groups), so we will correct for multiple comparisons using False Discovery Rate (FDR).*

``` {r 16.1.4., echo=FALSE}
# in order
# clean data, 
# calculate each individual's prop. accept,
# calculate t.tests with optimality as null mean,
# correct pvalues with FDR
temp <- dataBtw %>%
  filter(Choice < 2) %>%
  group_by(SubjID, Handling, Offer, Cost) %>%
  summarise(pAccept = mean(Choice)) %>%
  mutate(optimal = case_when(
          (Handling == 10 & Offer < 8) ~ 0,
          (Handling == 14 & Offer < 20) ~ 0,
          TRUE ~ 1
        )) %>% 
  group_by(Handling, Offer, Cost) %>%
  summarise(meanAccept = mean(pAccept),
            sdAccept = sd(pAccept),
            optimal = unique(optimal),
            pvals = tryCatch(t.test(pAccept, mu = unique(optimal))$p.value, error = function(e) {1})) %>%
  mutate(FDR = p.adjust(pvals, method = "BY"),
         significant = FDR < 0.05)


# proportion of significant ones
# eventually find a way to plot this
splitData_prop <- mean(temp$significant, na.rm = T)
```

Of the 36 tests, around `r splitData_prop` were significantly deviant from optimality. Most of these were from effortful groups, as the wait group just showed deviations for 2 seconds handling/4 cent, and 10 seconds handling/8 cent offers (note to self: think about a way to properly visualize these).

\bigskip

### **Between-subjects: Comparisons among the four delay and effort conditions**

### 16.2.1.	

*To compare preferences (hypothesis 4.2.), we will first perform a one-way ANOVA on the proportion of trials accepted using group as a factor. In addition, we will do pairwise comparisons on the proportion completed among all 4 groups using non-parametric permutation contrasts (6 tests). The same approach will be used for total earnings. This will give us an initial glimpse on the potential differences in cost among conditions.*

``` {r 16.2.1. Overall Proportion/Earnings, fig.align = "center", fig.width = 8, fig.height = 4, echo = FALSE}  
## ECDF of proportion completed per cost type
temp <- dataBtw %>%
        filter(Choice < 2) %>%
        mutate(Cost = factor(Cost, levels = list("Physical", "Cognitive", "Wait", "Easy"))) %>%
        group_by(SubjID, Cost) %>%
        summarise(Earnings = sum(Offer[rawChoice == 1] / 100),
                  pAccept = mean(Choice))

## formal testing 
# proportion completed
propAov <- aov(pAccept ~ factor(Cost), data = temp)

propTests <- temp %>% 
  plyr::dlply("Cost", identity) %>%
  sapply("[[", "pAccept") %>%
  as.data.frame() %>%
  combn(2, simplify = F) %>%
  sapply(function(x) {
    c(contrast = paste(colnames(x), collapse = "vs"), 
      permPval = round(permute(x[, 1], x[, 2], simple = T), digits = 3),
      cohenD = round(cohenD(x[, 1], x[, 2]), digits = 3))
  }) %>% 
  t()

# pairwise.wilcox.test(temp$pComplete, temp$Cost)

# earnings
earnAov <- aov(Earnings ~ factor(Cost), data = temp)

earnTests <- temp %>% 
  plyr::dlply("Cost", identity) %>%
  sapply("[[", "Earnings") %>%
  as.data.frame() %>%
  combn(2, simplify = F) %>%
  sapply(function(x) {
    c(contrast = paste(colnames(x), collapse = "vs"), 
      permPval = round(permute(x[, 1], x[, 2], simple = T), digits = 3),
      cohenD = round(cohenD(x[, 1], x[, 2]), digits = 3))
  }) %>% 
  t()

## plots
# ECDF pAcceptd
p1 <- ggplot(aes(pAccept, color = Cost), data = temp) +
        stat_ecdf(lwd = 1.2, show.legend = F) +
        scale_color_manual(values = colsBtw) +
        xlim(0, 1) +
        labs(y="ECDF", x = "Proportion Accepted") +
        theme(panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line = element_line(colour = "black"),
              text = element_text(size = 16))

# pAccepted
p2 <- ggplot(temp, aes(Cost, pAccept, fill = Cost)) + 
        geom_boxplot(show.legend = F) +
        scale_fill_manual(values = colsBtw) +
        ylim(c(0, 1)) +
        labs(y = "Proportion Accepted") +
        theme(panel.grid.major = element_blank(), 
            panel.grid.minor = element_blank(), 
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black"),
            text = element_text(size = 16))

# earnings
p3 <- ggplot(temp, aes(Cost, Earnings, fill = Cost)) + 
        geom_boxplot(show.legend = F) +
        scale_fill_manual(values = colsBtw) +
        labs(y = "Total Earnings (dollars)") +
        theme(panel.grid.major = element_blank(), 
            panel.grid.minor = element_blank(), 
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black"),
            text = element_text(size = 16))

grid.arrange(p2, p3, ncol = 2)
```

The plots show that subjects in the wait condition accepted the least and earned the most, suggesting a more optimal pattern of choices. Participants in the cognitive effort and easy conditions had higher more variable acceptance patterns, which are reflected in the comparatively low earnings. The reason why the easy condition does not show variable earnings like the cognitive effort group is probably due to the quadratic relationship between earnings and acceptances shown before.

\bigskip

### 16.2.2.

*In order to further look at the effect of delay, work, and rewards, we will perform a repeated measures ANOVA on the proportion completed for each combination of factors. Offer and handling time will be within-subject factors, and condition a between-subjects factor. In support of hypotheses 4.1. and 4.2., we anticipate significant main effects of handling time, reward, and cost condition, but no interactions.*
  
``` {r 16.2.2. rmANOVA, echo=FALSE,fig.align="center",fig.width=5,fig.height=4, results = 'asis'}
temp <- dataBtw %>%
        filter(Choice < 2) %>%
        group_by(SubjID, Cost, Handling, Offer) %>%
        summarise(pAccept = mean(Choice))

## REPEATED MEASURES ANOVA
# I think I like this the most
prop.aov <- with(temp, aov(pAccept ~ factor(Cost) * Handling * Offer + 
                                 Error(SubjID / (Handling * Offer))))

## NOTES ON RESULTS
# According to the logistic fit, there are main effects for all predictors, but no interactions. It also says that the overall odds for the wait group to accept versus the effort is around 16%. Similarly, increasing the handling time results in a decrease in the multiplicative odds of ~25%. Increments in reward increase the odds of acceptance by ~40%.
```

The plot below shows the mean proportion of acceptances (± SEM) per combination of handling time, reward, and condition (optimal acceptance rate indicated by the gray triangles). Visually, the graph confirms a couple of important intuitions. First, as the handling time increases, the proportion of acceptances decreases. Second, this discounting did not affect the 20 cent offers, as is expected from the present foraging environment. Lastly, and similar to the previous point, as the value of the offers increases so does the willingness to accept a trial. Beyond these general features, it is clear that the cognitive effort group accepted more than the other groups, regardless of the combination of experimental parameters. Notably, the optimality of this greater acceptance rate is determined by the timing context (e.g. optimal at 2 second handling time, but detrimental at 14 seconds).      

``` {r 16.2.2. rmANOVA plot, echo = FALSE, fig.align = "center", fig.width = 10, fig.height = 6}
# # code to calculate the rwd/sec rate for each acceptance threshold per handling
# rwd <- c(4, 8, 20)
# handling <- c(14, 10, 2) + 2 # add 2 to H and T to account for offer and reward disclosure windows
# travel <- c(2, 6, 14) + 2
# sapply(3:1, function(thresh) {(sum(tail(rwd, n = thresh))) / ((handling * thresh) + (travel * 3))})
# 
# as_tibble(rwdRates) %>% 
#   mutate(Handling = handling) %>%
#   rename(Five = V1,
#          Eight = V2,
#          Twenty = V3) %>%
#   gather(Reward, Rate, -Handling) %>%
#   mutate(Reward = factor(Reward, levels = list("Five", "Eight", "Twenty")),
#          Handling = as.character(Handling)) %>%
#   group_by(Handling) %>%
#   mutate(Optimal = ifelse(Rate == max(Rate), max(Rate), -1)) %>%
#   ungroup() %>%
#   ggplot(aes(Reward, Rate, group = Handling, color = Handling)) +
#     geom_point(size = 3) +
#     geom_line(size = 1.5) +
#     geom_point(aes(Reward, Optimal), pch = 21, color = "black", fill = NA, size = 7, show.legend = F) +
#     scale_color_manual(values = c("purple", "grey50", "darkgoldenrod3")) +
#     ylim(0, 1.2) +
#     labs(y = "Expected eward per sec.", x = "Reward acceptance threshold") +
#     theme(legend.position = c(0.85, 0.2),
#           legend.key = element_blank(),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 22))

# prep data to plot the p(accept) based on reward, handling time and cost type
trueProp <- dataBtw %>%
        filter(Choice < 2) %>%
        group_by(SubjID, Cost, Handling, Offer, optimal) %>%
        summarise(pAccept = mean(Choice)) %>%
        group_by(Cost, Handling, Offer, optimal) %>%
        summarise(meanComplete = mean(pAccept),
                  SE = sd(pAccept) / sqrt(length(pAccept)))

# and plot
(a1 <- ggplot(data = trueProp, aes(interaction(Offer, Handling), meanComplete, color = Cost)) + 
        geom_point(size = 3) + 
        geom_errorbar(aes(ymin = meanComplete - SE, ymax = meanComplete + SE), width = 0.2, size = 1) +
        geom_line(aes(group = interaction(Handling, Cost)), size = 1) +
        geom_point(aes(y = round(optimal)), shape = 21, fill = "grey75", color = "grey30", size = 3, show.legend = F) +
        labs(x = "Offer.Handling", y = "Proportion Accepted") +
        scale_color_manual(values = colsBtw) +
        theme(legend.key = element_blank(),
              panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line = element_line(colour = "black"),
              text = element_text(size = 16)))

```

These intuitions were formally tested using a repeated measures ANOVA, whose results are presented in the table below. Overall, the analysis partially confirmed our predictions. While all main effects were significant, there was an unexpected significant condition-by-reward interaction, which could be due to the performance of the "easy" group relative to their peers. Importantly, we found that the interaction between all three main parameters was not significant, thus suggesting that the effects of handling and reward on choices were not different across groups.

``` {r 16.2.2. rmANOVA table, pander}
# anova table
x <- summary(prop.aov)   
x <- x$`Error: Within`
panderOptions("digits", 2)
pander(x, style = "rmarkdown", split.table = 110)
```

\bigskip

### 16.2.3.	

*We will compute the probability of accepting a trial with a mixed-effects logistic regression. Based on the task structure and our main question, our a priori model of interest will include cost condition, handling time, and reward amount as fixed main effects, and subject ID as a random effect. Cost condition will be modeled with three categorical terms, with the fourth condition as the reference condition. We will run three versions of the model with different reference conditions, in order to test all pairwise differences among the four cost conditions. As with 16.2.2., we anticipate significant main effects (coefficients different than zero) of handling time, reward, and cost condition. We hypothesize that the differences among cost conditions will follow the pattern described in 4.2.*

``` {r 16.2.3. Mixed Effects Model, echo = FALSE, fig.align = "center", fig.width = 5, fig.height = 4}
# clean the data so the total acceptances and quits per subj x handling x reward are ready for modeling
mixLogis_main <- list()
mixData <- dataBtw %>%
  filter(Choice < 2) %>%
  group_by(SubjID, Cost, Handling, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice)) %>%
  ungroup()


# model with a random intercept and cost-type slope, then relevel the cost to get all pairwise comparisons
# the correct way to model a logistic with proportions as dependent vars is by prividing n of hits and quits as a matrix
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait"))
mixLogis_main$Wait <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Cognitive"))
mixLogis_main$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Physical"))
mixLogis_main$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Easy"))
mixLogis_main$Easy <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)

# matrix of coefficients and pvalues
rearrange <- c("Cognitive", "Easy", "Wait", "Physical") # to kinda match the order in Cost 3
mixSummary <- betaMatrix(mixLogis_main, rearrange = rearrange)
diag(mixSummary$Pvals) <- NA

# plot coefficients and pvalues in a matrix
# invert colors
col2 <- colorRampPalette(c("#053061", "#2166AC", "#4393C3", "#92C5DE", "#D1E5F0", "#FFFFFF", "#FDDBC7", "#F4A582", "#D6604D", "#B2182B", "#67001F"))

corrplot(mixSummary$Betas, 
         is.corr = F, 
         p.mat = mixSummary$Pvals, 
         type = "lower",
         outline = T,
         #insig = "p-value",
         sig.level = 0.05, 
         na.label = "square", 
         na.label.col = "grey",
         method = "color", 
         tl.col = "black",
         tl.cex = 0.8,
         col = col2(200))

```


``` {r 16.2.3. Mixed Effects Model plot rand intercepts, echo = FALSE, fig.align = "center", fig.width = 5, fig.height = 4}
# plot random intercepts in increasing order
#RI1 <- qplot(seq(83), sort(ranef(mixLogis_main$Wait)$SubjID[,1]), color = colsBtw[1], show.legend = F) + theme_classic()

# # plot per subject (as expected, the intercepts are highest for those who accepted everythiing)
# temp <- ranef(mixLogis_main$Wait)$SubjID %>%
#   gather(key = Cost, value = RandEffects, colnames(.)) 
# 
# (RI1 <- ggplot(data = temp, aes(Cost, RandEffects, fill = Cost)) + 
#   geom_hline(yintercept = 0, alpha = 0.7, linetype = "dashed") + 
#   geom_boxplot(show.legend = F) +
#   scale_fill_manual(values = colsBtw) + 
#   labs(x = "") +
#   theme(panel.grid.major = element_blank(), 
#               panel.grid.minor = element_blank(), 
#               panel.background = element_blank(), 
#               axis.line = element_line(colour = "black"),
#               text = element_text(size = 16)))

```

As predicted, the model showed significant main effects of handling time (Beta = `r mixLogis_main$Summary$coefficients[5,1]`, SE = `r mixLogis_main$Summary$coefficients[5,2]`, p < 0.001) and reward (Beta = `r mixLogis_main$Summary$coefficients[6,1]`, SE = `r mixLogis_main$Summary$coefficients[6,2]`, p < 0.001). In order to show the comparisons among all conditions, the following plot portrays the coefficients (color scale), and p-values (numbers within squares) that resulted from switching the reference condition. Specifically, each entry shows how much more likely was the reference group to accept an offer than the comparison group. Based on this, we can see that the cognitive group was significantly more likely to accept any offer than the physical and wait groups, but not the easy group.


### 16.2.4.

*Next, we will examine whether the a priori model from 16.2.3. outperforms both simpler and more complex models. Unlike the individual logistic regression fits in 16.1.1., a mixed-effects approach gives us a better goodness of fit measure for model comparisons. We will determine the best model (combination of predictors) using Akaike’s Information Criterion (AIC) to determine the model that minimizes the negative log-likelihood while penalizing the addition of parameters. The regression with each combination of predictors will be fitted in the following order: 1) intercept only; 2) condition only; 3) handling time only; 4) reward only; 5) condition, handling, and reward main effects (from 16.2.3.); 6) adding a handling-by-reward interaction; and 7) adding all three possible two-way interactions. We predict that model 5 will have the lowest AIC.*


``` {r 16.2.4. Model comparison, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# Quasibinomial GLM versions (with proper proportion setup)
mixLogis_compare <- list()
mixLogis_compare$Base <- glmer(cbind(totalAccepted, totalQuits) ~ 1 + (1 | SubjID), family = "binomial", data = mixData) # intercept only
mixLogis_compare$Cost <- glmer(cbind(totalAccepted, totalQuits) ~ Cost + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare$Handling <- glmer(cbind(totalAccepted, totalQuits) ~ Handling + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare$Offer <- glmer(cbind(totalAccepted, totalQuits) ~ Offer + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare$AllMain <- mixLogis_main$Wait
mixLogis_compare$Interaction <- glmer(cbind(totalAccepted, totalQuits) ~ Cost * Offer + (Cost | SubjID), family = "binomial", data = mixData)

# Model selection 
abicLogis_compare <- data.frame(Model = names(mixLogis_compare),
                                AIC = sapply(mixLogis_compare, AIC),
                                BIC = sapply(mixLogis_compare, BIC))
  
# effect size
mixLogis_compare$Rsq <- 1 - (deviance(mixLogis_compare$AllMain)/deviance(mixLogis_compare$Base)) # denominator is the null deviance
```

``` {r 16.2.4. Plot Deviance comparison, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# plot AIC and BIC
(mixLogis_compare$plot <- abicLogis_compare %>% 
  gather(Comparison, Value, AIC:BIC) %>%
  mutate(Model = factor(Model, levels = names(mixLogis_compare))) %>%
  ggplot(aes(Model, Value, fill = Comparison)) +
    geom_point(color = "black", pch = 21, size = 3) +
    theme_classic())
    
```

This plot shows the AIC for every model. The figure below shows AIC and BIC for all the models. As predicted, the model with all main effects reduced the deviance the most ($R^2$ = `r round(mixLogis_compare$Rsq, digits = 2)`). The reduction in deviance from an intercept-only model was drastic enough that we did not compute an LRT.

\bigskip

### **Between-subjects: Modeling the subjective opportunity cost in each condition**

### 16.3.1.

*Response times (RT) for quit responses will be presented in a descriptive manner in order to examine whether participants tended to quit early or late within individual trials. Each cost group’s response time distribution will contain the pooled RT across its corresponding participants, and we will display the empirical cumulative distribution functions for each condition. Short RT would suggest confident and stable decisions (in support of 4.3.1.).*

``` {r 16.3.1. RTs, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# Proportion of forced travels per subject and cost type
propFails <- dataBtw %>% 
  filter(Cost != "Wait") %>% 
  group_by(SubjID, Cost) %>% 
  summarise(propFails = mean(rawChoice == 2)) %>%
  group_by(Cost) %>% 
  summarise(meanFT = mean(propFails),
            sdFT = sd(propFails))

# this version of a survival curve forces all successful acceptances to have an RT of 14
library(survival)
library(ggfortify)

# that's to avoid jumps from 2s to 10s to 14s
# 1 - Choice is to reverse the coding of choices, since the survival package assumes 0 = censor?
temp <- dataBtw %>%
    #filter(rawChoice < 2) %>% 
    mutate(#RT = ifelse(Choice == 1, 14, RT),
           Choice = 1 - Choice) 

survData <- survfit(Surv(RT, Choice) ~ Cost, data = temp)

autoplot(survData) +
  geom_vline(xintercept = 1, lty = 2) + # end of the grace period for physical trials
  scale_color_manual(values = colsBtw) +
  scale_fill_manual(values = colsBtw) +
  scale_x_continuous(breaks = seq(14)) +
  ylim(0, 1) +
  labs(x = "Time in Trial", y = "Proportion Accepted", fill = "Cost", color = "Cost") +
  theme(legend.position = c(0.3, 0.2),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        text = element_text(size = 22))

# RT ecdfs for the whole group across blocks, with the option to subdivide by cost group
dataBtw %>%
  filter(Choice == 0,
         RT < 1) %>%
  ggplot(aes(RT, group = Block, color = Block)) +
    stat_ecdf() +
    #facet_wrap(vars(Cost)) +
    theme_classic()



```

The survival curve below shows instances of quiting across a trial for all participants and handling times. Censors signal completed trials and forced quits, so most occur at the end of the three possible handling times. Participants could only quit once the trial begun. This shows that most decisions to quit happened within the first second into the handling time, and participants rarely quit during the handling time. The reason why the physical and easy condition show slightly lagged responses is that the experiment allowed a one second grace period for participants to begin gripping (marked by the dashed line). This resulted in some of them choosing to wait for that second to indicate an offer rejection. The proportion of forced quits for cognitive effort was `r round(propFails[2, 2], digits = 2)`, and 0 for the physical conditions (the censor around 4s in the physical condition was an acceptance during 2s handling that was incorrectly recorded at 4s by the experimental code).

\bigskip

### 16.3.2.

*In order to further examine choice stability (hypothesis 4.3.1.), we will compute each participant’s total proportion of acceptances pre- and post-midpoint. For each cost condition separately, we will fit a linear model that predicts post-midpoint acceptance from before-midpoint rates. We will report the slopes and 95% confidence intervals (CI) for each cost group. CIs containing 1 will denote that participants in that group produced consistent choices.*
  
``` {r 16.3.2. Pre/Post Break, echo=FALSE,fig.align="center",fig.width=5,fig.height=5}
# divide the data into proportion accepted before/after the break (one per column)
temp <- dataBtw %>%
  filter(Choice < 2) %>%
  group_by(SubjID, Cost, Half) %>%
  summarise(pAccepted = mean(Choice)) %>%
  ungroup() %>%
  spread(Half, pAccepted)

# separate by cost type and compute a linear model to predict half 2 from 1
sep <- temp %>% plyr::dlply("Cost", identity)
prepost <- list()
prepost$LM <- lapply(sep, function(data) {lm(data$Half_2 ~ data$Half_1)})

# get the coefficients and confidence intervals
# and concatenate
prepost$summary <- as.data.frame(cbind(t(sapply(prepost$LM, coefficients)), t(sapply(prepost$LM, confint))))
colnames(prepost$summary) <- c("Intercept", "Coefficient", "Int_CI-low", "Int_CI-high", "Beta_CI-low", "Beta_CI-high")

## PRE/POST PAIRED PERMUTATIONS
temp <- dataBtw %>%
  filter(Choice < 2) %>%
  group_by(SubjID, Cost) %>%
  summarise(propAccept_1 = mean(Choice[Half == "Half_1"]),
            propAccept_2 = mean(Choice[Half == "Half_2"])) %>%
  plyr::dlply("Cost", identity) 


# paired perms and effect sizes
prepost$CohenD <- sapply(temp, function(data) {DescTools::CohenD(data$propAccept_1, data$propAccept_2)})
prepost$Perms <- sapply(temp, function(data) {permute(data$propAccept_1, data$propAccept_2, paired = T, simple = T)})
```

The plot below shows that acceptance rates before and after the mid-point were mostly consistent, although it was more likely for participants to accept less in the second half of the experiment. **NOTE: The lines of means in the scatterplot are calculated by first computing the mean acceptances per block, then averaging those for blocks pre/post break. That's how they match the second half of the right plot, but the linear model is computed with the overall proportion acceptances pre-post, which gives slightly different mean estimates (which is also the reason for slight mean differences in proportion accepted between these and the boxplot in 16.2.1.)**

``` {r 16.3.2. Plot Pre/Post, echo=FALSE, fig.align = "center", fig.width = 8, fig.height = 4}
# scatter plot of pre/post acceptances
# BIG CAVEAT: THE 
# first calculate the mean acceptances per cost/half to draw horizontal and vertical lines on the plot
df_mean <- dataBtw %>%
            group_by(SubjID, Cost, Block, Half) %>%
            summarise(propAccept = mean(Choice)) %>%
            group_by(SubjID, Cost, Half) %>%
            summarise(propAccept = mean(propAccept)) %>%
            spread(Half, propAccept) %>%
            group_by(Cost) %>%
            summarise(propAccept_1 = mean(Half_1),
                      propAccept_2 = mean(Half_2))


# and plot
prepost$scatterplot <- dataBtw %>%
                        filter(Choice < 2) %>%
                        group_by(SubjID, Cost) %>%
                        summarise(propAccept_1 = mean(Choice[Half == "Half_1"]),
                                  propAccept_2 = mean(Choice[Half == "Half_2"])) %>%
                        ggplot(aes(propAccept_1, propAccept_2, fill = Cost)) +
                          geom_point(size = 3, pch = 21, color = "black", show.legend = F) +
                          scale_fill_manual(values = colsBtw) +
                          scale_color_manual(values = colsBtw) +
                          xlim(0, 1) +
                          ylim(0, 1) +
                          labs(x = "Proportion Accepted - 1st Half", y = "Proportion Accepted - 2nd Half") +
                          geom_abline(slope = 1, intercept = 0, lty = 2) +
                          geom_hline(data = df_mean, aes(yintercept = propAccept_2, color = Cost), linetype="dashed", show.legend = F) +
                          geom_vline(data = df_mean, aes(xintercept = propAccept_1, color = Cost), linetype="dashed", show.legend = F) +
                          theme(panel.grid.major = element_blank(),
                                panel.grid.minor = element_blank(),
                                panel.background = element_blank(),
                                axis.line = element_line(colour = "black"),
                                text = element_text(size = 16))

# acceptances per cost across blocks
# note that because the same handling/travel time combos repeat in the same order pre/post, 1/4-2/4-3/6 are direct comparisons
prepost$propBlocks <- dataBtw %>%
                         filter(Choice < 2) %>%
                         group_by(SubjID, Cost, Block) %>%
                         summarise(propAccept_subj = mean(Choice)) %>%
                         group_by(Cost, Block) %>%
                         summarize(propAccept = mean(propAccept_subj),
                                   seAccept = sd(propAccept_subj)/sqrt(nSubjs_btw)) %>%
                         ggplot(aes(Block, propAccept, color = Cost)) +
                           geom_point(size = 2) +
                           geom_errorbar(aes(ymin = propAccept - seAccept, ymax = propAccept + seAccept, color = Cost), width = 0.1) +
                           geom_line() +
                           ylim(0, 1) +
                           labs(y = "Proportion Acccepted") +
                           scale_x_continuous(breaks = seq(6)) +
                           scale_color_manual(values = colsBtw) +
                           scale_fill_manual(values = colsBtw) +
                           theme(legend.key = element_blank(),
                                 legend.position = c(0.8, 0.25),
                                 panel.grid.major = element_blank(),
                                 panel.grid.minor = element_blank(),
                                 panel.background = element_blank(),
                                 axis.line = element_line(colour = "black"),
                                 text = element_text(size = 16))

grid.arrange(prepost$scatterplot, prepost$propBlocks, ncol = 2)
```

The following table shows the coefficients and CI for each linear model, and show that pre-post acceptance rates were very similar for each condition (i.e. one is present in all confidence intervals). Note that interpreting the coefficients depends on the intercept, and the wait condition had a relatively high intercept. 

A better test is to perform a pairwise comparison between halves per cost type. Paired permutations on means of proportions showed no differences for wait and easy groups (p > 0.5), but significantly lower acceptance rates for physical (*p* = `r prepost$Perms[1]`, Cohen's D = `r prepost$CohenD[1]`) and cognitive (*p* = `r prepost$Perms[2]`, Cohen's D = `r prepost$CohenD[2]`) groups. These results are more consistent with the plots above.


``` {r 16.3.2. prepost table, pander}
panderOptions("digits", 2)
pander(prepost$summary, style = "rmarkdown", split.table = 110)
```

Now let's divide them by reward and handling amounts. The plots below show that the overall rates of acceptances remain mostly consistent across first and second halves of the experimental session.

``` {r 16.3.2. Plot Pre/Post reward x handling, echo=FALSE, fig.align = "center", fig.width = 8, fig.height = 4}
# # pre
# t1 <- dataBtw %>%
#   filter(Choice < 2, Half == "Half_1") %>%
#   group_by(SubjID) %>%
#   filter(Handling == 10) %>%
#   group_by(SubjID, Cost, Offer) %>%
#   summarise(pAccept = mean(Choice)) %>%
#   group_by(Cost, Offer) %>%
#   summarise(propAccept = mean(pAccept),
#             SE = sd(pAccept) / sqrt(nSubjs_btw)) %>%
#   ungroup() %>%
#   mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
#   ggplot(aes(Offer, propAccept, color = Cost)) +
#     geom_point(size = 3, show.legend = F) +
#     geom_line(show.legend = F) +
#     scale_color_manual(values = colsBtw) +
#     scale_fill_manual(values = colsBtw) +
#     geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.3, size = 1, show.legend = T) +
#     scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1)) +
#     scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
#     labs(y = "Proportion Accepted", title = "First Half") +
#     theme(legend.key = element_blank(),
#           legend.position = c(0.8, 0.25),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))
# 
# # post half
# t2 <- dataBtw %>%
#   filter(Choice < 2, Half == "Half_2") %>%
#   group_by(SubjID) %>%
#   filter(Handling == 10) %>%
#   group_by(SubjID, Cost, Offer) %>%
#   summarise(pAccept = mean(Choice)) %>%
#   group_by(Cost, Offer) %>%
#   summarise(propAccept = mean(pAccept),
#             SE = sd(pAccept) / sqrt(nSubjs_btw)) %>%
#   ungroup() %>%
#   mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
#   ggplot(aes(Offer, propAccept, color = Cost)) +
#     geom_point(size = 3, show.legend = F) +
#     geom_line(show.legend = F) +
#     scale_color_manual(values = colsBtw) +
#     scale_fill_manual(values = colsBtw) +
#     geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.3, size = 1, show.legend = F) +
#     scale_y_continuous(limits = c(-0.1, 1.1), breaks = c(0, 0.5, 1)) +
#     scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
#     labs(y = "", title = "Second Half") +
#     theme(legend.position = c(0.8, 0.2),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))
# 
# 
# 
# grid.arrange(t1, t2, ncol = 2)

dataBtw %>%
  filter(Choice < 2) %>%
  group_by(SubjID) %>%
  filter(Handling == 10) %>%
  mutate(Half = ifelse(Half == "Half_1", "First Block", "Last Block")) %>%
  group_by(SubjID, Half, Cost, Offer) %>%
  summarise(pAccept = mean(Choice)) %>%
  group_by(Half, Cost, Offer) %>%
  summarise(propAccept = mean(pAccept),
            SE = sd(pAccept) / sqrt(nSubjs_btw)) %>%
  ungroup() %>%
  mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
  ggplot(aes(Offer, propAccept, color = Cost)) +
    geom_point(size = 3, show.legend = T) +
    geom_line(size = 1, show.legend = F) +
    scale_color_manual(values = colsBtw) +
    scale_fill_manual(values = colsBtw) +
    geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.3, size = 1, show.legend = T) +
    scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1)) +
    scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
    labs(y = "Proportion Accepted") +
    facet_wrap(vars(Half), ncol = 2) +
    theme(legend.key = element_blank(),
          legend.position = c(0.9, 0.25),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          panel.background = element_blank(), 
          axis.line = element_line(colour = "black"),
          text = element_text(size = 16))

```


This prompted the question of whether fitting the mixed logistic model could confirm this shift in preferences. Just as before, the matrix below show the coefficient magnitude (color) and pvalues for differences based on iterating through costs as reference dummy codes.

``` {r 16.3.2. mix pre/post separately, echo = FALSE, fig.align="center", fig.width = 5, fig.height = 4}
## compute the same mixed effects logistic for each half
# mixed logistic for the first half
mixLogis_pre <- list()
mixData <- dataBtw %>%
  filter(Half == "Half_1") %>%
  group_by(SubjID, Cost, Handling, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_pre$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait"))
mixLogis_pre$Wait <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Easy"))
mixLogis_pre$Easy <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Cognitive"))
mixLogis_pre$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)

# mixed logistic for the second half
mixLogis_post <- list()
mixData <- dataBtw %>%
  filter(Half == "Half_2") %>%
  group_by(SubjID, Cost, Handling, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_post$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait"))
mixLogis_post$Wait <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Easy"))
mixLogis_post$Easy <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Cognitive"))
mixLogis_post$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Offer + (Cost | SubjID), family = "binomial", data = mixData)


## now produce a summary matrix
# get the beta and pvalue matrices 
betasPre <- betaMatrix(mixLogis_pre, rearrange = rearrange)
betasPost <- betaMatrix(mixLogis_post, rearrange = rearrange)

# and combine them
betaMat <- matrix(NA, nrow = nrow(betasPre$Betas), ncol = nrow(betasPre$Betas))
betaMat[lower.tri(betaMat)] <- betasPre$Betas[lower.tri(betasPre$Betas)]
betaMat[upper.tri(betaMat)] <- betasPost$Betas[lower.tri(betasPost$Betas)]
dimnames(betaMat) <- list(rearrange, rearrange)

pvalMat <- matrix(NA, nrow = nrow(betasPre$Pvals), ncol = nrow(betasPre$Pvals))
pvalMat[lower.tri(pvalMat)] <- betasPre$Pvals[lower.tri(betasPre$Pvals)]
pvalMat[upper.tri(pvalMat)] <- betasPost$Pvals[lower.tri(betasPost$Pvals)]
dimnames(pvalMat) <- list(rearrange, rearrange)

# remove uninteresting comparisons 
diag(betaMat) <- 0

# aand plot
corrplot(betaMat, 
         is.corr = F, 
         p.mat = pvalMat, 
         outline = T,
         #insig = "p-value",
         sig.level = 0.05, 
         na.label = "square", 
         na.label.col = "grey",
         method = "color", 
         tl.col = "black",
         tl.cex = 0.8,
         col = col2(200))
````

\bigskip

### 16.3.3.

*To estimate the subjective opportunity cost (hypothesis 4.3.2.), we will use a logistic function to model each participant’s probability of completing a trial based on the difference between the delayed reward’s magnitude and the estimated opportunity cost (OC) for each cost type. OC will be computed as the product of a free parameter (gamma) and the handling time. Both gamma and the temperature parameter of the logistic function will be estimated at the subject level, independently for each subject.*

``` {r 16.3.3. OC modeling, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# group
summaryOC <- list()
summaryOC$all <- dataBtw %>%
                 #filter(rawChoice < 2) %>%
                 group_by(Cost, SubjID) %>%
                 do(optimizeOCModel(., simplify = T)) %>%
                 ungroup()

# plot
(summaryOC$plot <- ggplot(summaryOC$all, aes(Cost, Gamma, fill = Cost)) +
                      geom_hline(yintercept = 0.64, alpha = 0.9, color = "gray20", size = 1) +
                      geom_hline(yintercept = 0.7, alpha = 0.9, color = "gray40", size = 1) +
                      geom_hline(yintercept = 0.9, alpha = 0.9, color = "gray60", size = 1) +
                      geom_dotplot(stackdir = 'center', binaxis = 'y', show.legend = F) +
                      ylim(0,1.5) +
                      labs(title = "Gamma Values per Group", x = "") +
                      scale_fill_manual(values = colsBtw) +
                      theme(panel.grid.major = element_blank(),
                        panel.grid.minor = element_blank(),
                        panel.background = element_blank(),
                        axis.line = element_line(colour = "black"),
                        text = element_text(size = 16)))

```

The gamma optimization search space was bound by extreme choice values. For example, someone with an extremely high OC would not accept even the most beneficial offers, and would reject a 20 cent offer when the handling time was 2 seconds (or 20/2). Conversely, a participant experiencing low opportunity costs would accept the unbeneficial offer of 4 cents for a 14 second delay (or 4/14). The model was optimized to find the lowest value of gamma that significantly reduced the negative log likelihood (R-squared: mean = `r mean(summaryOC$all$Rsquared)`, SD = `r sd(summaryOC$all$Rsquared)`). The resulting gamma values per participant are shown per group below (gray lines denote the rate of earnings under optimal behavior for all 3 timing contexts), and reflect what was seen in the previous sections: participants in the wait and physical conditions showed higher gamma values (and thus opportunity costs) than in the other two groups. Given the high variability of the easy condition, it might be worth modeling each timing context independently, or perhaps fitting the model on subgroups defined by a median split of the participants based on their acceptance rate. 

\bigskip

### 16.3.4.

*We will cross-validate each subject’s OC value using the pre-midpoint data for estimation, and post-midpoint choices for testing. The estimates will be used to predict acceptances in the testing sample, and the mean percent correctly predicted will be reported for each group. This will also provide information on the stability of each participant’s choices (4.3.1.).*
  
As can be seen below, the gamma parameter from the OC model was able to predict post-midpoint choices successfully regardless of group. 

``` {r 16.3.4. OC cross-validation, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# Compute the OC for pre-midpoint
summaryOC$pre <- dataBtw %>%
                 filter(Half == "Half_1") %>%
                 group_by(Cost, SubjID) %>%
                 do(optimizeOCModel(., simplify = T)) %>%
                 ungroup() %>%
                 select(SubjID, Gamma)

# let's add the gammas to the second half data and estimate prop predicted on the second half
summaryOC$predicted <- dataBtw %>%
                 filter(Half == "Half_2") %>%
                 group_by(Cost, SubjID) %>%
                 left_join(summaryOC$pre) %>%
                 mutate(OC = Handling * Gamma,
                        predicted = Offer > OC) %>% 
                 summarise(percentPredicted = mean(predicted == Choice) * 100)

# plot
(summaryOC$predictPlot <- ggplot(summaryOC$predicted, aes(Cost, percentPredicted, fill = Cost)) +
                            labs(x = "") +
                            geom_boxplot(show.legend = F) +
                            scale_fill_manual(values = colsBtw) +
                            labs(y = "Percent Predicted") +
                            ylim(0,100) +
                            theme(panel.grid.major = element_blank(),
                            panel.grid.minor = element_blank(),
                            panel.background = element_blank(),
                            axis.line = element_line(colour = "black"),
                            text = element_text(size = 16)))

```

\bigskip

### 16.3.5.

*The OC estimates for each group will be compared using an ANOVA with condition as a factor. This will let us determine which cost type produced the highest discounting.*

``` {r 16.3.5. OC comparisons, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# ANOVA
OCanova <- list()
OCanova$aov <- summary(aov(Gamma ~ Cost, data = summaryOC$all))
Fval <- OCanova$aov[[1]]$`F value`[1]
Pval <- OCanova$aov[[1]]$`Pr(>F)`[1]
Rsquared_aov <- round(OCanova$aov[[1]]$`Sum Sq`[1] / sum(OCanova$aov[[1]]$`Sum Sq`), digits = 2)
  
# Post-hoc pairwise tests
OCanova$perms <- summaryOC$all %>%
  select(Cost, Gamma) %>%
  plyr::dlply("Cost", identity) %>%
  do.call(cbind, .) %>%
  select_if(is.numeric) %>%
  rename(Physical = Physical.Gamma,
         Cognitive = Cognitive.Gamma,
         Wait = Wait.Gamma,
         Easy = Easy.Gamma) %>%
  outer(., ., FUN = Vectorize(permute), simple = T)
diag(OCanova$perms) <- 1

OCanova$ES <- summaryOC$all %>%
  select(Cost, Gamma) %>%
  plyr::dlply("Cost", identity) %>%
  do.call(cbind, .) %>%
  select_if(is.numeric) %>%
  rename(Physical = Physical.Gamma,
         Cognitive = Cognitive.Gamma,
         Wait = Wait.Gamma,
         Easy = Easy.Gamma) %>%
  outer(., ., FUN = Vectorize(cohenD)) %>%
  abs()
```

The analysis of variance was significant (F = `r round(Fval, digits = 2)`, p = `r ifelse(Pval < 0.001, "< 0.001", paste("=", round(Pval, digits = 2)))`, R-squared = `r Rsquared_aov`). Pairwise post-hoc permutations are shown in the following matrix. These results suggest that participants engaged in cognitively effortful demands experience significantly lower opportunity costs than those whose demands involve physical effort and pure delay. The reason for this is unclear, and we will work on it in the near future. 

``` {r 16.3.5. corrplot, echo = FALSE, fig.align="center", fig.width=5, fig.height=5}
# plot all pairwise comparisons
corrplot(OCanova$ES,
         is.corr = F, 
         p.mat = OCanova$perms,
         type = "upper",
         insig = "p-value",
         sig.level = -1,
         na.label = "square",
         na.label.col = "grey",
         method = "color", 
         tl.col = "black", 
         tl.cex = 0.8,
         outline = T)
```



\bigskip





\newpage

## Discussion