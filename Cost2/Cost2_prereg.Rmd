---
title: "Preregistration analysis, Cost 2 study"
author: "Claudio Toro Serey & Joseph T. McGuire"
output:
  html_document: default
  pdf_document: default
---

This document follows the steps planned for the first pre-registration (https://osf.io/2rsgm/). The bullet points match the organization shown on the website as closely as possible. First, here is a restatement of the questions and hypotheses.

```{r Global Options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

``` {r Libraries & Functions, echo=FALSE}
# libraries
library(tidyverse)
library(ggfortify)
library(knitr)
library(pander) 
library(nloptr)
library(pwr)
library(lme4)
#library(lmerTest)
library(gridExtra)
library(reshape2)
library(corrplot)

## Miscelaneous
lbls <- c("Wait","Cognitive","Physical","Easy")
cols = c("#78AB05","#D9541A","deepskyblue4", "darkgoldenrod2") # plot colors (wait, effort)
lthick = 2 # line thickness for plots

## Functions
# Permutation for 2 groups
permute <- function(group1 = 1, group2 = 2, statType = mean, nPerms = 5000, paired = FALSE, simple = FALSE){
  
    # prep data
    summaryPerm <- list()
    lOne <- length(group1)
    lTwo <- length(group2)
    bigSample <- c(group1,group2)  
    
    if (paired == FALSE) {
      
        
        for (i in 1:nPerms){
          
            # relabel samples
            tempBig <- sample(bigSample)
            tempOne <- tempBig[1:lOne]
            tempTwo <- tempBig[(lOne+1):length(bigSample)]
            
            # stats
            tempDiffs <- statType(tempOne,na.rm=T) - statType(tempTwo,na.rm=T)
            summaryPerm$jointDist[i] <- tempDiffs # statType(tempDiffs, na.rm = T) 
          
        }  
        
    } else {
      
        for (i in 1:nPerms){
          
            # shift labels in a pairwise fashion
            tempDiffs <- statType((-1)^rbinom(lOne,1,0.5) * (group1 - group2))
            summaryPerm$jointDist[i] <- tempDiffs
          
        }
      
    }
  
    # get the observed difference
    diffs <- statType(group1,na.rm=T) - statType(group2,na.rm=T)
    observedAbs <- abs(diffs) # maybe leave it as means here
    observed <- diffs
    summaryPerm$Pval <- 2 * (1 - ecdf(summaryPerm$jointDist)(observedAbs))
    if (length(unique(abs(summaryPerm$jointDist))) == 1) {summaryPerm$Pval <- 1} # if the difference is always the same, then p = 1
    summaryPerm$Observed <- observed
    
    # return the results
    if (simple) {
      
      return(summaryPerm$Pval)
      
    } else if (!simple) {
      
      return(summaryPerm)
      
    }
    
}

# Non-parametric Bootstrap for a single group
bootstrap <- function(group = 1, statType = mean, B = 5000){
  
    # prep param
    bootStats <- rep(0,B)
    
    # iterate
    for(b in 1:B){
      
      # wait group
      x <- sample(group,length(group),replace=T)  
      bootStats[b] <- statType(x,na.rm = T)
      
    }
    
    return(bootStats)
    
}

# plot logistic (for the prop complete test, mainly, but it can be easily expanded for more flexibility in n-params)
plotLogis <- function(coeffs = 1, group = "Cognitive", handling = 1:14, reward = 1:20, xAxis = "Reward", plotType = lines, color = "black", lType = 1, pchType = 16){
  
    
    # divide into relevant coefficients (assumes logisComb was the best model)
    intercept <- coeffs[1]
    cogBeta <- coeffs[2]    
    physBeta <- coeffs[3]
    easyBeta <- coeffs[4]
    handBeta <- coeffs[5]
    rwdBeta <- coeffs[6]
    
    #wait <- numeric()
    #phys <- numeric()
    
    # now the parameters
    label = "Reward Amount"
    if (group == "Cognitive"){cog = 1} else {cog = 0}
    if (group == "Phys"){phys = 1} else {phys = 0}
    if (group == "Easy") {easy = 1} else {easy = 0}
    if (xAxis == "Reward"){
        xAxis = reward 
        label = "Reward Amount" 
    } else {
        xAxis = handling
        label = "Handling Time"
    }
    
    # logistic function
    model <- intercept + cogBeta*cog + physBeta*phys + easyBeta*easy + handBeta*handling + rwdBeta*reward 
    func <- 1 / (1 + exp(-(model)))
    plotType(xAxis,
         func,
         ylim = c(0,1),
         xlim = c(0,25),
         xlab = label,
         ylab = "Probability of Completing a Trial",
         type="l",
         col = color,
         lty = lType,
         pch = pchType,
         lwd = 2,
         xaxt = "n",
         bty = "n")
  
    # use title() to add title afterwards
}

# Cohen's D for 2 groups
# for a more flexible approach, make the data input to be a list with entries for n groups, 
# then do length(list) for the number of groups. 
cohenD <- function(group1 = 1, group2 = 2){
  
      # means
      mean1 <- mean(group1, na.rm = T)
      mean2 <- mean(group2, na.rm = T)
      
      # variance
      var1 <- var(group1, na.rm = T)
      var2 <- var(group2, na.rm = T)
      
      # equation
      out <- (mean1 - mean2) / sqrt((var1 + var2)/2)
      
      return(out)
  
}

# get the number of continuous quits before current trial
seqQuits <- function(subjData = 1){
  
    # get the lag
    tempSeq <- sequence(rle(subjData)$lengths) # sequence
    tempSeq[subjData==1] <- 0 # not interested in completed runs, just quits
    tempSeq <- c(0,tempSeq) # lag it so it can be used as a t-1 regressor

    # return data so that the length is equivalent
    out <- tempSeq[1:length(subjData)]
    
    return(out)
    
}

# basic power calculation for a one way ANOVA
powerCalc <- function(d = 1.2, Za = 1.96, Zb = 0.842){
  
  out <- 2*((Za + Zb)/d)^2 + (0.25*(Za^2))
  
  return(round(out))
  
}

## gather basic data (this is better as a function, then rbind)
summarizeData <- function(Data = waitData, nSubs = nSubsW, type = "Wait"){
  
  tempSummary <- data.frame(group = rep(type, nSubs))
  
  if (type == "Cognitive") {
    
    for (subj in seq(nSubs)){
      
        # Choice index
        tempChoice <- Data[[subj]]$Choice
        
        # Proportion complete
        tempSummary[subj,2] <- mean(tempChoice)
      
        # earnings
        tempSummary[subj,3] <- sum(Data[[subj]]$Reward[tempChoice==1])
        
        # RTs
        tempSummary[subj,4] <- median(Data[[subj]]$`Choice RT`[tempChoice==0])
    
        # Mistakes (only for cognitive effort group)
        tempMistakes <- sum(cogDataLong[[subj]]$`Trial Outcome`=="Forced travel") # how many forced travel trials
        tempTrialnum <- length(Data[[subj]]$Choice) # how many decision trials
        tempSummary[subj,5] <- tempMistakes / (tempMistakes + tempTrialnum) # proportion of forced travels throughout the exp        
    
    }    
    
  } else {
    
    for (subj in seq(nSubs)){
        
        # choice index
        tempChoice <- Data[[subj]]$Choice
      
        # propotion complete
        tempSummary[subj,2] <- mean(tempChoice)
        
        # earnings
        tempSummary[subj,3] <- sum(Data[[subj]]$Reward[tempChoice==1])
        
        # RTs
        tempSummary[subj,4] <- median(Data[[subj]]$RT[tempChoice==0])
        
        # Mistakes (to allow for mistakes on the cognitive group)
        tempSummary[subj,5] <- NA
        
    }
  
  }
  
  return(tempSummary)
  
}

# OC-specific computation of the negative log likelihood for model optimization
negLogLik <- function(params, choice, handling, reward) {
  gamma <- exp(params[2]) * handling
  model <- exp(params[1]) * (reward - gamma)
  p = 1 / (1 + exp(-model))
  p[p == 1] <- 0.999
  p[p == 0] <- 0.001
  tempChoice <- rep(NA, length(choice))
  tempChoice[choice==1] <- log(p[choice==1])
  tempChoice[choice==0] <- log(1 - p[choice==0])
  negLL <- -sum(tempChoice)
  return(negLL)
}


# optimize the OC model
optimizeOCModel <- function(Data, Algorithm = "NLOPT_LN_NEWUOA", simplify = F, optfun = negLogLik) {
  
  # Data: The participant's log
  # Algorithm: probably let be
  # optfun: an external function to minimize (in this case OC, separately defined as negloglik)
  
  # Prep data
  handling <- Data$Handling
  ifelse(is.null(try(dataAll$Offer)), reward <- dataAll$Reward, reward <- dataAll$Offer) #reward <- Data$Offer (FIX THIS BY CALLING ALL COLUMNS REWARD, NOT OFFER)
  choice <- Data$Choice
  
  # Prep list of results to be returned
  out <- list()
  out$percentQuit <- mean(choice == 0) * 100
  out$percentAccept <- mean(choice== 1) * 100 
  miss <- (choice != 1) & (choice != 0)
  out$percentMiss <- mean(miss)  * 100
  choice <- choice[!miss]
  reward <- as.numeric(reward[!miss])
  handling <- as.numeric(handling[!miss])
  
  # Establish lower and upper bounds
  LB <- round(log((min(reward)/max(handling)) * 0.99), digits = 4)
  UB <- round(log((max(reward)/min(handling)) * 1.01), digits = 4) # in reality this should be the second largest, since no one would reject the highest val
  
  # Begin defining parameters
  # If choices are one-sided (i.e. all accepted)
  if ((sum(choice) == length(choice)) | (sum(choice) == 0)) {
    ifelse(sum(choice) == length(choice), out$Gamma <- exp(LB), out$Gamma <- exp(UB)) 
    out$Scale <- 1 # it was NA, but in theory a temperature of 1 also indicates noiseless estimates, and allows for easier fit computations
    out$LL <- 0
  } else {
    # Create a feasible region (search space)
    params <- as.matrix(expand.grid(scale = c(-1, 1), gamma = seq(LB, UB, length = 3)))
    # Create a list to check the minimization of the negative log lik.
    info <- list()
    info$negLL <- Inf
    # Define the options to be used during optimization
    # Consider looking into other optimization algorithms and global minima
    opts <- list("algorithm" = Algorithm,
               "xtol_rel" = 1.0e-8)
    # Optimize the sOC over all possible combinations of starting points
    for (i in seq(nrow(params))) {
      tempInfo <- nloptr(x0 = params[i, ], 
                   eval_f = optfun, 
                   lb = c(log(0.001), LB),
                   ub = c(-log(0.001), UB),
                   opts = opts,
                   choice = choice, 
                   handling = handling,
                   reward = reward)
      if (tempInfo$objective < info$negLL) {
        #print("Minimized")
        info$negLL <- tempInfo$objective
        info$params <- tempInfo$solution
      }
    }
    out$Gamma <- exp(info$params[2])
    out$Scale <- exp(info$params[1])
    out$LL <- -info$negLL
  }
  
  # Summarize the outputs
  out$LL0 <- log(0.5) * length(choice)
  out$Rsquared <- 1 - (out$LL/out$LL0)
  out$subjOC <- out$Gamma * handling
  out$p <- 1 / (1 + exp(-(out$Scale*(reward - out$subjOC))))
  out$predicted <- reward > out$subjOC
  out$predicted[out$predicted == TRUE] <- 1
  out$percentPredicted <- mean(out$predicted == choice) 
  
  # adjust the probabilities in case of extreme gammas
  if (out$Gamma <= exp(LB)) {
    out$Gamma <- exp(LB) # temporary condition because Nlopt is not respecting the lower bound
    out$p <- rep(1, length(choice))
  } else if (out$Gamma == exp(UB)) {
    out$p <- rep(0, length(choice))
  } 
  
  # if doing this with dplyr::do(), return a simplified data.frame instead with the important parameters
  if (simplify) {
    out <- data.frame(out[c(seq(8), 12)])
  }   
  
  return(out)

}


# summary matrices for refrence-changing models
betaMatrix <- function(model, rearrange = NA) {
# get a similarity matrix of the resulting coefficient pairings for the cost conditions
# first, do a full_join based on column names on the list of coefficient vectors from each dummy code relevel
# then match the names of columns and rows so NAs are in the diagonal
  
  # get the names of the reference group per model iteration
  refnames <- names(model)
  
  # coefficient matrix
  temp <- lapply(model, function(data) {coefficients(data)$SubjID[1, 2:4]})
  mixCoeffs <- bind_rows(temp) 
  preln <- ifelse("Cost" %in% substr(names(mixCoeffs), 1, 4), 4, 5) # count how many characters precede the name of each cost (diff across studies)
  dimnames(mixCoeffs) <- list(refnames, substr(names(mixCoeffs), preln + 1, 20))
  mixCoeffs <- as.matrix(mixCoeffs[, match(rownames(mixCoeffs), colnames(mixCoeffs))])
  mixCoeffs[is.na(mixCoeffs)] <- 0
  
  # now the pvals
  temp <- lapply(model, function(data) {as.list(summary(data)$coefficients[2:4, 4])})
  mixPvals <- as.matrix(bind_rows(temp)) 
  dimnames(mixPvals) <- list(refnames, substr(colnames(mixPvals), preln + 1, 20))
  mixPvals <- as.matrix(mixPvals[, match(rownames(mixPvals), colnames(mixPvals))])
  mixPvals[is.na(mixPvals)] <- 1

  # if you would like to re-arrange the coefficient order, supply a vector with the desired sequence
  if (length(rearrange) > 1) {
    mixCoeffs <- mixCoeffs[rearrange, rearrange]
    dimnames(mixCoeffs) <- list(rearrange, rearrange)
    mixPvals <- mixPvals[rearrange, rearrange]
    dimnames(mixPvals) <- list(rearrange, rearrange)
  }
  
  # combine matrices into list to return
  out <- list(Betas = round(mixCoeffs, digits = 2),
              Pvals = round(mixPvals, digits = 5))
  
  return(out)
  
}

```

``` {r Load Data, echo=FALSE}
### data loading and cleaning up
# recent addition: turning forced travels to acceptances, as they reflect a preference for that trial
# rawChoice will be used to compute the # of mistakes
# NOTE: P389 IS DUPLICATED. FIND OUT REAL ID # FOR ONE OF THEM.
setwd("./data")
files <- dir(pattern = '_log.csv')

dataAll <- tibble(SubjID = files) %>%
  mutate(contents = map(SubjID, ~ read_csv(., col_types = cols())))  %>%
  mutate(Cost = substring(SubjID, 5, 8),
         Cost = case_when(Cost == "wait" ~ "Wait",
                          Cost == "cogT" ~ "Cognitive",
                          Cost == "phys" ~ "Physical",
                          Cost == "phea" ~ "Easy"),
         SubjID = as.integer(substring(SubjID, 0, 3))) %>%
  unnest() %>%
  mutate(rawChoice = Choice,
         Choice = ifelse(Choice == 2, 1, Choice),
         Half = ifelse(Block < 4, "Half_1", "Half_2"),
         Cost = factor(Cost, levels = c("Physical", "Cognitive", "Wait", "Easy")),
         optimal = case_when(
          (Handling == 10 & Reward < 8) ~ 0,
          (Handling == 14 & Reward < 20) ~ 0,
          TRUE ~ 1
         )) 

# get a simple subject list and the number of subjects
subjList <- unique(dataAll$SubjID)
nSubjs <- length(subjList)

```

## 3. Research Questions

### 3.1.	

Do decision makers in foraging environments integrate information about delay durations and reward amounts to produce reward-maximizing behavior?

### 3.2.	

Main question: Are decisions affected differently by equivalent time periods of pure delay, cognitive effort, physical effort, and non-effortful task engagement? Do these four conditions involve different levels of subjective costs? 

### 3.3.	

How can we best computationally model the perceived cost of time and effort to predict choices in each condition?

## 4. Hypotheses

### 4.1.	

Single-option, accept/reject decisions will be influenced by within-subject manipulations of reward magnitude and associated delay duration in line with a theoretical reward-maximizing strategy.

 **4.1.1.**	Participants will more frequently accept high-reward prospects than low-reward prospects.
 
 **4.1.2.**	The tendency to reject low-reward prospects will be greater in environments where rewards are associated with longer delays (handling times).

### 4.2.	

Prospect acceptance rates will differ across four between-subject conditions, in which the delays associated with rewards (a) are unfilled, (b) include a cognitive effort requirement, (c) include a physical effort requirement, or (d) require a trivial level of physical effort but have matched visual stimuli to the physical effort condition.

  **4.2.1.**	In the cognitive effort condition, overall acceptance rates will be greater than in the unfilled-delay condition.
  
  **4.2.2.**	In the physical effort condition, overall acceptance rates will be greater than in the unfilled-delay condition, and equivalent to the cognitive effort condition.
  
  **4.2.3.**	In the trivial effort condition, acceptance rates will be equivalent to the unfilled-delay condition, and lower than in the physical effort and cognitive effort conditions.

### 4.3.	

Choices will be well fit by a computational model in which the subjective opportunity cost of time is free to vary across the four between-subject conditions.

  **4.3.1.**	Participants will display stable preferences, meaning that the reward amounts they accept in a given timing condition will be similar throughout the experiment.
  
  **4.3.2.**	Subject-specific opportunity cost (OC) estimates will vary inversely with acceptance rates. Thus, the unfilled-delay condition will produce higher OC estimates than both effort conditions, which in turn will show no differences between them.  

\bigskip

## 16. Analyses
### 16.1.	Tests of whether decision makers integrate delay and reward information. 

### 16.1.1.

  *To address hypothesis 4.1., A logistic regression will be fit for each participant in order to predict trial-wise acceptances, using handling time and reward amount as predictors. The resulting beta coefficients for handling time and reward will be pooled across all participants, and we will perform a one-sample rank-sum test on each set of coefficients to examine whether the they are significantly positive or negative (compared to 0). If the group coefficients are significantly positive, it would mean that a predictor reliably increases the likelihood of acceptance. This will allow us to determine whether increments in handling time and reward amounts increased and decreased the likelihood of acceptance for each participant, respectively.* 
  
``` {r 16.1.1., fig.align="center", fig.width = 4, fig.height = 4, echo = FALSE}
# filterout errors and compute R + H logistic per subject, then get coefficients
logisRH <- dataAll %>%
  filter(Choice < 2) %>%
  plyr::dlply("SubjID", identity) %>%
  lapply(function(data) {glm(Choice ~ Handling + Reward, data = data, family = "binomial")}) %>%
  sapply(coefficients)

# Rank sum tests
rankHand <- wilcox.test(logisRH["Handling", ])
rankReward <- wilcox.test(logisRH["Reward", ])

# Plot coeffs
temp <- melt(logisRH[c("Handling", "Reward"), ])
qplot(data = temp, x = Var1, y = value, geom = "boxplot") + 
  labs(x = "", y = "Coefficients") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme(legend.position = c(0.9, 0.7),
       panel.grid.major = element_blank(), 
       panel.grid.minor = element_blank(), 
       panel.background = element_blank(), 
       axis.line = element_line(colour = "black"),
       text = element_text(size = 16))
```

The plot shows the pooled coefficients across subjects for the effect of each predictor. The results show the expected discounting effect of increasing the handling time, as well as the increase in acceptance likelihood as a function of reward increments. We found that these coefficients were significantly different from zero for both handling (V = `r rankHand$statistic`, p `r ifelse(rankHand$p.value < 0.001, "< 0.001", paste("=", round(rankHand$p.value, digits = 2)))`) and reward (V = `r rankReward$statistic`, p `r ifelse(rankReward$p.value < 0.001, "< 0.001", paste("=", round(rankHand$p.value, digits = 2)))`) regardless of cost condition.

\bigskip

### 16.1.2.	

*We will perform an extension of the logistic regression from 16.1.1., this time adding an autoregressive covariate containing the number of consecutive quits prior to a given trial. In this way, we will examine the possibility that participant choices were governed by recent quitting history rather than the experimental parameters (see 11.1.3.). Coefficients not significantly different from 0 will denote that a participant did not rely on recent quitting history.*
  
``` {r 16.1.2., echo = FALSE, include = FALSE}
# add an AR regressor to track recent quits
logisRHAR <- dataAll %>%
              filter(Choice < 2) %>%
              group_by(SubjID) %>%
              mutate(AR = seqQuits(Choice)) %>%
              plyr::dlply("SubjID", identity) %>%
              lapply(function(data) {glm(Choice ~ Handling + Reward + AR, data = data, family = "binomial")}) 
  
# Rank sums
rankAR <- logisRHAR %>% 
  sapply(coefficients) %>%
  wilcox.test(.["AR", ])

AR_CIs <- sapply(logisRHAR, function(x) {tryCatch(confint(x)[4, ], error = function(e){c(0, 0)})}) %>%
  replace_na(0) %>%
  t() %>% 
  apply(1, function(x) !(x[1] <= 0 & x[2] >= 0))

```

To measure the significance of each subject's autoregressive coefficient, we computed its CI and checked how many contained zero. By this measure, `r sum(AR_CIs)` out of `r nSubjs` of our participants seemed to have been influenced by recent quitting. However, the autoregressive predictor did not preclude the effect of the remaininig experimental parameters.

\bigskip

### 16.1.3.	

*A general linear model with constant, linear, and quadratic terms will be used to estimate the correspondence between proportion accepted (independent variable) and total earnings (dependent variable). No other covariates will be used, as this analysis is to confirm that over and under accepting are detrimental to total earnings. The quadratic term will be defined as the squared deviation from the optimal overall acceptance rate.*
  
``` {r 16.1.3. Earnings, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4}
# calculate the proportions and earnings per subject, while keeping group info
temp <- dataAll %>%
  filter(rawChoice < 2) %>%
  group_by(SubjID, Cost) %>%
  summarise(Earnings = sum(Reward[Choice == 1] / 100),
            pComplete = mean(Choice)) %>%
  mutate(pComplete2 = pComplete^2) %>%
  ungroup()

# linear fit of earnings x prop accept
lmEarn <- lm(Earnings ~ pComplete + pComplete2, data = temp)

# plot
(earnFitplot <- ggplot(data = temp, aes(pComplete, Earnings)) + 
         scale_color_manual(values = cols) + 
         geom_point(aes(fill = Cost), pch = 21, color = "black", size = 3, show.legend = T) + 
         stat_smooth(aes(y = Earnings), method = "lm", formula = y ~ x + I(x^2), color = "grey30") +
         labs(x = "Proportion Completed", y = "Total Earnings (dollars)") +
         scale_fill_manual(values = cols) +
         theme(panel.grid.major = element_blank(), 
           panel.grid.minor = element_blank(), 
           panel.background = element_blank(), 
           axis.line = element_line(colour = "black"),
           text = element_text(size = 16)))

```

The figure shows that participants that over and unceraccepted earned less money overall, as predicted. This is supported by a significant quadratic term from the linear model (F = `r round(summary(lmEarn)$fstatistic[1], digits = 2)`, Beta = `r summary(lmEarn)$coefficients[3,1]`, SE = `r round(summary(lmEarn)$coefficients[3,2], digits = 2)`, R-squared = `r round(summary(lmEarn)$r.squared, digits = 2)`; black line shows the fit). The following table shows all the results from the model.

``` {r 16.1.3. Earnings table, echo=FALSE,fig.align="center",fig.width=9,fig.height=5}
# Summary table for the linear model
panderOptions("digits", 2)
pander(lmEarn, style = "rmarkdown")
```  
  
\bigskip
  
### 16.1.4.	

*To determine the optimality of each groupâ€™s decisions, we will perform two-sided one-sample t-tests (with mu being the optimal proporion of acceptances--either 0 or 1) to see if the proportion of acceptances for each time/reward combination was significantly different from the optimal rate (see 11.2.). This will result in 36 independent tests (3 rewards amounts, 3 handling/travel time combinations, and 4 groups), so we will correct for multiple comparisons using False Discovery Rate (FDR).*

``` {r 16.1.4., echo=FALSE}
# in order
# clean data, 
# calculate each individual's prop. accept,
# calculate t.tests with optimality as null mean,
# correct pvalues with FDR
temp <- dataAll %>%
  filter(Choice < 2) %>%
  group_by(SubjID, Handling, Reward, Cost) %>%
  summarise(pAccept = mean(Choice)) %>%
  mutate(optimal = case_when(
          (Handling == 10 & Reward < 8) ~ 0,
          (Handling == 14 & Reward < 20) ~ 0,
          TRUE ~ 1
        )) %>% 
  group_by(Handling, Reward, Cost) %>%
  summarise(meanAccept = mean(pAccept),
            sdAccept = sd(pAccept),
            optimal = unique(optimal),
            pvals = tryCatch(t.test(pAccept, mu = unique(optimal))$p.value, error = function(e) {1})) %>%
  mutate(FDR = p.adjust(pvals, method = "BY"),
         significant = FDR < 0.05)


# proportion of significant ones
# eventually find a way to plot this
splitData_prop <- mean(temp$significant, na.rm = T)
```

Of the 36 tests, around `r splitData_prop` were significantly deviant from optimality. Most of these were from effortful groups, as the wait group just showed deviations for 2 seconds handling/4 cent, and 10 seconds handling/8 cent offers (note to self: think about a way to properly visualize these).

\bigskip

### 16.2.	Comparisons among the four delay and effort conditions.

### 16.2.1.	

*To compare preferences (hypothesis 4.2.), we will first perform a one-way ANOVA on the proportion of trials accepted using group as a factor. In addition, we will do pairwise comparisons on the proportion completed among all 4 groups using non-parametric permutation contrasts (6 tests). The same approach will be used for total earnings. This will give us an initial glimpse on the potential differences in cost among conditions.*

``` {r 16.2.1. Overall Proportion/Earnings, fig.align = "center", fig.width = 8, fig.height = 4, echo = FALSE}  
## ECDF of proportion completed per cost type
temp <- dataAll %>%
        filter(Choice < 2) %>%
        mutate(Cost = factor(Cost, levels = list("Physical", "Cognitive", "Wait", "Easy"))) %>%
        group_by(SubjID, Cost) %>%
        summarise(Earnings = sum(Reward[rawChoice == 1] / 100),
                  pAccept = mean(Choice))

## formal testing 
# proportion completed
propAov <- aov(pAccept ~ factor(Cost), data = temp)

propTests <- temp %>% 
  plyr::dlply("Cost", identity) %>%
  sapply("[[", "pAccept") %>%
  as.data.frame() %>%
  combn(2, simplify = F) %>%
  sapply(function(x) {
    c(contrast = paste(colnames(x), collapse = "vs"), 
      permPval = round(permute(x[, 1], x[, 2], simple = T), digits = 3),
      cohenD = round(cohenD(x[, 1], x[, 2]), digits = 3))
  }) %>% 
  t()

# pairwise.wilcox.test(temp$pComplete, temp$Cost)

# earnings
earnAov <- aov(Earnings ~ factor(Cost), data = temp)

earnTests <- temp %>% 
  plyr::dlply("Cost", identity) %>%
  sapply("[[", "Earnings") %>%
  as.data.frame() %>%
  combn(2, simplify = F) %>%
  sapply(function(x) {
    c(contrast = paste(colnames(x), collapse = "vs"), 
      permPval = round(permute(x[, 1], x[, 2], simple = T), digits = 3),
      cohenD = round(cohenD(x[, 1], x[, 2]), digits = 3))
  }) %>% 
  t()

## plots
# ECDF pAcceptd
p1 <- ggplot(aes(pAccept, color = Cost), data = temp) +
        stat_ecdf(lwd = 1.2, show.legend = F) +
        scale_color_manual(values = cols) +
        xlim(0, 1) +
        labs(y="ECDF", x = "Proportion Accepted") +
        theme(panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line = element_line(colour = "black"),
              text = element_text(size = 16))

# pAccepted
p2 <- ggplot(temp, aes(Cost, pAccept, fill = Cost)) + 
        geom_boxplot(show.legend = F) +
        scale_fill_manual(values = cols) +
        ylim(c(0, 1)) +
        labs(y = "Proportion Accepted") +
        theme(panel.grid.major = element_blank(), 
            panel.grid.minor = element_blank(), 
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black"),
            text = element_text(size = 16))

# earnings
p3 <- ggplot(temp, aes(Cost, Earnings, fill = Cost)) + 
        geom_boxplot(show.legend = F) +
        scale_fill_manual(values = cols) +
        labs(y = "Total Earnings (dollars)") +
        theme(panel.grid.major = element_blank(), 
            panel.grid.minor = element_blank(), 
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black"),
            text = element_text(size = 16))

grid.arrange(p2, p3, ncol = 2)
```

The plots show that subjects in the wait condition accepted the least and earned the most, suggesting a more optimal pattern of choices. Participants in the cognitive effort and easy conditions had higher more variable acceptance patterns, which are reflected in the comparatively low earnings. The reason why the easy condition does not show variable earnings like the cognitive effort group is probably due to the quadratic relationship between earnings and acceptances shown before.

\bigskip

### 16.2.2.

*In order to further look at the effect of delay, work, and rewards, we will perform a repeated measures ANOVA on the proportion completed for each combination of factors. Reward and handling time will be within-subject factors, and condition a between-subjects factor. In support of hypotheses 4.1. and 4.2., we anticipate significant main effects of handling time, reward, and cost condition, but no interactions.*
  
``` {r 16.2.2. rmANOVA, echo=FALSE,fig.align="center",fig.width=5,fig.height=4, results = 'asis'}
temp <- dataAll %>%
        filter(Choice < 2) %>%
        group_by(SubjID, Cost, Handling, Reward) %>%
        summarise(pAccept = mean(Choice))

## REPEATED MEASURES ANOVA
# I think I like this the most
prop.aov <- with(temp, aov(pAccept ~ factor(Cost) * Handling * Reward + 
                                 Error(SubjID / (Handling * Reward))))

## NOTES ON RESULTS
# According to the logistic fit, there are main effects for all predictors, but no interactions. It also says that the overall odds for the wait group to accept versus the effort is around 16%. Similarly, increasing the handling time results in a decrease in the multiplicative odds of ~25%. Increments in reward increase the odds of acceptance by ~40%.
```

The plot below shows the mean proportion of acceptances (Â± SEM) per combination of handling time, reward, and condition (optimal acceptance rate indicated by the gray triangles). Visually, the graph confirms a couple of important intuitions. First, as the handling time increases, the proportion of acceptances decreases. Second, this discounting did not affect the 20 cent offers, as is expected from the present foraging environment. Lastly, and similar to the previous point, as the value of the offers increases so does the willingness to accept a trial. Beyond these general features, it is clear that the cognitive effort group accepted more than the other groups, regardless of the combination of experimental parameters. Notably, the optimality of this greater acceptance rate is determined by the timing context (e.g. optimal at 2 second handling time, but detrimental at 14 seconds).      

``` {r 16.2.2. rmANOVA plot, echo = FALSE, fig.align = "center", fig.width = 10, fig.height = 6}
# prep data to plot the p(accept) based on reward, handling time and cost type
trueProp <- dataAll %>%
        filter(Choice < 2) %>%
        group_by(SubjID, Cost, Handling, Reward, optimal) %>%
        summarise(pAccept = mean(Choice)) %>%
        group_by(Cost, Handling, Reward, optimal) %>%
        summarise(meanComplete = mean(pAccept),
                  SE = sd(pAccept) / sqrt(length(pAccept)))

# and plot
(a1 <- ggplot(data = trueProp, aes(interaction(Reward, Handling), meanComplete, color = Cost)) + 
        geom_point(size = 3) + 
        geom_errorbar(aes(ymin = meanComplete - SE, ymax = meanComplete + SE), width = 0.2, size = 1) +
        geom_line(aes(group = interaction(Handling, Cost)), size = 1) +
        geom_point(aes(y = round(optimal)), shape = 21, fill = "grey75", color = "grey30", size = 3, show.legend = F) +
        labs(x = "Reward.Handling", y = "Proportion Accepted") +
        scale_color_manual(values = cols) +
        theme(legend.key = element_blank(),
              panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line = element_line(colour = "black"),
              text = element_text(size = 16)))

```

These intuitions were formally tested using a repeated measures ANOVA, whose results are presented in the table below. Overall, the analysis partially confirmed our predictions. While all main effects were significant, there was an unexpected significant condition-by-reward interaction, which could be due to the performance of the "easy" group relative to their peers. Importantly, we found that the interaction between all three main parameters was not significant, thus suggesting that the effects of handling and reward on choices were not different across groups.

``` {r 16.2.2. rmANOVA table, pander}
# anova table
x <- summary(prop.aov)   
x <- x$`Error: Within`
panderOptions("digits", 2)
pander(x, style = "rmarkdown", split.table = 110)
```

\bigskip

### 16.2.3.	

*We will compute the probability of accepting a trial with a mixed-effects logistic regression. Based on the task structure and our main question, our a priori model of interest will include cost condition, handling time, and reward amount as fixed main effects, and subject ID as a random effect. Cost condition will be modeled with three categorical terms, with the fourth condition as the reference condition. We will run three versions of the model with different reference conditions, in order to test all pairwise differences among the four cost conditions. As with 16.2.2., we anticipate significant main effects (coefficients different than zero) of handling time, reward, and cost condition. We hypothesize that the differences among cost conditions will follow the pattern described in 4.2.*

``` {r 16.2.3. Mixed Effects Model, echo = FALSE, fig.align = "center", fig.width = 5, fig.height = 4}
# clean the data so the total acceptances and quits per subj x handling x reward are ready for modeling
mixLogis_main <- list()
mixData <- dataAll %>%
  filter(Choice < 2) %>%
  group_by(SubjID, Cost, Handling, Reward) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice)) %>%
  ungroup()


# model with a random intercept and cost-type slope, then relevel the cost to get all pairwise comparisons
# the correct way to model a logistic with proportions as dependent vars is by prividing n of hits and quits as a matrix
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait"))
mixLogis_main$Wait <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Cognitive"))
mixLogis_main$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Physical"))
mixLogis_main$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Easy"))
mixLogis_main$Easy <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)

# matrix of coefficients and pvalues
rearrange <- c("Cognitive", "Easy", "Wait", "Physical") # to kinda match the order in Cost 3
mixSummary <- betaMatrix(mixLogis_main, rearrange = rearrange)
diag(mixSummary$Pvals) <- NA

# plot coefficients and pvalues in a matrix
# invert colors
col2 <- colorRampPalette(c("#053061", "#2166AC", "#4393C3", "#92C5DE", "#D1E5F0", "#FFFFFF", "#FDDBC7", "#F4A582", "#D6604D", "#B2182B", "#67001F"))

corrplot(mixSummary$Betas, 
         is.corr = F, 
         p.mat = mixSummary$Pvals, 
         type = "lower",
         outline = T,
         #insig = "p-value",
         sig.level = 0.05, 
         na.label = "square", 
         na.label.col = "grey",
         method = "color", 
         tl.col = "black",
         tl.cex = 0.8,
         col = col2(200))

```


``` {r 16.2.3. Mixed Effects Model plot rand intercepts, echo = FALSE, fig.align = "center", fig.width = 5, fig.height = 4}
# plot random intercepts in increasing order
#RI1 <- qplot(seq(83), sort(ranef(mixLogis_main$Wait)$SubjID[,1]), color = cols[1], show.legend = F) + theme_classic()

# # plot per subject (as expected, the intercepts are highest for those who accepted everythiing)
# temp <- ranef(mixLogis_main$Wait)$SubjID %>%
#   gather(key = Cost, value = RandEffects, colnames(.)) 
# 
# (RI1 <- ggplot(data = temp, aes(Cost, RandEffects, fill = Cost)) + 
#   geom_hline(yintercept = 0, alpha = 0.7, linetype = "dashed") + 
#   geom_boxplot(show.legend = F) +
#   scale_fill_manual(values = cols) + 
#   labs(x = "") +
#   theme(panel.grid.major = element_blank(), 
#               panel.grid.minor = element_blank(), 
#               panel.background = element_blank(), 
#               axis.line = element_line(colour = "black"),
#               text = element_text(size = 16)))

```

As predicted, the model showed significant main effects of handling time (Beta = `r mixLogis_main$Summary$coefficients[5,1]`, SE = `r mixLogis_main$Summary$coefficients[5,2]`, p < 0.001) and reward (Beta = `r mixLogis_main$Summary$coefficients[6,1]`, SE = `r mixLogis_main$Summary$coefficients[6,2]`, p < 0.001). In order to show the comparisons among all conditions, the following plot portrays the coefficients (color scale), and p-values (numbers within squares) that resulted from switching the reference condition. Specifically, each entry shows how much more likely was the reference group to accept an offer than the comparison group. Based on this, we can see that the cognitive group was significantly more likely to accept any offer than the physical and wait groups, but not the easy group.


### 16.2.4.

*Next, we will examine whether the a priori model from 16.2.3. outperforms both simpler and more complex models. Unlike the individual logistic regression fits in 16.1.1., a mixed-effects approach gives us a better goodness of fit measure for model comparisons. We will determine the best model (combination of predictors) using Akaikeâ€™s Information Criterion (AIC) to determine the model that minimizes the negative log-likelihood while penalizing the addition of parameters. The regression with each combination of predictors will be fitted in the following order: 1) intercept only; 2) condition only; 3) handling time only; 4) reward only; 5) condition, handling, and reward main effects (from 16.2.3.); 6) adding a handling-by-reward interaction; and 7) adding all three possible two-way interactions. We predict that model 5 will have the lowest AIC.*


``` {r 16.2.4. Model comparison, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# Quasibinomial GLM versions (with proper proportion setup)
mixLogis_compare <- list()
mixLogis_compare$Base <- glmer(cbind(totalAccepted, totalQuits) ~ 1 + (1 | SubjID), family = "binomial", data = mixData) # intercept only
mixLogis_compare$Cost <- glmer(cbind(totalAccepted, totalQuits) ~ Cost + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare$Handling <- glmer(cbind(totalAccepted, totalQuits) ~ Handling + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare$Reward <- glmer(cbind(totalAccepted, totalQuits) ~ Reward + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare$AllMain <- mixLogis_main$Wait
mixLogis_compare$Interaction <- glmer(cbind(totalAccepted, totalQuits) ~ Cost * Reward + (Cost | SubjID), family = "binomial", data = mixData)

# Model selection 
abicLogis_compare <- data.frame(Model = names(mixLogis_compare),
                                AIC = sapply(mixLogis_compare, AIC),
                                BIC = sapply(mixLogis_compare, BIC))
  
# effect size
mixLogis_compare$Rsq <- 1 - (deviance(mixLogis_compare$AllMain)/deviance(mixLogis_compare$Base)) # denominator is the null deviance
```

``` {r 16.2.4. Plot Deviance comparison, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# plot AIC and BIC
(mixLogis_compare$plot <- abicLogis_compare %>% 
  gather(Comparison, Value, AIC:BIC) %>%
  mutate(Model = factor(Model, levels = names(mixLogis_compare))) %>%
  ggplot(aes(Model, Value, fill = Comparison)) +
    geom_point(color = "black", pch = 21, size = 3) +
    theme_classic())
    
```

This plot shows the AIC for every model. The figure below shows AIC and BIC for all the models. As predicted, the model with all main effects reduced the deviance the most ($R^2$ = `r round(mixLogis_compare$Rsq, digits = 2)`). The reduction in deviance from an intercept-only model was drastic enough that we did not compute an LRT.

\bigskip

### 16.3.	Modeling the subjective opportunity cost in each condition. 

### 16.3.1.

*Response times (RT) for quit responses will be presented in a descriptive manner in order to examine whether participants tended to quit early or late within individual trials. Each cost groupâ€™s response time distribution will contain the pooled RT across its corresponding participants, and we will display the empirical cumulative distribution functions for each condition. Short RT would suggest confident and stable decisions (in support of 4.3.1.).*
  
The ECDF below shows the proportion of quits and acceptances occurring as the trial advanced. Drastic changes indicate the end of one of the possible handling times, and sccessful receipt of a reward. This shows that there were no quits during the handling time, and displays the proportion of acceptances (1 - the y axis here) for each cost at each handling/travel time combination. We can see in the ECDFs below that most decisions to quit happened within the first second into the handling time. The reason why the physical and easy condition show slightly lagged responses is that the experiment allowed a one second grace period for participants to begin gripping. This results in some of them choosing to wait for that second to indicate an offer rejection.

``` {r 16.3.1. RTs, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# Proportion of forced travels per subject and cost type
propFails <- dataAll %>% 
  filter(Cost != "Wait") %>% 
  group_by(SubjID, Cost) %>% 
  summarise(propFails = mean(rawChoice == 2)) %>%
  group_by(Cost) %>% 
  summarise(meanFT = mean(propFails))

# Reaction times ECDFs including offer and in-trial quits
(RTs <- dataAll %>%
  #mutate(RT = ifelse(Choice != 0, RT + 2, RT)) %>%
  filter(rawChoice < 2) %>% # change this to 1 to reproduce the original plot, since it would only plot the cdf of quits
  ggplot(aes(RT, color = Cost)) + 
    #stat_ecdf(lwd = 1.2, geom = "smooth") +
    geom_line(aes(y = 1 - ..y..), stat = 'ecdf', lwd = 1.5) +
    geom_vline(xintercept = 1, lty = 2) + # end of the grace period for physical trials
    scale_color_manual(values = cols) +
    scale_x_continuous(breaks = seq(14)) +
    labs(x = "Time in Trial", y = "Proportion Completed") +
    theme(panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      panel.background = element_blank(),
      axis.line = element_line(colour = "black"),
      text = element_text(size = 16)))

```


\bigskip

### 16.3.2.

*In order to further examine choice stability (hypothesis 4.3.1.), we will compute each participantâ€™s total proportion of acceptances pre- and post-midpoint. For each cost condition separately, we will fit a linear model that predicts post-midpoint acceptance from before-midpoint rates. We will report the slopes and 95% confidence intervals (CI) for each cost group. CIs containing 1 will denote that participants in that group produced consistent choices.*
  
``` {r 16.3.2. Pre/Post Break, echo=FALSE,fig.align="center",fig.width=5,fig.height=5}
# divide the data into proportion accepted before/after the break (one per column)
temp <- dataAll %>%
  filter(Choice < 2) %>%
  group_by(SubjID, Cost, Half) %>%
  summarise(pAccepted = mean(Choice)) %>%
  ungroup() %>%
  spread(Half, pAccepted)

# separate by cost type and compute a linear model to predict half 2 from 1
sep <- temp %>% plyr::dlply("Cost", identity)
prepost <- list()
prepost$LM <- lapply(sep, function(data) {lm(data$Half_2 ~ data$Half_1)})

# get the coefficients and confidence intervals
# and concatenate
prepost$summary <- as.data.frame(cbind(t(sapply(prepost$LM, coefficients)), t(sapply(prepost$LM, confint))))
colnames(prepost$summary) <- c("Intercept", "Coefficient", "Int_CI-low", "Int_CI-high", "Beta_CI-low", "Beta_CI-high")

## PRE/POST PAIRED PERMUTATIONS
temp <- dataAll %>%
  filter(Choice < 2) %>%
  group_by(SubjID, Cost) %>%
  summarise(propAccept_1 = mean(Choice[Half == "Half_1"]),
            propAccept_2 = mean(Choice[Half == "Half_2"])) %>%
  plyr::dlply("Cost", identity) 


# paired perms and effect sizes
prepost$CohenD <- sapply(temp, function(data) {DescTools::CohenD(data$propAccept_1, data$propAccept_2)})
prepost$Perms <- sapply(temp, function(data) {permute(data$propAccept_1, data$propAccept_2, paired = T, simple = T)})
```

The plot below shows that acceptance rates before and after the mid-point were mostly consistent, although it was more likely for participants to accept less in the second half of the experiment. **NOTE: The lines of means in the scatterplot are calculated by first computing the mean acceptances per block, then averaging those for blocks pre/post break. That's how they match the second half of the right plot, but the linear model is computed with the overall proportion acceptances pre-post, which gives slightly different mean estimates (which is also the reason for slight mean differences in proportion accepted between these and the boxplot in 16.2.1.)**

``` {r 16.3.2. Plot Pre/Post, echo=FALSE, fig.align = "center", fig.width = 8, fig.height = 4}
# scatter plot of pre/post acceptances
# BIG CAVEAT: THE 
# first calculate the mean acceptances per cost/half to draw horizontal and vertical lines on the plot
df_mean <- dataAll %>%
            group_by(SubjID, Cost, Block, Half) %>%
            summarise(propAccept = mean(Choice)) %>%
            group_by(SubjID, Cost, Half) %>%
            summarise(propAccept = mean(propAccept)) %>%
            spread(Half, propAccept) %>%
            group_by(Cost) %>%
            summarise(propAccept_1 = mean(Half_1),
                      propAccept_2 = mean(Half_2))


# and plot
prepost$scatterplot <- dataAll %>%
                        filter(Choice < 2) %>%
                        group_by(SubjID, Cost) %>%
                        summarise(propAccept_1 = mean(Choice[Half == "Half_1"]),
                                  propAccept_2 = mean(Choice[Half == "Half_2"])) %>%
                        ggplot(aes(propAccept_1, propAccept_2, fill = Cost)) +
                          geom_point(size = 3, pch = 21, color = "black", show.legend = F) +
                          scale_fill_manual(values = cols) +
                          scale_color_manual(values = cols) +
                          xlim(0, 1) +
                          ylim(0, 1) +
                          labs(x = "Proportion Accepted - 1st Half", y = "Proportion Accepted - 2nd Half") +
                          geom_abline(slope = 1, intercept = 0, lty = 2) +
                          geom_hline(data = df_mean, aes(yintercept = propAccept_2, color = Cost), linetype="dashed", show.legend = F) +
                          geom_vline(data = df_mean, aes(xintercept = propAccept_1, color = Cost), linetype="dashed", show.legend = F) +
                          theme(panel.grid.major = element_blank(),
                                panel.grid.minor = element_blank(),
                                panel.background = element_blank(),
                                axis.line = element_line(colour = "black"),
                                text = element_text(size = 16))

# acceptances per cost across blocks
# note that because the same handling/travel time combos repeat in the same order pre/post, 1/4-2/4-3/6 are direct comparisons
prepost$propBlocks <- dataAll %>%
                         filter(Choice < 2) %>%
                         group_by(SubjID, Cost, Block) %>%
                         summarise(propAccept_subj = mean(Choice)) %>%
                         group_by(Cost, Block) %>%
                         summarize(propAccept = mean(propAccept_subj),
                                   seAccept = sd(propAccept_subj)/sqrt(nSubjs)) %>%
                         ggplot(aes(Block, propAccept, color = Cost)) +
                           geom_point(size = 2) +
                           geom_errorbar(aes(ymin = propAccept - seAccept, ymax = propAccept + seAccept, color = Cost), width = 0.1) +
                           geom_line() +
                           ylim(0, 1) +
                           labs(y = "Proportion Acccepted") +
                           scale_x_continuous(breaks = seq(6)) +
                           scale_color_manual(values = cols) +
                           scale_fill_manual(values = cols) +
                           theme(legend.key = element_blank(),
                                 legend.position = c(0.8, 0.25),
                                 panel.grid.major = element_blank(),
                                 panel.grid.minor = element_blank(),
                                 panel.background = element_blank(),
                                 axis.line = element_line(colour = "black"),
                                 text = element_text(size = 16))

grid.arrange(prepost$scatterplot, prepost$propBlocks, ncol = 2)
```

The following table shows the coefficients and CI for each linear model, and show that pre-post acceptance rates were very similar for each condition (i.e. one is present in all confidence intervals).


``` {r 16.3.2. prepost table, pander}
panderOptions("digits", 2)
pander(prepost$summary, style = "rmarkdown", split.table = 110)
```

Now let's divide them by reward and handling amounts. The plots below show that the overall rates of acceptances remain mostly consistent across first and second halves of the experimental session.

``` {r 16.3.2. Plot Pre/Post reward x handling, echo=FALSE, fig.align = "center", fig.width = 8, fig.height = 4}
# # pre
# t1 <- dataAll %>%
#   filter(Choice < 2, Half == "Half_1") %>%
#   group_by(SubjID) %>%
#   filter(Handling == 10) %>%
#   group_by(SubjID, Cost, Reward) %>%
#   summarise(pAccept = mean(Choice)) %>%
#   group_by(Cost, Reward) %>%
#   summarise(propAccept = mean(pAccept),
#             SE = sd(pAccept) / sqrt(nSubjs)) %>%
#   ungroup() %>%
#   mutate(Reward = ifelse(Reward == 20, 12, Reward)) %>%
#   ggplot(aes(Reward, propAccept, color = Cost)) +
#     geom_point(size = 3, show.legend = F) +
#     geom_line(show.legend = F) +
#     scale_color_manual(values = cols) +
#     scale_fill_manual(values = cols) +
#     geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.3, size = 1, show.legend = T) +
#     scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1)) +
#     scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
#     labs(y = "Proportion Accepted", title = "First Half") +
#     theme(legend.key = element_blank(),
#           legend.position = c(0.8, 0.25),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))
# 
# # post half
# t2 <- dataAll %>%
#   filter(Choice < 2, Half == "Half_2") %>%
#   group_by(SubjID) %>%
#   filter(Handling == 10) %>%
#   group_by(SubjID, Cost, Reward) %>%
#   summarise(pAccept = mean(Choice)) %>%
#   group_by(Cost, Reward) %>%
#   summarise(propAccept = mean(pAccept),
#             SE = sd(pAccept) / sqrt(nSubjs)) %>%
#   ungroup() %>%
#   mutate(Reward = ifelse(Reward == 20, 12, Reward)) %>%
#   ggplot(aes(Reward, propAccept, color = Cost)) +
#     geom_point(size = 3, show.legend = F) +
#     geom_line(show.legend = F) +
#     scale_color_manual(values = cols) +
#     scale_fill_manual(values = cols) +
#     geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.3, size = 1, show.legend = F) +
#     scale_y_continuous(limits = c(-0.1, 1.1), breaks = c(0, 0.5, 1)) +
#     scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
#     labs(y = "", title = "Second Half") +
#     theme(legend.position = c(0.8, 0.2),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))
# 
# 
# 
# grid.arrange(t1, t2, ncol = 2)

dataAll %>%
  filter(Choice < 2) %>%
  group_by(SubjID) %>%
  filter(Handling == 10) %>%
  mutate(Half = ifelse(Half == "Half_1", "First Block", "Last Block")) %>%
  group_by(SubjID, Half, Cost, Reward) %>%
  summarise(pAccept = mean(Choice)) %>%
  group_by(Half, Cost, Reward) %>%
  summarise(propAccept = mean(pAccept),
            SE = sd(pAccept) / sqrt(nSubjs)) %>%
  ungroup() %>%
  mutate(Reward = ifelse(Reward == 20, 12, Reward)) %>%
  ggplot(aes(Reward, propAccept, color = Cost)) +
    geom_point(size = 3, show.legend = F) +
    geom_line(size = 1, show.legend = F) +
    scale_color_manual(values = cols) +
    scale_fill_manual(values = cols) +
    geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.3, size = 1, show.legend = T) +
    scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1)) +
    scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
    labs(y = "Proportion Accepted") +
    facet_wrap(vars(Half), ncol = 2) +
    theme(legend.key = element_blank(),
          legend.position = c(0.9, 0.25),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          panel.background = element_blank(), 
          axis.line = element_line(colour = "black"),
          text = element_text(size = 16))

```


This prompted the question of whether fitting the mixed logistic model could confirm this shift in preferences. Just as before, the matrix below show the coefficient magnitude (color) and pvalues for differences based on iterating through costs as reference dummy codes.

``` {r 16.3.2. mix pre/post separately, echo = FALSE, fig.align="center", fig.width = 5, fig.height = 4}
## compute the same mixed effects logistic for each half
# mixed logistic for the first half
mixLogis_pre <- list()
mixData <- dataAll %>%
  filter(Half == "Half_1") %>%
  group_by(SubjID, Cost, Handling, Reward) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_pre$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait"))
mixLogis_pre$Wait <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Easy"))
mixLogis_pre$Easy <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Cognitive"))
mixLogis_pre$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)

# mixed logistic for the second half
mixLogis_post <- list()
mixData <- dataAll %>%
  filter(Half == "Half_2") %>%
  group_by(SubjID, Cost, Handling, Reward) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_post$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait"))
mixLogis_post$Wait <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Easy"))
mixLogis_post$Easy <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Cognitive"))
mixLogis_post$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Handling + Reward + (Cost | SubjID), family = "binomial", data = mixData)


## now produce a summary matrix
# get the beta and pvalue matrices 
betasPre <- betaMatrix(mixLogis_pre, rearrange = rearrange)
betasPost <- betaMatrix(mixLogis_post, rearrange = rearrange)

# and combine them
betaMat <- matrix(NA, nrow = nrow(betasPre$Betas), ncol = nrow(betasPre$Betas))
betaMat[lower.tri(betaMat)] <- betasPre$Betas[lower.tri(betasPre$Betas)]
betaMat[upper.tri(betaMat)] <- betasPost$Betas[lower.tri(betasPost$Betas)]
dimnames(betaMat) <- list(rearrange, rearrange)

pvalMat <- matrix(NA, nrow = nrow(betasPre$Pvals), ncol = nrow(betasPre$Pvals))
pvalMat[lower.tri(pvalMat)] <- betasPre$Pvals[lower.tri(betasPre$Pvals)]
pvalMat[upper.tri(pvalMat)] <- betasPost$Pvals[lower.tri(betasPost$Pvals)]
dimnames(pvalMat) <- list(rearrange, rearrange)

# remove uninteresting comparisons 
diag(betaMat) <- 0

# aand plot
corrplot(betaMat, 
         is.corr = F, 
         p.mat = pvalMat, 
         outline = T,
         #insig = "p-value",
         sig.level = 0.05, 
         na.label = "square", 
         na.label.col = "grey",
         method = "color", 
         tl.col = "black",
         tl.cex = 0.8,
         col = col2(200))
````

\bigskip

### 16.3.3.

*To estimate the subjective opportunity cost (hypothesis 4.3.2.), we will use a logistic function to model each participantâ€™s probability of completing a trial based on the difference between the delayed rewardâ€™s magnitude and the estimated opportunity cost (OC) for each cost type. OC will be computed as the product of a free parameter (gamma) and the handling time. Both gamma and the temperature parameter of the logistic function will be estimated at the subject level, independently for each subject.*

The gamma optimization search space was bound by extreme choice values. For example, someone with an extremely high OC would not accept even the most beneficial offers, and would reject a 20 cent offer when the handling time was 2 seconds (or 20/2). Conversely, a participant experiencing low opportunity costs would accept the unbeneficial offer of 4 cents for a 14 second delay (or 4/14). The model was optimized to find the lowest value of gamma that significantly reduced the negative log likelihood. The resulting gamma values per participant are shown per group below (gray lines denote the rate of earnings under optimal behavior for all 3 timing contexts), and reflect what was seen in the previous sections: participants in the wait and physical conditions showed higher gamma values (and thus opportunity costs) than in the other two groups. Given the high variability of the easy condition, it might be worth modeling each timing context independently, or perhaps fitting the model on subgroups defined by a median split of the participants based on their acceptance rate.

``` {r 16.3.3. OC modeling, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# group
summaryOC <- list()
summaryOC$all <- dataAll %>%
                 filter(Choice < 2) %>%
                 rename(Offer = Reward) %>%
                 group_by(Cost, SubjID) %>%
                 do(optimizeOCModel(., simplify = T)) %>%
                 ungroup()

# plot
(summaryOC$plot <- ggplot(summaryOC$all, aes(Cost, Gamma, fill = Cost)) +
                      geom_hline(yintercept = 0.64, alpha = 0.9, color = "gray20", size = 1) +
                      geom_hline(yintercept = 0.7, alpha = 0.9, color = "gray40", size = 1) +
                      geom_hline(yintercept = 0.9, alpha = 0.9, color = "gray60", size = 1) +
                      geom_dotplot(stackdir = 'center', binaxis = 'y', show.legend = F) +
                      ylim(0,1.5) +
                      labs(title = "Gamma Values per Group", x = "") +
                      scale_fill_manual(values = cols) +
                      theme(panel.grid.major = element_blank(),
                        panel.grid.minor = element_blank(),
                        panel.background = element_blank(),
                        axis.line = element_line(colour = "black"),
                        text = element_text(size = 16)))

```

\bigskip

### 16.3.4.

*We will cross-validate each subjectâ€™s OC value using the pre-midpoint data for estimation, and post-midpoint choices for testing. The estimates will be used to predict acceptances in the testing sample, and the mean percent correctly predicted will be reported for each group. This will also provide information on the stability of each participantâ€™s choices (4.3.1.).*
  
As can be seen below, the gamma parameter from the OC model was able to predict post-midpoint choices successfully regardless of group. 

``` {r 16.3.4. OC cross-validation, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# Compute the OC for pre-midpoint
summaryOC$pre <- dataAll %>%
                 filter(Choice < 2, Half == "Half_1") %>%
                 group_by(Cost, SubjID) %>%
                 do(optimizeOCModel(., simplify = T)) %>%
                 ungroup() %>%
                 select(SubjID, Gamma)

# let's add the gammas to the second half data and estimate prop predicted on the second half
summaryOC$predicted <- dataAll %>%
                 filter(Choice < 2, Half == "Half_2") %>%
                 group_by(Cost, SubjID) %>%
                 left_join(summaryOC$pre) %>%
                 mutate(OC = Handling * Gamma,
                        predicted = Reward > OC) %>% 
                 summarise(percentPredicted = mean(predicted == Choice) * 100)

# plot
(summaryOC$predictPlot <- ggplot(summaryOC$predicted, aes(Cost, percentPredicted, fill = Cost)) +
                          labs(x = "") +
                          geom_boxplot(show.legend = F) +
                          scale_fill_manual(values = cols) +
                          labs(y = "Percent Predicted") +
                          ylim(0,100) +
                          theme(panel.grid.major = element_blank(),
                          panel.grid.minor = element_blank(),
                          panel.background = element_blank(),
                          axis.line = element_line(colour = "black"),
                          text = element_text(size = 16)))

```

\bigskip

### 16.3.5.

*The OC estimates for each group will be compared using an ANOVA with condition as a factor. This will let us determine which cost type produced the highest discounting.*

``` {r 16.3.5. OC comparisons, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# ANOVA
OCanova <- list()
OCanova$aov <- summary(aov(Gamma ~ Cost, data = summaryOC$all))
Fval <- OCanova$aov[[1]]$`F value`[1]
Pval <- OCanova$aov[[1]]$`Pr(>F)`[1]
Rsquared_aov <- round(OCanova$aov[[1]]$`Sum Sq`[1] / sum(OCanova$aov[[1]]$`Sum Sq`), digits = 2)
  
# Post-hoc pairwise tests
OCanova$perms <- summaryOC$all %>%
  select(Cost, Gamma) %>%
  plyr::dlply("Cost", identity) %>%
  do.call(cbind, .) %>%
  select_if(is.numeric) %>%
  rename(Physical = Physical.Gamma,
         Cognitive = Cognitive.Gamma,
         Wait = Wait.Gamma,
         Easy = Easy.Gamma) %>%
  outer(., ., FUN = Vectorize(permute), simple = T)
diag(OCanova$perms) <- 1

OCanova$ES <- summaryOC$all %>%
  select(Cost, Gamma) %>%
  plyr::dlply("Cost", identity) %>%
  do.call(cbind, .) %>%
  select_if(is.numeric) %>%
  rename(Physical = Physical.Gamma,
         Cognitive = Cognitive.Gamma,
         Wait = Wait.Gamma,
         Easy = Easy.Gamma) %>%
  outer(., ., FUN = Vectorize(cohenD)) %>%
  abs()
```

The analysis of variance was significant (F = `r round(Fval, digits = 2)`, p = `r ifelse(Pval < 0.001, "< 0.001", paste("=", round(Pval, digits = 2)))`, R-squared = `r Rsquared_aov`). Pairwise post-hoc permutations are shown in the following matrix. These results suggest that participants engaged in cognitively effortful demands experience significantly lower opportunity costs than those whose demands involve physical effort and pure delay. The reason for this is unclear, and we will work on it in the near future. 

``` {r 16.3.5. corrplot, echo = FALSE, fig.align="center", fig.width=5, fig.height=5}
# plot all pairwise comparisons
corrplot(OCanova$ES,
         is.corr = F, 
         p.mat = OCanova$perms,
         type = "upper",
         insig = "p-value",
         sig.level = -1,
         na.label = "square",
         na.label.col = "grey",
         method = "color", 
         tl.col = "black", 
         tl.cex = 0.8,
         outline = T)
```



\bigskip

### Testing grounds

``` {r Testing plots, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4}
# a facet grid of p(acceptances) as a function of reward x handling, distinguishing between first handling time experienced,
# for each cost separately.
# think of doing it for the block order and then prop accepted across costs
# dataAll %>%
#   group_by(SubjID) %>%
#   mutate(BlockOrder = case_when(
#     Handling[1] == 2 ~ "Two",
#     Handling[1] == 10 ~ "Ten",
#     Handling[1] == 14 ~ "Fourteen",
#   )) %>%
#   group_by(SubjID, Cost, Handling, Reward, BlockOrder) %>%
#   summarise(pAccept = mean(Choice)) %>%
#   ggplot(aes(interaction(Reward, Handling), pAccept, linetype = BlockOrder, color = BlockOrder)) + 
#     geom_point(size = 3) + 
#     #geom_errorbar(aes(ymin = meanComplete - SE, ymax = meanComplete + SE), width = 0.2, size = 1) +
#     geom_line(aes(group = interaction(Handling, SubjID)), size = 1) +
#     #geom_point(aes(y = round(optimal)), shape = 21, fill = "grey75", color = "grey30", size = 3, show.legend = F) +
#     labs(x = "Reward.Handling", y = "Proportion Accepted") +
#     scale_color_manual(values = cols) +
#     facet_wrap(vars(Cost), ncol = 2) +
#     theme(legend.key = element_blank(),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))

# # now for each first block experienced, how do costs compare per reward x handling
# dataAll %>%
#   group_by(SubjID) %>%
#   mutate(FirstBlock = case_when(
#     Handling[1] == 2 ~ "Two",
#     Handling[1] == 10 ~ "Ten",
#     Handling[1] == 14 ~ "Fourteen",
#   )) %>%
#   group_by(Cost, Handling, Reward, FirstBlock) %>%
#   summarise(pAccept = mean(Choice),
#             SE = sd(Choice) / sqrt(length(Choice))) %>%
#   ggplot(aes(interaction(Reward, Handling), pAccept, color = Cost)) + 
#     geom_point(size = 3) + 
#     geom_errorbar(aes(ymin = pAccept - SE, ymax = pAccept + SE), width = 0.4, size = 1) +
#     geom_line(aes(group = interaction(Handling, Cost)), size = 1) +
#     labs(x = "Reward.Handling", y = "Proportion Accepted") +
#     scale_color_manual(values = cols) +
#     facet_wrap(vars(FirstBlock), ncol = 3) +
#     theme(legend.key = element_blank(),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))


# # now also separated by half
# dataAll %>%
#   group_by(SubjID) %>%
#   mutate(FirstBlock = case_when(
#     Handling[1] == 2 ~ "Two",
#     Handling[1] == 10 ~ "Ten",
#     Handling[1] == 14 ~ "Fourteen",
#   )) %>%
#   group_by(Cost, Handling, Reward, FirstBlock, Half) %>%
#   summarise(pAccept = mean(Choice),
#             SE = sd(Choice) / sqrt(length(Choice))) %>%
#   ggplot(aes(interaction(Reward, Handling), pAccept, color = Cost)) + 
#     geom_point(size = 3) + 
#     geom_errorbar(aes(ymin = pAccept - SE, ymax = pAccept + SE), width = 0.4, size = 1) +
#     geom_line(aes(group = interaction(Handling, Cost)), size = 1) +
#     labs(x = "Reward.Handling", y = "Proportion Accepted") +
#     scale_color_manual(values = cols) +
#     facet_wrap(vars(Half, FirstBlock), ncol = 3) +
#     theme(legend.key = element_blank(),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))

# forced travels as a function of time
# number of forced travels as a function of time for cognitive and grip trials
forcedTravels <- dataAll %>%
  #filter(rawChoice == 2) %>%
  group_by(SubjID, Cost, Block) %>%
  filter(Cost %in% c("Cognitive", "Physical")) %>%
  summarise(pTravel = mean(rawChoice == 2)) %>%
  group_by(Cost, Block) %>%
  summarise(mTravels = mean(pTravel),
            SE = sd(pTravel) / sqrt(21)) 

tempphys <- tibble(Cost = as.factor(rep("Physical", 6)),
                   Block = as.numeric(seq(6)),
                   mTravels = rep(0, 6),
                   SE = rep(0, 6))

forcedTravels <- full_join(forcedTravels, tempphys)

ggplot(data = forcedTravels, aes(Block, mTravels, color = Cost )) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mTravels - SE, ymax = mTravels + SE), width = 0.4) +
  scale_x_continuous(breaks = seq(6)) +
  labs(y = "Prop. of forced travels") +
  ylim(0, NA) +
  theme(legend.key = element_blank(),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black"),
        text = element_text(size = 16))
```












































## 17. Inference criteria

These criteria will remain the same as before. For the ANOVAs, we will report the F statistic as well as the p-value. For the multiple linear regressions and logistic regression, we will report the betas (coefficients) and their associated p-values. When comparing two groups of values as in pairwise comparisons, we will use non-parametric permutation tests. We will report p-values and effect sizes, and all tests will be two-tailed. The alpha threshold for p-value evaluation will be set at 0.05.

## 18. Data exclusion

### 18.1. Missing data and quitting early. 

Participants are told that they are allowed to stop participation in the study at any time without consequence. Data from participants who do not finish the experiment will be excluded from the study, and those participants will be replaced. 

### 18.2. Inattentiveness

To guard against inattentiveness, a single 30 second catch trial will be placed at the end of each experimental block, merely prompting participants to press a key. From the pilot study, participants are able to respond within three seconds. Based on this, a response within three seconds will be considered as proper engagement, and a participant that fails two or more of these checks will be excluded and replaced. 

### 18.3. Reasonable choices

The task is structured so that one reward amount must always be accepted. Thus, a participant who quits every trial in at least one block will be assumed not to have followed or understood task instructions, or to have disengaged from the task altogether, and will be excluded and replaced. 

### 18.4. Performance

In the cognitive effort condition, participants were forced to travel if they made 2 mistakes in a trial. Any participant with more than 30% forced travels will be excluded and replaced. 

## 5. Existing data 

Registration prior to creation of data

## 6. Data collection
	
The study will use human subjects recruited from the university community, who will be recruited through the university job board. We will pay up to $30 for a session (1 hour), which includes incentive-based rewards. Participants will be ineligible if they have participated in our pilot study, or they are under 18 years of age. The study will run for however long it takes to reach our desired sample size. 
	
## 7. Sample size 

We will acquire a sample of 48 participants. 
	
## 8. Sample size rationale? 

Sample sizes were determined by means of power analysis (repeated measures ANOVA), using a significance level of 0.05, power of 0.8, a desired effect size of f = 0.5, and four groups (one for each condition). The resulting sample size was 45, which we increased to 48 in order to match the potential order of blocks and colors (section 15) while further ensuring that we have enough power. 

## 9. Variables

### 9.1 The present study has three levels of manipulation. 

9.1.1. Cost condition, manipulated within subjects. The conditions are (a) unfilled delay, (b) cognitive effort, and (c) physical effort. Unfilled delays will be paired with each effort type in a blocked manner (7-minute blocks). Each combination will be experienced three times.
		
9.1.2. Handling and travel times. These will dictate how long each trial is. The "handling time" governs how long it is necessary to wait or work to obtain a reward if a trial is accepted. The "travel time" governs how long the inter-trial interval will be, regardless of whether the trial is accepted. Participants will know that the handling time will be always 10 seconds, and the travel time 6 seconds (matching one of the between-group experiment conditions). 
		
9.1.3. Reward magnitude, manipulated within-subject across individual trials. On each trial, participants will learn the amount of money they can earn if they complete the trial (i.e. 4, 8, or 20 cents). The amounts will be sampled uniformly, while guaranteeing that each participant experiences each amount at least twice during each block.


## 10. Measured variables 

### 10.1. Behavioral variables 
		
10.1.1. Choice (complete or quit a given trial). 

10.1.2. Response times. 

10.1.3. Total earnings. 

10.1.3. Performance in the cognitive effort tasks (right or wrong answer). 

10.1.4. Grip force throughout the experiment and maximum grip strength (physical effort condition). 


### 10.2. Questionnaire 

10.2.1. Age.

10.2.2. Gender.

10.2.3. Post-experimental oral assessment of choice strategy adoption. We will ask participants the following: "Did you follow a specific acceptance strategy? If so, what was it?" and "Which condition felt most effortful to you?". Answers will be written down by the experimenter, and recorded on an excel spreadsheet. 

## 11. Indices

### 11.1. Behavioral data 

11.1.1. Proportion of prospects accepted. 

11.1.2. Percentage of cognitive effort trials with above-threshold errors (over 2 errors), in which participants are forced to travel to the next trial. 

### 11.2. Optimal behavior 

The theoretical opportunity cost of time will be calculated for each handling/travel/reward acceptance combination in order to assess choice optimality. This measure is computed as the cents per second possible when consistently accepting only certain reward amounts (e.g. 8 and 20 cents) in a given timing condition. This ground truth measure is independent of cost type, allowing us to assess subject optimality.

## 12. Study type

Experiment - A researcher randomly assigns treatments to study subjects, this includes field or lab experiments. This is also known as an intervention experiment and includes randomized controlled trials.

## 13. Blinding

No blinding is involved in this study.

## 14. Study design 

The present study complements our previous one, which evaluated cost types through a between-group design. Many of the details remain the same. However, this time every participant will experience all forms of cost (delay, physical effort, and cognitive effort). Delay trials will be intermixed with both physical and cognitive trials, providing a common condition to compare these effortful costs. On each trial, participants will have the opportunity to either wait or work for 10 seconds (the "handling time") in order to earn a reward; alternatively, they can quit by pressing the space bar and skip to the next trial (which incurs a 6 second travel time). Two types of trial combinations (physical/wait and cognitive/wait) will be experienced three times by each participant in 7-minute-long, interleaved blocks. Reward amounts can be 4, 8, or 20 cents, uniformly distributed. Timing information will be disclosed at the beginning of each block, and reward prospects displayed before each trial begins. 

## 15. Randomization

### 15.1. Block type 
Each block will contain unfilled delay trials and trials of one effortful cost type (se 14.1). Blocks with each cost type will be interleaved, and participants will be assigned to one of two orders: beginning with a cognitive/wait or a physical/wait block. A row will be assigned to subject IDs prior to recruitment, ensuring that each order is experienced equally often. Participants will be offered a break after completing three out of the six blocks. 

### 15.2. Rewards and cost type 
Participants can earn 4, 8, or 20 cents per trial at the cost of performing one of the three conditions. These values are sampled without replacement from a vector containing every combination of cost (effort defined by the current block, and delay) and reward. The vector is reshuffled once completed. This ensures that each combination of cost and reward is presented at least twice per block.

### 15.3. Trial color 
During the offer phase, each cost type will appear in a unique color in order to ease its identification. Colors were chosen to contrast each other while being amenable to red-green color-blindness. Colors will be assigned to each cost type so that all three colors are equally paired to each cost type across participants. A row will be assigned to subject IDs prior to recruitment to indicate their block order and color pairing. 
