---
title: "Effort and Delay Discounting in a Foraging Environment"
author: "Claudio Toro Serey & Joseph T. McGuire"
output:
  html_document: default
  pdf_document: default
  fig_caption: yes
  word_document: default
---

# Pre-registration follow-up

This document follows the steps planned for the pre-registration (https://osf.io/2rsgm/). The bullet points match the organization shown on the website as closely as possible. First, here is a restatement of the questions and hypotheses.

## Research Questions

#### 3.1.	Do decision makers in foraging environments integrate information about delay durations and reward amounts to produce reward-maximizing behavior?

#### 3.2.	Main question: Are decisions affected differently by equivalent time periods of pure delay, cognitive effort, physical effort, and non-effortful task engagement? Do these four conditions involve different levels of subjective costs? 

#### 3.3.	How can we best computationally model the perceived cost of time and effort to predict choices in each condition?

## Hypotheses

#### 4.1.	Single-option, accept/reject decisions will be influenced by within-subject manipulations of reward magnitude and associated delay duration in line with a theoretical reward-maximizing strategy.
 **4.1.1.**	Participants will more frequently accept high-reward prospects than low-reward prospects.
 
 **4.1.2.**	The tendency to reject low-reward prospects will be greater in environments where rewards are associated with longer delays (handling times).

#### 4.2.	Prospect acceptance rates will differ across four between-subject conditions, in which the delays associated with rewards (a) are unfilled, (b) include a cognitive effort requirement, (c) include a physical effort requirement, or (d) require a trivial level of physical effort but have matched visual stimuli to the physical effort condition.
  **4.2.1.**	In the cognitive effort condition, overall acceptance rates will be greater than in the unfilled-delay condition.
  
  **4.2.2.**	In the physical effort condition, overall acceptance rates will be greater than in the unfilled-delay condition, and equivalent to the cognitive effort condition.
  
  **4.2.3.**	In the trivial effort condition, acceptance rates will be equivalent to the unfilled-delay condition, and lower than in the physical effort and cognitive effort conditions.

#### 4.3.	Choices will be well fit by a computational model in which the subjective opportunity cost of time is free to vary across the four between-subject conditions.
  **4.3.1.**	Participants will display stable preferences, meaning that the reward amounts they accept in a given timing condition will be similar throughout the experiment.
  
  **4.3.2.**	Subject-specific opportunity cost (OC) estimates will vary inversely with acceptance rates. Thus, the unfilled-delay condition will produce higher OC estimates than both effort conditions, which in turn will show no differences between them.  

\bigskip

## Analyses

```{r Global Options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

``` {r Libraries & Functions, echo=FALSE}
# libraries
library(tidyverse)
library(ggfortify)
library(knitr)
library(pander) 
library(nloptr)
library(pwr)
library(lme4)
#library(lmerTest)
library(gridExtra)
library(reshape2)
library(corrplot)

## Miscelaneous
lbls <- c("Wait","Cognitive","Physical","Easy")
cols = c("#78AB05","#D9541A","deepskyblue4", "darkgoldenrod2") # plot colors (wait, effort)
lthick = 2 # line thickness for plots

## Functions
# Permutation for 2 groups
permute <- function(group1 = 1, group2 = 2, statType = mean, nPerms = 5000, paired = FALSE){
  
    # prep data
    summaryPerm <- list()
    lOne <- length(group1)
    lTwo <- length(group2)
    bigSample <- c(group1,group2)  
    
    if (paired == FALSE) {
      
        
        for (i in 1:nPerms){
          
            # relabel samples
            tempBig <- sample(bigSample)
            tempOne <- tempBig[1:lOne]
            tempTwo <- tempBig[(lOne+1):length(bigSample)]
            
            # stats
            tempDiffs <- statType(tempOne,na.rm=T) - statType(tempTwo,na.rm=T)
            summaryPerm$jointDist[i] <- tempDiffs # statType(tempDiffs, na.rm = T) 
          
        }  
        
    } else {
      
        for (i in 1:nPerms){
          
            # shift labels in a pairwise fashion
            tempDiffs <- statType((-1)^rbinom(lOne,1,0.5) * (group1 - group2))
            summaryPerm$jointDist[i] <- tempDiffs
          
        }
      
    }
  
    # get the observed difference
    diffs <- statType(group1,na.rm=T) - statType(group2,na.rm=T)
    observedAbs <- abs(diffs) # maybe leave it as means here
    observed <- diffs
    summaryPerm$Pval <- 2 * (1 - ecdf(summaryPerm$jointDist)(observedAbs))
    if (length(unique(abs(summaryPerm$jointDist))) == 1) {summaryPerm$Pval <- 1} # if the difference is always the same, then p = 1
    summaryPerm$Observed <- observed
    
    return(summaryPerm)
    
}

# Non-parametric Bootstrap for a single group
bootstrap <- function(group = 1, statType = mean, B = 5000){
  
    # prep param
    bootStats <- rep(0,B)
    
    # iterate
    for(b in 1:B){
      
      # wait group
      x <- sample(group,length(group),replace=T)  
      bootStats[b] <- statType(x,na.rm = T)
      
    }
    
    return(bootStats)
    
}

# plot logistic (for the prop complete test, mainly, but it can be easily expanded for more flexibility in n-params)
plotLogis <- function(coeffs = 1, group = "Cognitive", handling = 1:14, reward = 1:20, xAxis = "Reward", plotType = lines, color = "black", lType = 1, pchType = 16){
  
    
    # divide into relevant coefficients (assumes logisComb was the best model)
    intercept <- coeffs[1]
    cogBeta <- coeffs[2]    
    physBeta <- coeffs[3]
    easyBeta <- coeffs[4]
    handBeta <- coeffs[5]
    rwdBeta <- coeffs[6]
    
    #wait <- numeric()
    #phys <- numeric()
    
    # now the parameters
    label = "Reward Amount"
    if (group == "Cognitive"){cog = 1} else {cog = 0}
    if (group == "Phys"){phys = 1} else {phys = 0}
    if (group == "Easy") {easy = 1} else {easy = 0}
    if (xAxis == "Reward"){
        xAxis = reward 
        label = "Reward Amount" 
    } else {
        xAxis = handling
        label = "Handling Time"
    }
    
    # logistic function
    model <- intercept + cogBeta*cog + physBeta*phys + easyBeta*easy + handBeta*handling + rwdBeta*reward 
    func <- 1 / (1 + exp(-(model)))
    plotType(xAxis,
         func,
         ylim = c(0,1),
         xlim = c(0,25),
         xlab = label,
         ylab = "Probability of Completing a Trial",
         type="l",
         col = color,
         lty = lType,
         pch = pchType,
         lwd = 2,
         xaxt = "n",
         bty = "n")
  
    # use title() to add title afterwards
}

# Cohen's D for 2 groups
# for a more flexible approach, make the data input to be a list with entries for n groups, 
# then do length(list) for the number of groups. 
cohenD <- function(group1 = 1, group2 = 2){
  
      # means
      mean1 <- mean(group1, na.rm = T)
      mean2 <- mean(group2, na.rm = T)
      
      # variance
      var1 <- var(group1, na.rm = T)
      var2 <- var(group2, na.rm = T)
      
      # equation
      out <- (mean1 - mean2) / sqrt((var1 + var2)/2)
      
      return(out)
  
}

# get the number of continuous quits before current trial
seqQuits <- function(subjData = 1){
  
    # get the lag
    tempSeq <- sequence(rle(subjData)$lengths) # sequence
    tempSeq[subjData==1] <- 0 # not interested in completed runs, just quits
    tempSeq <- c(0,tempSeq) # lag it so it can be used as a t-1 regressor

    # return data so that the length is equivalent
    out <- tempSeq[1:length(subjData)]
    
    return(out)
    
}

# basic power calculation for a one way ANOVA
powerCalc <- function(d = 1.2, Za = 1.96, Zb = 0.842){
  
  out <- 2*((Za + Zb)/d)^2 + (0.25*(Za^2))
  
  return(round(out))
  
}

## gather basic data (this is better as a function, then rbind)
summarizeData <- function(Data = waitData, nSubs = nSubsW, type = "Wait"){
  
  tempSummary <- data.frame(group = rep(type, nSubs))
  
  if (type == "Cognitive") {
    
    for (subj in seq(nSubs)){
      
        # Choice index
        tempChoice <- Data[[subj]]$Choice
        
        # Proportion complete
        tempSummary[subj,2] <- mean(tempChoice)
      
        # earnings
        tempSummary[subj,3] <- sum(Data[[subj]]$Reward[tempChoice==1])
        
        # RTs
        tempSummary[subj,4] <- median(Data[[subj]]$`Choice RT`[tempChoice==0])
    
        # Mistakes (only for cognitive effort group)
        tempMistakes <- sum(cogDataLong[[subj]]$`Trial Outcome`=="Forced travel") # how many forced travel trials
        tempTrialnum <- length(Data[[subj]]$Choice) # how many decision trials
        tempSummary[subj,5] <- tempMistakes / (tempMistakes + tempTrialnum) # proportion of forced travels throughout the exp        
    
    }    
    
  } else {
    
    for (subj in seq(nSubs)){
        
        # choice index
        tempChoice <- Data[[subj]]$Choice
      
        # propotion complete
        tempSummary[subj,2] <- mean(tempChoice)
        
        # earnings
        tempSummary[subj,3] <- sum(Data[[subj]]$Reward[tempChoice==1])
        
        # RTs
        tempSummary[subj,4] <- median(Data[[subj]]$RT[tempChoice==0])
        
        # Mistakes (to allow for mistakes on the cognitive group)
        tempSummary[subj,5] <- NA
        
    }
  
  }
  
  return(tempSummary)
  
}

# Summarize the proportion complete for GLMs
propComplete <- function(Data = waitData, nSubs = nSubsW, indx = indxWait, type = "Wait") {
    
    tempProp <- data.frame(SubjID = rep(indx,9),
                           Group = rep(type,nSubs*9),
                           Handling = c(rep(2,nSubs*3), 
                                        rep(10,nSubs*3), 
                                        rep(14,nSubs*3)),
                           Reward = rep(c(rep(4,nSubs),rep(8,nSubs),rep(20,nSubs)),3),
                           propComplete = rep(NA,length(indx)*9),
                           totalTrials = rep(NA,length(indx)*9),
                           totalComplete = rep(NA,length(indx)*9))                         
                           
    for (subj in seq(nSubs)){  
      
      # handling of 2s
      tempProp$propComplete[subj] <- mean(Data[[subj]]$Choice[Data[[subj]]$Handling==2 & Data[[subj]]$Reward==4])
      tempProp$propComplete[subj+nSubs] <- mean(Data[[subj]]$Choice[Data[[subj]]$Handling==2 & Data[[subj]]$Reward==8])
      tempProp$propComplete[subj+(nSubs*2)] <- mean(Data[[subj]]$Choice[Data[[subj]]$Handling==2 & Data[[subj]]$Reward==20])
      tempProp$totalTrials[subj] <- length(Data[[subj]]$Choice[Data[[subj]]$Handling==2 & Data[[subj]]$Reward==4])
      tempProp$totalTrials[subj+nSubs] <- length(Data[[subj]]$Choice[Data[[subj]]$Handling==2 & Data[[subj]]$Reward==8])
      tempProp$totalTrials[subj+(nSubs*2)] <- length(Data[[subj]]$Choice[Data[[subj]]$Handling==2 & Data[[subj]]$Reward==20])
  
      # handling of 10s
      tempProp$propComplete[subj+(nSubs*3)] <- mean(Data[[subj]]$Choice[Data[[subj]]$Handling==10 & Data[[subj]]$Reward==4])
      tempProp$propComplete[subj+(nSubs*4)] <- mean(Data[[subj]]$Choice[Data[[subj]]$Handling==10 & Data[[subj]]$Reward==8])
      tempProp$propComplete[subj+(nSubs*5)] <- mean(Data[[subj]]$Choice[Data[[subj]]$Handling==10 & Data[[subj]]$Reward==20])
      tempProp$totalTrials[subj+(nSubs*3)] <- length(Data[[subj]]$Choice[Data[[subj]]$Handling==10 & Data[[subj]]$Reward==4])
      tempProp$totalTrials[subj+(nSubs*4)] <- length(Data[[subj]]$Choice[Data[[subj]]$Handling==10 & Data[[subj]]$Reward==8])
      tempProp$totalTrials[subj+(nSubs*5)] <- length(Data[[subj]]$Choice[Data[[subj]]$Handling==10 & Data[[subj]]$Reward==20])    
      
      # handling of 14s
      tempProp$propComplete[subj+(nSubs*6)] <- mean(Data[[subj]]$Choice[Data[[subj]]$Handling==14 & Data[[subj]]$Reward==4])
      tempProp$propComplete[subj+(nSubs*7)] <- mean(Data[[subj]]$Choice[Data[[subj]]$Handling==14 & Data[[subj]]$Reward==8])
      tempProp$propComplete[subj+(nSubs*8)] <- mean(Data[[subj]]$Choice[Data[[subj]]$Handling==14 & Data[[subj]]$Reward==20])
      tempProp$totalTrials[subj+(nSubs*6)] <- length(Data[[subj]]$Choice[Data[[subj]]$Handling==14 & Data[[subj]]$Reward==4])
      tempProp$totalTrials[subj+(nSubs*7)] <- length(Data[[subj]]$Choice[Data[[subj]]$Handling==14 & Data[[subj]]$Reward==8])
      tempProp$totalTrials[subj+(nSubs*8)] <- length(Data[[subj]]$Choice[Data[[subj]]$Handling==14 & Data[[subj]]$Reward==20])
    
    }
      
    return(tempProp)
    
}

# OC-specific computation of the negative log likelihood for model optimization
negLogLik <- function(params, choice, handling, reward) {
  gamma <- exp(params[2]) * handling
  model <- exp(params[1]) * (reward - gamma)
  p = 1 / (1 + exp(-model))
  p[p == 1] <- 0.999
  p[p == 0] <- 0.001
  tempChoice <- rep(NA, length(choice))
  tempChoice[choice==1] <- log(p[choice==1])
  tempChoice[choice==0] <- log(1 - p[choice==0])
  negLL <- -sum(tempChoice)
  return(negLL)
}

# minimize the negative log likelihood
optimizeOCModel <- function(Data, Algorithm = "NLOPT_LN_NEWUOA") {
  # Prep data
  handling <- Data$Handling
  reward <- Data$Reward
  choice <- Data$Choice
  
  # Prep list of results to be returned
  out <- list()
  out$percentQuit <- (sum(choice==0) / length(choice)) * 100
  out$percentAccept <- (sum(choice==1) / length(choice)) * 100 
  miss <- (choice != 1) & (choice != 0)
  out$percentMiss <- (sum(miss) / length(choice))  * 100
  choice <- choice[!miss]
  reward <- as.numeric(reward[!miss])
  handling <- as.numeric(handling[!miss])
  
  # Establish lower and upper bounds
  LB <- log((min(reward)/max(handling)) * 0.99)
  UB <- log((max(reward)/min(handling)) * 1.01)
  
  # Begin defining parameters
  # If choices are one-sided (i.e. all accepted)
  if ((sum(choice) == length(choice)) | (sum(choice) == 0)) {
    ifelse(sum(choice) == length(choice), out$Gamma <- exp(LB), out$Gamma <- exp(UB)) 
    out$Scale <- NA
    out$LL <- 0
  } else {
    # Create a feasible region (search space)
    params <- as.matrix(expand.grid(scale = c(-1, 1), gamma = seq(LB, UB, length = 3)))
    # Create a list to check the minimization of the negative log lik.
    info <- list()
    info$negLL <- Inf
    # Define the options to be used during optimization
    # Consider looking into other optimization algorithms and global minima
    opts <- list("algorithm" = Algorithm,
               "xtol_rel" = 1.0e-8)
    # Optimize the sOC over all possible combinations of starting points
    for (i in seq(nrow(params))) {
      tempInfo <- nloptr(x0 = params[i, ], 
                   eval_f = negLogLik, 
                   lb = c(log(0.001), LB),
                   ub = c(-log(0.001), UB),
                   opts = opts,
                   choice = choice, 
                   handling = handling,
                   reward = reward)
      if (tempInfo$objective < info$negLL) {
        #print("Minimized")
        info$negLL <- tempInfo$objective
        info$params <- tempInfo$solution
      }
    }
    out$Gamma <- exp(info$params[2])
    out$Scale <- exp(info$params[1])
    out$LL <- -info$negLL
  }
  
  # Summarize the outputs
  out$LL0 <- log(0.5) * length(choice)
  out$Rsquared <- 1 - (out$LL/out$LL0)
  out$subjOC <- out$Gamma * handling
  out$p <- 1 / (1 + exp(-(out$Scale*(reward - out$subjOC))))
  out$predicted <- reward > out$subjOC
  out$predicted[out$predicted == TRUE] <- 1
  out$percentPredicted <- (sum(out$predicted == choice) / length(choice)) * 100
  
  # adjust the probabilities in case of extreme gammas
  if (out$Gamma <= exp(LB)) {
    out$Gamma <- exp(LB) # temporary condition because Nlopt is not respecting the lower bound
    out$p <- rep(1, length(choice))
  } else if (out$Gamma == exp(UB)) {
    out$p <- rep(0, length(choice))
  } 
  
  return(out)

}

```

``` {r Load Data, echo=FALSE}
### data loading and cleaning up
setwd("./data")
files <- dir(pattern = '_log.csv')
nSubjs <- length(files)

dataAll <- tibble(SubjID = files) %>%
  mutate(contents = map(SubjID, ~ read_csv(., col_types = cols())))  %>%
  mutate(Cost = substring(SubjID, 5, 8),
         Cost = case_when(Cost == "wait" ~ "Wait",
                          Cost == "cogT" ~ "Cognitive",
                          Cost == "phys" ~ "Physical",
                          Cost == "phea" ~ "Easy"),
         Cost = factor(Cost, levels = c("Physical", "Cognitive", "Wait", "Easy")),
         SubjID = substring(SubjID, 0, 3)) %>%
  unnest() %>%
  mutate(Half = ifelse(Block < 4, "Half_1", "Half_2")) 

subjList <- unique(dataAll$SubjID)


```

``` {r Basic Prepping, echo=FALSE}  
# # params for reference
# handling <- c(2, 10, 14)
# cond <- c("Wait", "Cognitive", "Physical", "Easy")
# 
# # create data frame with summary stats
# tempW <- summarizeData()
# tempC <- summarizeData(Data = cogData, nSubs = nSubsC, type = "Cognitive")
# tempP <- summarizeData(Data = physData, nSubs = nSubsP, type = "Physical")
# tempE <- summarizeData(Data = easyData, nSubs = nSubsE, type = "Easy")
# 
# summaryData <- rbind(tempW,
#                      tempC,
#                      tempP,
#                      tempE)
# 
# colnames(summaryData) <- c("Group", "propComplete", "Earnings", "RT", "forcedTravel")
# 
# summaryData$Cols <- c(rep(cols[1],nSubsW), rep(cols[2],nSubsC), rep(cols[3],nSubsP), rep(cols[4],nSubsE))
# rm(tempW, tempC, tempP, tempE)
# 
# # Indexes for groups
# indxWait <- seq(nSubsW)
# indxWaitLong <- seq(nSubsW * 9) # 9 being the number of combinations
# indxCog <- (nSubsW + 1):(nSubsW + nSubsC)
# indxCogLong <- ((nSubsW * 9) + 1):((nSubsW * 9) + (nSubsC * 9))
# indxPhys <- ((nSubsW + nSubsC) + 1):(nSubsW + nSubsC + nSubsP)
# indxPhysLong <- ((nSubsW * 9) + (nSubsC * 9) + 1):((nSubsW * 9) + (nSubsC * 9) + (nSubsP * 9))
# indxEasy <- ((nSubsW + nSubsC + nSubsP) + 1):totalSubs #(nSubsW + nSubsC + nSubsP + nSubsE)
# indxEasyLong <- ((nSubsW * 9) + (nSubsC * 9) + (nSubsP * 9) + 1):((nSubsW * 9) + (nSubsC * 9) + (nSubsP * 9) + (nSubsE * 9))
# 
# 
# # the other, just summarizing for all participants
# # Not sure if this is necessary anymore
# # allProp <- data.frame(Group = c(rep("Wait",nSubsW*3),
# #                                 rep("Cognitive",nSubsC*3),
# #                                 rep("Physical",nSubsP*3)),
# #                       Handling = c(rep(2,nSubsW), rep(10,nSubsW), rep(14,nSubsW),
# #                                    rep(2,nSubsC), rep(10,nSubsC), rep(14,nSubsC), 
# #                                    rep(2,nSubsP), rep(10,nSubsP), rep(14,nSubsP)),
# #                       FivePts = rep(NA,(nSubsW + nSubsC + nSubsP)*9),
# #                       TenPts = rep(NA,(nSubsW + nSubsC + nSubsP)*9),
# #                       TwntyfivePts = rep(NA,(nSubsW + nSubsC + nSubsP)*9))
# 
# 
# # Get the proportion completed per combination for GLMs
# tempW <- propComplete()
# tempC <- propComplete(Data = cogData, nSubs = nSubsC, indx = indxCog, type = "Cognitive")
# tempP <- propComplete(Data = physData, nSubs = nSubsP, indx = indxPhys, type = "Physical")
# tempE <- propComplete(Data = easyData, nSubs = nSubsE, indx = indxEasy, type = "Easy")
# 
# allProp2 <- rbind(tempW,
#                   tempC,
#                   tempP,
#                   tempE)  
# allProp2$totalComplete <- allProp2$propComplete * allProp2$totalTrials
# 
# rm(tempW, tempC, tempP, tempE)
# 
# # add optimal vector and difference from optimal
# wait <- c(rep(1,nSubsW*3),
#           c(rep(0,nSubsW), rep(1,nSubsW*2)),
#           c(rep(0,nSubsW*2), rep(1,nSubsW)))
# cog <- c(rep(1,nSubsC*3),
#           c(rep(0,nSubsC), rep(1,nSubsC*2)),
#           c(rep(0,nSubsC*2), rep(1,nSubsC)))
# phys <- c(rep(1,nSubsP*3),
#           c(rep(0,nSubsP), rep(1,nSubsP*2)),
#           c(rep(0,nSubsP*2), rep(1,nSubsP)))
# easy <- c(rep(1,nSubsE*3),
#           c(rep(0,nSubsE), rep(1,nSubsE*2)),
#           c(rep(0,nSubsE*2), rep(1,nSubsE)))
# allProp2$Optimal <- c(wait,cog,phys,easy)
# allProp2$Deviation <- allProp2$Optimal - allProp2$propComplete
# 
# # sort by subject ID
# #allProp2 <- allProp2[order(allProp2$SubjID),]
# 
# # actual proportions and standard errors, and the optimal summary for plotting
# trueProp <- aggregate(allProp2$propComplete,
#             by = list(allProp2$Reward, allProp2$Handling, allProp2$Group),
#             FUN = 'mean')
# 
# trueSD <- aggregate(allProp2$propComplete,
#           by = list(allProp2$Reward, allProp2$Handling, allProp2$Group),
#           FUN = 'sd')
# colnames(trueProp) <- c("Reward","Handling","Group","propComplete")
# trueProp$SE[1:9] <- (trueSD$x[1:9])/(sqrt(nSubsW))
# trueProp$SE[10:18] <- (trueSD$x[10:18])/(sqrt(nSubsC))
# trueProp$SE[19:27] <- (trueSD$x[19:27])/(sqrt(nSubsP))
# trueProp$SE[28:36] <- (trueSD$x[28:36])/(sqrt(nSubsE))
# 
# trueProp$Optimal <- aggregate(allProp2$Optimal,
#             by = list(allProp2$Reward, allProp2$Handling, allProp2$Group),
#             FUN = 'mean')$x
```

#### 16.1.	Tests of whether decision makers integrate delay and reward information. 

  **16.1.1.**	*To address hypothesis 4.1., A logistic regression will be fit for each participant in order to predict trial-wise acceptances, using handling time and reward amount as predictors. The resulting beta coefficients for handling time and reward will be pooled across all participants, and we will perform a one-sample rank-sum test on each set of coefficients to examine whether the they are significantly positive or negative (compared to 0). If the group coefficients are significantly positive, it would mean that a predictor reliably increases the likelihood of acceptance. This will allow us to determine whether increments in handling time and reward amounts increased and decreased the likelihood of acceptance for each participant, respectively.* 
  
``` {r 16.1.1., fig.align="center", fig.width = 4, fig.height = 4, echo = FALSE}
# filterout errors and compute R + H logistic per subject, then get coefficients
logisRH <- dataAll %>%
  filter(Choice < 2) %>%
  plyr::dlply("SubjID", identity) %>%
  lapply(function(data) {glm(Choice ~ Handling + Reward, data = data, family = "binomial")}) %>%
  sapply(coefficients)

# Rank sum tests
rankHand <- wilcox.test(logisRH["Handling", ])
rankReward <- wilcox.test(logisRH["Reward", ])

# Plot coeffs
temp <- melt(logisRH[c("Handling", "Reward"), ])
qplot(data = temp, x = Var1, y = value, geom = "boxplot") + 
  labs(title = "Handling and Reward Coefficients", x = "", y = "Coefficients") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme(legend.position = c(0.9, 0.7),
       panel.grid.major = element_blank(), 
       panel.grid.minor = element_blank(), 
       panel.background = element_blank(), 
       axis.line = element_line(colour = "black"),
       text = element_text(size = 16))
```

The plot shows the pooled coefficients across subjects for the effect of each predictor. The results show the expected discounting effect of increasing the handling time, as well as the increase in acceptance likelihood as a function of reward increments. We found that these coefficients were significantly different from zero for both handling (V = `r rankHand$statistic`, p `r ifelse(rankHand$p.value < 0.001, "< 0.001", paste("=", round(rankHand$p.value, digits = 2)))`) and reward (V = `r rankReward$statistic`, p `r ifelse(rankReward$p.value < 0.001, "< 0.001", paste("=", round(rankHand$p.value, digits = 2)))`) regardless of cost condition.

\bigskip

  **16.1.2.**	*We will perform an extension of the logistic regression from 16.1.1., this time adding an autoregressive covariate containing the number of consecutive quits prior to a given trial. In this way, we will examine the possibility that participant choices were governed by recent quitting history rather than the experimental parameters (see 11.1.3.). Coefficients not significantly different from 0 will denote that a participant did not rely on recent quitting history.*
  
``` {r 16.1.2., echo = FALSE, include = FALSE}
# add an AR regressor to track recent quits
logisRHAR <- dataAll %>%
  filter(Choice < 2) %>%
  group_by(SubjID) %>%
  mutate(AR = seqQuits(Choice)) %>%
  plyr::dlply("SubjID", identity) %>%
  lapply(function(data) {glm(Choice ~ Handling + Reward + AR, data = data, family = "binomial")}) 
  
# Rank sums
rankAR <- logisRHAR %>% 
  sapply(coefficients) %>%
  wilcox.test(.["AR", ])

AR_CIs <- sapply(logisRHAR, function(x) {tryCatch(confint(x)[4, ], error = function(e){c(0, 0)})}) %>%
  replace_na(0) %>%
  t() %>% 
  apply(1, function(x) !(x[1] <= 0 & x[2] >= 0))

# Around 10% seemed to have been influenced by previous quitting
prcntAR_effects <- mean(AR_CIs) * 100
```

To measure the significance of each subject's autoregressive coefficient, we computed its CI and checked how many contained zero. By this measure, around `r round(prcntAR_effects)` percent of our participants seemed to have been influenced by recent quitting. However, the autoregressive predictor did not preclude the effect of the remaininig experimental parameters.

\bigskip

  **16.1.3.**	*A general linear model with constant, linear, and quadratic terms will be used to estimate the correspondence between proportion accepted (independent variable) and total earnings (dependent variable). No other covariates will be used, as this analysis is to confirm that over and under accepting are detrimental to total earnings. The quadratic term will be defined as the squared deviation from the optimal overall acceptance rate.*
  
``` {r 16.1.3. Earnings, echo=FALSE,fig.align="center",fig.width=5,fig.height=4}
# calculate the proportions and earnings per subject, while keeping group info
temp <- dataAll %>%
  filter(Choice < 2) %>%
  group_by(SubjID, Cost) %>%
  summarise(Earnings = sum(Reward[Choice == 1] / 100),
            pComplete = mean(Choice)) %>%
  mutate(pComplete2 = pComplete^2) %>%
  ungroup()

# linear fit of earnings x prop accept
lmEarn <- lm(Earnings ~ pComplete + pComplete2, data = temp)

# plot
(earnFitplot <- ggplot(data = temp, aes(pComplete, Earnings)) + 
         scale_color_manual(values = cols) + 
         geom_point(aes(fill = Cost), pch = 21, color = "black", size = 3, show.legend = T) + 
         stat_smooth(aes(y = Earnings), method = "lm", formula = y ~ x + I(x^2), color = "grey30") +
         labs(x = "Proportion Accepted", y = "Total Earnings (dollars)") +
         scale_fill_manual(values = cols) +
         theme(panel.grid.major = element_blank(), 
           panel.grid.minor = element_blank(), 
           panel.background = element_blank(), 
           axis.line = element_line(colour = "black"),
           text = element_text(size = 16)))

```

The figure shows that participants that over and unceraccepted earned less money overall, as predicted. This is supported by a significant quadratic term from the linear model (F = `r round(summaryEarn$fstatistic[1], digits = 2)`, Beta = `r summaryEarn$coefficients[3,1]`, SE = `r round(summaryEarn$coefficients[3,2], digits = 2)`, R-squared = `r round(summaryEarn$r.squared, digits = 2)`; black line shows the fit). The following table shows all the results from the model.

``` {r 16.1.3. Earnings table, echo=FALSE,fig.align="center",fig.width=9,fig.height=5}
# Summary table for the linear model
panderOptions("digits", 2)
pander(lmEarn, style = "rmarkdown")

```  
  
\bigskip
  
  **16.1.4.**	*To determine the optimality of each group’s decisions, we will perform two-sided one-sample t-tests (with mu being the optimal proporion of acceptances--either 0 or 1) to see if the proportion of acceptances for each time/reward combination was significantly different from the optimal rate (see 11.2.). This will result in 36 independent tests (3 rewards amounts, 3 handling/travel time combinations, and 4 groups), so we will correct for multiple comparisons using False Discovery Rate (FDR).*

``` {r 16.1.4., echo=FALSE}
# in order
# clean data, 
# calculate each individual's prop. accept,
# calculate t.tests with optimality as null mean,
# correct pvalues with FDR
temp <- dataAll %>%
  filter(Choice < 2) %>%
  group_by(SubjID, Handling, Reward, Cost) %>%
  summarise(pComplete = mean(Choice)) %>%
  mutate(optimal = case_when(
          (Handling == 10 & Reward < 8) ~ 0,
          (Handling == 14 & Reward < 20) ~ 0,
          TRUE ~ 1
        )) %>% 
  group_by(Handling, Reward, Cost) %>%
  summarise(meanAccept = mean(pComplete),
            sdAccept = sd(pComplete),
            optimal = unique(optimal),
            pvals = tryCatch(t.test(pComplete, mu = unique(optimal))$p.value, error = function(e) {1})) %>%
  mutate(FDR = p.adjust(pvals, method = "BY"),
         significant = FDR < 0.05)


# proportion of significant ones
# eventually find a way to plot this
splitData_prop <- mean(temp$significant, na.rm = T)
```

Of the 36 tests, around `r splitData_prop` were significantly deviant from optimality. Most of these were from effortful groups, as the wait group just showed deviations for 2 seconds handling/4 cent, and 10 seconds handling/8 cent offers (note to self: think about a way to properly visualize these).

\bigskip

#### 16.2.	Comparisons among the four delay and effort conditions.

  **16.2.1.**	*To compare preferences (hypothesis 4.2.), we will first perform a one-way ANOVA on the proportion of trials accepted using group as a factor. In addition, we will do pairwise comparisons on the proportion completed among all 4 groups using non-parametric permutation contrasts (6 tests). The same approach will be used for total earnings. This will give us an initial glimpse on the potential differences in cost among conditions.*

``` {r 16.2.1. Overall Proportion/Earnings,fig.align="center",fig.width=10,fig.height=4, echo=FALSE}  
## prop complete
# ecdfs
wait <- ecdf(summaryData$propComplete[indxWait])
cog <- ecdf(summaryData$propComplete[indxCog])
phys <- ecdf(summaryData$propComplete[indxPhys])
easy <- ecdf(summaryData$propComplete[indxEasy])
range <- seq(0,1,by=0.01)

test <- data.frame(range = range,
                   ecdf = c(wait(range),cog(range),phys(range),easy(range)),
                   Group = factor(c(rep("Wait",101),rep("Cognitive",101),rep("Physical",101),rep("Easy", 101)), levels = c("Wait","Cognitive","Physical","Easy")))

p1 <- ggplot(test, aes(x=range,y=ecdf, group=Group, color=Group))
p1 <- p1 + geom_line(show.legend = F, size = 1.5) +
    scale_color_manual(values = cols) +
    labs(title = "ECDF of Acceptances per Group", y="ECDF", x = "Proportion Accepted") +
    theme_classic()

# problem is, data is not normal, so we're going to bootstrap it and DEFINITELY permute
# Bootstrap nonparametric
B <- 5000
meanBootW <- bootstrap(summaryData$propComplete[indxWait],B = B) #rep(0,B)
meanBootC <- bootstrap(summaryData$propComplete[indxCog], B = B) #rep(0,B)
meanBootP <- bootstrap(summaryData$propComplete[indxPhys], B = B)
meanBootE <- bootstrap(summaryData$propComplete[indxEasy], B = B)

#CIs wait
meanW <- mean(meanBootW)
upperW <- quantile(meanBootW,0.975) # meanBoot?
lowerW <- quantile(meanBootW,0.025)

#CIs effort
meanC <- mean(meanBootC)
upperC <- quantile(meanBootC,0.975) # meanBoot?
lowerC <- quantile(meanBootC,0.025)

#CIs physical
meanP <- mean(meanBootP)
upperP <- quantile(meanBootP,0.975) # meanBoot?
lowerP <- quantile(meanBootP,0.025)

#CIs easy
meanE <- mean(meanBootE)
upperE <- quantile(meanBootE,0.975) # meanBoot?
lowerE <- quantile(meanBootE,0.025)

# formal testing 
# proportion completed
propAov <- aov(propComplete ~ factor(Group), data = summaryData)

# effect size (get the one for anovas)
# propRsq <- cohenD(summaryData$propComplete[indxWait],summaryData$propComplete[indxCog])

# post-hoc
temp <- do.call(cbind, lapply(split(summaryData, summaryData$Group),"[[","propComplete"))
temp2 <- t(combn(colnames(temp),2))

propTests <- data.frame(Comparison = paste(temp2[,1], temp2[,2]),
                        T_pval = rep(1,6),
                        CohenD = rep(0,6))
                        
for (i in seq(nrow(temp2))) {
  
  propTests$T_pval[i] <- round(permute(temp[,temp2[i,1]], temp[,temp2[i,2]])$Pval, digits = 2)
  propTests$CohenD[i] <- round(cohenD(temp[,temp2[i,1]], temp[,temp2[i,2]]), digits = 2)
  
}

# earnings
earnAov <- aov(Earnings ~ factor(Group), data = summaryData)

# effect size
# earnRsq <- cohenD(summaryData$propComplete[indxWait],summaryData$propComplete[indxCog])

# post-hoc
temp <- do.call(cbind, lapply(split(summaryData, summaryData$Group),"[[","Earnings"))
temp2 <- t(combn(colnames(temp),2))

earnTests <- data.frame(Comparison = paste(temp2[,1], temp2[,2]),
                        T_pval = rep(1,6),
                        CohenD = rep(0,6))
                        
for (i in seq(nrow(temp2))) {
  
  earnTests$T_pval[i] <- round(permute(temp[,temp2[i,1]], temp[,temp2[i,2]])$Pval, digits = 2)
  earnTests$CohenD[i] <- round(cohenD(temp[,temp2[i,1]], temp[,temp2[i,2]]), digits = 2)
  
}


rm(B,cog,wait,phys,easytemp,temp2)

# plot
p2 <- ggplot(summaryData, aes(x=Group, y=propComplete, fill = Group))
p2 <- p2 + geom_boxplot(show.legend = F) +
    scale_fill_manual(values = cols) +
    ylim(c(0,1)) +
    labs(title = "Proportion Completed by Group", y="Proportion Completed") +
    theme_classic()


p3 <- ggplot(summaryData, aes(x=Group, y=Earnings, fill = Group))
p3 <- p3 + geom_boxplot(show.legend = F) +
    scale_fill_manual(values = cols) +
    labs(title = "Earnings by Group", y="Total Earnings (Cents)") +
    theme_classic()

grid.arrange(p1, p2, p3, ncol = 3)
```

The plots show that subjects in the wait condition accepted the least and earned the most, suggesting a more optimal pattern of choices. Participants in the cognitive effort and easy conditions had higher more variable acceptance patterns, which are reflected in the comparatively low earnings. The reason why the easy condition does not show variable earnings like the cognitive effort group is probably due to the quadratic relationship between earnings and acceptances shown before.

\bigskip

  **16.2.2.**	*In order to further look at the effect of delay, work, and rewards, we will perform a repeated measures ANOVA on the proportion completed for each combination of factors. Reward and handling time will be within-subject factors, and condition a between-subjects factor. In support of hypotheses 4.1. and 4.2., we anticipate significant main effects of handling time, reward, and cost condition, but no interactions.*
  
``` {r 16.2.2. rmANOVA, echo=FALSE,fig.align="center",fig.width=5,fig.height=4, results = 'asis'}
## REPEATED MEASURES ANOVA
# I think I like this the most
prop.aov <- with(allProp2, aov(propComplete ~ factor(Group) * Handling * Reward + 
                                 Error(SubjID / (Handling * Reward))))

#prop.aov <- with(allProp2, aov(propComplete ~ factor(Group) * Handling * Reward + 
 #                                Error(SubjID / (Handling * Reward))))

## NOTES ON RESULTS
# According to the logistic fit, there are main effects for all predictors, but no interactions. It also says that the overall odds for the wait group to accept versus the effort is around 16%. Similarly, increasing the handling time results in a decrease in the multiplicative odds of ~25%. Increments in reward increase the odds of acceptance by ~40%.
```

The plot below shows the mean proportion of acceptances (± SEM) per combination of handling time, reward, and condition (optimal acceptance rate indicated by the gray triangles). Visually, the graph confirms a couple of important intuitions. First, as the handling time increases, the proportion of acceptances decreases. Second, this discounting did not affect the 20 cent offers, as is expected from the present foraging environment. Lastly, and similar to the previous point, as the value of the offers increases so does the willingness to accept a trial. Beyond these general features, it is clear that the cognitive effort group accepted more than the other groups, regardless of the combination of experimental parameters. Notably, the optimality of this greater acceptance rate is determined by the timing context (e.g. optimal at 2 second handling time, but detrimental at 14 seconds).      

``` {r 16.2.2. rmANOVA plot, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
### THE PLOTS FOR YALE
# Ggplot made plotting way easier than the previous version
# This is in case you don't want to plot the easy condition (easier to see)
cols = c("#78AB05","#D9541A","deepskyblue4", "grey50")
trueProp2 <- trueProp[-(28:36), ]

a1 <- ggplot(data = trueProp, aes(x=interaction(Reward, Handling), y=propComplete, color=Group)) + 
        geom_point(size=3) + 
        geom_errorbar(aes(ymin=propComplete-SE, ymax=propComplete+SE), width=0.2, size=1) +
        geom_line(aes(group = interaction(Handling, Group)), size=1) +
        geom_point(aes(y=round(Optimal)), shape = 24, fill = "grey75", color = "grey30", size = 3, show.legend = F) +
        labs(x = "Reward.Handling", y = "Proportion Accepted", title = "Proportion Accepted per Handling and Reward Combination") +
        scale_color_manual(values=cols) +
        theme_classic()

a1
```

These intuitions were formally tested using a repeated measures ANOVA, whose results are presented in the table below. Overall, the analysis partially confirmed our predictions. While all main effects were significant, there was an unexpected significant condition-by-reward interaction, which could be due to the performance of the "easy" group relative to their peers. Importantly, we found that the interaction between all three main parameters was not significant, thus suggesting that the effects of handling and reward on choices were not different across groups.

``` {r 16.2.2. rmANOVA table, pander}
# anova table
x <- summary(prop.aov)   
x <- x$`Error: Within`
panderOptions("digits", 2)
pander(x, style = "rmarkdown",split.table = 110)
```

\bigskip

  **16.2.3.**	*We will compute the probability of accepting a trial with a mixed-effects logistic regression. Based on the task structure and our main question, our a priori model of interest will include cost condition, handling time, and reward amount as fixed main effects, and subject ID as a random effect. Cost condition will be modeled with three categorical terms, with the fourth condition as the reference condition. We will run three versions of the model with different reference conditions, in order to test all pairwise differences among the four cost conditions. As with 16.2.2., we anticipate significant main effects (coefficients different than zero) of handling time, reward, and cost condition. We hypothesize that the differences among cost conditions will follow the pattern described in 4.2.*

``` {r 16.2.3. Mixed Effects Model, echo=FALSE,fig.align="center",fig.width=5,fig.height=4}
# just to make the logistic plotting comply with the function
allProp2 <- allProp2[order(as.character(allProp2$Group), decreasing = F), ]

# This transformation is necessary for a proper binomial fit of proportion data
totalQuit <- allProp2$totalTrials - allProp2$totalComplete
testBinom <- cbind(allProp2$totalComplete,totalQuit)

# Using a mixed-model fitted with GLMER
# Note: GLMER does not take quasi family distributions
# ranef() to display the random intercepts, fixef() for the model coefficients

# fit model
mixLogis_main <- list()
allProp2 <- within(allProp2, Group <- relevel(Group, ref = "Wait"))
mixLogis_main$Wait <-  glmer(testBinom ~ Group + Handling + Reward + (1 | SubjID), family = "binomial", data = allProp2)
allProp2 <- within(allProp2, Group <- relevel(Group, ref = "Cognitive"))
mixLogis_main$Cognitive <-  glmer(testBinom ~ Group + Handling + Reward + (1 | SubjID), family = "binomial", data = allProp2)
allProp2 <- within(allProp2, Group <- relevel(Group, ref = "Physical"))
mixLogis_main$Physical <-  glmer(testBinom ~ Group + Handling + Reward + (1 | SubjID), family = "binomial", data = allProp2)
allProp2 <- within(allProp2, Group <- relevel(Group, ref = "Easy"))
mixLogis_main$Easy <-  glmer(testBinom ~ Group + Handling + Reward + (1 | SubjID), family = "binomial", data = allProp2)

# Summarize model 
mixLogis_main$Summary <- summary(mixLogis_main$Wait)

# Coefficient matrices for all reference combos
t1 <- c(summary(mixLogis_main$Wait)$coefficients[2:4,1], GroupWait = 0)
t2 <- c(summary(mixLogis_main$Cognitive)$coefficients[2:4,1], GroupCognitive = 0)
t3 <- c(summary(mixLogis_main$Physical)$coefficients[2:4,1], GroupPhysical = 0)
t4 <- c(summary(mixLogis_main$Easy)$coefficients[2:4,1], GroupEasy = 0)
mixLogis_mainBetas <- cbind(merge(t1,t2, by = 0), merge(t1,t3,by=0)[,3], merge(t1,t4,by=0)[,3])
dimnames(mixLogis_mainBetas) <- list(cond[c(2,4,3,1)],c("Group",cond))
mixLogis_mainBetas <- as.matrix(mixLogis_mainBetas[,c(3,5,4,2)])

# And their respective p-values
t1 <- c(summary(mixLogis_main$Wait)$coefficients[2:4,4], GroupWait = 1)
t2 <- c(summary(mixLogis_main$Cognitive)$coefficients[2:4,4], GroupCognitive = 1)
t3 <- c(summary(mixLogis_main$Physical)$coefficients[2:4,4], GroupPhysical = 1)
t4 <- c(summary(mixLogis_main$Easy)$coefficients[2:4,4], GroupEasy = 1)
mixLogis_mainPvals <- cbind(merge(t1,t2, by = 0), merge(t1,t3,by=0)[,3], merge(t1,t4,by=0)[,3])
dimnames(mixLogis_mainPvals) <- list(cond[c(2,4,3,1)],c("Group",cond))
mixLogis_mainPvals <- as.matrix(mixLogis_mainPvals[,c(3,5,4,2)])

# Create CIs
sd <- sqrt(diag(vcov(mixLogis_main$Wait)))
mixLogisCI <- cbind(Est = fixef(mixLogis_main$Wait), LL = fixef(mixLogis_main$Wait) - 1.96 * sd, UL = fixef(mixLogis_main$Wait) + 1.96 * sd)
```

The plot below shows the distribution of random intercepts across participants of each group. Participants in the easy condition show much more variability than the other groups, potentially suggesting a bias in choices in this condition. Nonetheless, the intercepts per group are centered around zero.

``` {r 16.2.3. Mixed Effects Model plot rand intercepts, echo=FALSE,fig.align="center",fig.width=5,fig.height=4}
# plot random intercepts in increasing order
#RI1 <- qplot(seq(83), sort(ranef(mixLogis_main$Wait)$SubjID[,1]), color = cols[1], show.legend = F) + theme_classic()

# plot per subject (as expected, the intercepts are highest for those who accepted everythiing)
temp <- cbind(summaryData$Group, 
              ranef(mixLogis_main$Wait)$SubjID)#, 
              #Colors = c(rep(cols[1],nSubsW),rep(cols[2],nSubsC),rep(cols[3],nSubsP),rep(cols[4],nSubsE)))
colnames(temp) <- list("Group", "RandIntercept")
RI1 <- ggplot(data = temp, aes(x = Group, y = RandIntercept, fill = Group)) + 
  geom_hline(yintercept = 0, alpha = 0.7) + 
  geom_boxplot(show.legend = F) +
  scale_fill_manual(values = cols) + 
  labs(title = "Random Intercepts per Group", x = "") +
  theme_classic() 


RI1
```

As predicted, the model showed significant main effects of handling time (Beta = `r mixLogis_main$Summary$coefficients[5,1]`, SE = `r mixLogis_main$Summary$coefficients[5,2]`, p < 0.001) and reward (Beta = `r mixLogis_main$Summary$coefficients[6,1]`, SE = `r mixLogis_main$Summary$coefficients[6,2]`, p < 0.001). In order to show the comparisons among all conditions, the following plot portrays the coefficients (color scale), and p-values (numbers within squares) that resulted from switching the reference condition. Specifically, each entry shows how much more likely was the reference group to accept an offer than the comparison group (the darker the cell, the greater the odds). Based on this, we can see that the cognitive group was significantly more likely to accept any offer than the physical and wait groups, but not the easy group.

``` {r 16.2.3. Mixed Effects Model plot group coefficients, echo=FALSE,fig.align="center",fig.width=5,fig.height=5}
# plot coefficient matrix (show pvals!)
corrplot(abs(mixLogis_mainBetas), 
         p.mat = mixLogis_mainPvals, 
         is.corr = F, 
         method = "color", 
         tl.col = "black", 
         tl.srt = 0, 
         insig = "p-value", 
         sig.level = -1, 
         type = "lower",
         cl.length = 3)
mtext("Comparison", side = 2, line = 1.5, cex = 1.5)
mtext("Reference Group", side = 3, line = 1, cex = 1.5, at = 2.5)

rm(temp,t1,t2,t3,t4)

```

``` {r 16.2.3. Plot logistic model, echo=FALSE,fig.align="center",fig.width=6,fig.height=6}
# par(mfrow=c(1,1))
# ## Plotting the winning model (colors are messed up)
# rewards <- seq(1,20,length = 1000)
# # wait
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Wait",
#           handling = 2, 
#           reward = rewards, 
#           plotType = plot, 
#           color = cols[1],
#           lType = 6)
# 
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Wait",          
#           handling = 10, 
#           reward = rewards, 
#           color = cols[1],
#           lType = 3)
# 
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Wait",          
#           handling = 14, 
#           reward = rewards, 
#           color = cols[1])
# 
# 
# # cognitive
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Cognitive",
#           handling = 2, 
#           reward = rewards, 
#           color = cols[2],
#           lType = 6)
# 
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Cognitive",
#           handling = 10, 
#           reward = rewards, 
#           color = cols[2],
#           lType = 3)
# 
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Cognitive",
#           handling = 14, 
#           reward = rewards, 
#           color = cols[2])
# 
# 
# # physical
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Phys",
#           handling = 2, 
#           reward = rewards, 
#           color = cols[3],
#           lType = 6)
# 
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Phys",
#           handling = 10, 
#           reward = rewards, 
#           color = cols[3],
#           lType = 3)
# 
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Phys",
#           handling = 14, 
#           reward = rewards, 
#           color = cols[3])
# 
# # easy
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Easy",
#           handling = 2, 
#           reward = rewards, 
#           color = cols[4],
#           lType = 6)
# 
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Easy",
#           handling = 10, 
#           reward = rewards, 
#           color = cols[4],
#           lType = 3)
# 
# plotLogis(coeffs = logisComb$coefficients,
#           group = "Easy",
#           handling = 14, 
#           reward = rewards, 
#           color = cols[4])
# 
# # title
# title("Probabilities for All Handling-Reward Combinations \n for Each Group")
# 
# # x axis
# xticks <- c(0,min(rewards[rewards>4]),min(rewards[rewards>8]),20)
# axis(1,at = xticks,c(0,4,8,20))
# 
# # legend
# legend("bottomright",
#        c("Wait: 2s","Wait: 10s","Wait: 14s",
#          "Cognitive: 2s","Cognitive: 10s","Cognitive: 14s",
#          "Physical: 2s","Physical: 10s","Physical: 14s",
#          "Easy: 2s","Easy: 10s","Easy: 14s"), 
#        fill = c(cols[1],cols[1],cols[1],cols[2],cols[2],cols[2],cols[3],cols[3],cols[3],cols[4],cols[4],cols[4]),
#        lty = rep(c(6,3,1,8),3),
#        cex = 0.8)
# 
# 
# # actual proportions (maybe, maybe not)
# # points(c(5,10,25),trueProp$propComplete[1:3], col = cols[1], pch=17) # wait
# # points(c(5,10,25),trueProp$propComplete[4:6], col = cols[1], pch=17)
# # points(c(5,10,25),trueProp$propComplete[7:9], col = cols[1], pch=17)
# # 
# # points(c(5,10,25),trueProp$propComplete[19:21], col = cols[2], pch=17) # cog effort
# # points(c(5,10,25),trueProp$propComplete[22:24], col = cols[2], pch=17)
# # points(c(5,10,25),trueProp$propComplete[25:27], col = cols[2], pch=17)
# # 
# # points(c(5,10,25),trueProp$propComplete[10:12], col = cols[3], pch=17) # phys effort
# # points(c(5,10,25),trueProp$propComplete[13:15], col = cols[3], pch=17)
# # points(c(5,10,25),trueProp$propComplete[16:18], col = cols[3], pch=17)
# 
# # and the residuals
# # plot(logisComb$residuals,
# #      main = "Residuals for the Logistic Regression",
# #      xlab = "trial",
# #      ylab = "Residuals")
# # 
```  

  **16.2.4.**	*Next, we will examine whether the a priori model from 16.2.3. outperforms both simpler and more complex models. Unlike the individual logistic regression fits in 16.1.1., a mixed-effects approach gives us a better goodness of fit measure for model comparisons. We will determine the best model (combination of predictors) using Akaike’s Information Criterion (AIC) to determine the model that minimizes the negative log-likelihood while penalizing the addition of parameters. The regression with each combination of predictors will be fitted in the following order: 1) intercept only; 2) condition only; 3) handling time only; 4) reward only; 5) condition, handling, and reward main effects (from 16.2.3.); 6) adding a handling-by-reward interaction; and 7) adding all three possible two-way interactions. We predict that model 5 will have the lowest AIC.*

This plot shows the AIC for every model. The last model to make significant fit contributions under the parameter penalty was indeed number 5. 

``` {r 16.2.4. Model comparison, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# Quasibinomial GLM versions (with proper proportion setup)
mixLogis_compare <- list()
mixLogis_compare$Base <- glmer(testBinom ~ 1 + (1 | SubjID), family = "binomial", data = allProp2) # intercept only
mixLogis_compare$Group <- glmer(testBinom ~ factor(Group) + (1 | SubjID), family = "binomial", data = allProp2) 
mixLogis_compare$Handling <- glmer(testBinom ~ factor(Group) + Handling + (1 | SubjID), family = "binomial", data = allProp2)
mixLogis_compare$Reward <- glmer(testBinom ~ factor(Group) + Reward + (1 | SubjID), family = "binomial", data = allProp2)
mixLogis_compare$AllMain <- mixLogis_main$Easy
mixLogis_compare$HRInteraction <- glmer(testBinom ~ factor(Group) + Handling * Reward + (1 | SubjID), family = "binomial", data = allProp2)
mixLogis_compare$AllInteractions <- glmer(testBinom ~ factor(Group) * Handling * Reward + (1 | SubjID), family = "binomial", data = allProp2)

# Model selection 
aicLogis_compare <- sapply(mixLogis_compare, AIC)

# effect size
mixLogis_compare$Rsq <- 1 - (deviance(mixLogis_compare$AllMain)/deviance(mixLogis_compare$Base)) # denominator is the null deviance
```

``` {r 16.2.4. Plot Deviance comparison, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
    
par(mar=c(5.1,2.1,2.1,2.1))    
# plot the deviance for all models
# ver 3
modelLabels <- c('Intercept','G Only','H Only','R Only','G + H + R','G + H * R','G * H * R')
plot(aicLogis_compare,
     main = "AIC for All Models",
     xlab = "",
     ylab = "AIC",
     pch = 16,
     xaxt = "n")
axis(1, at = 1:7, labels = F)
text(1:7, 
     par("usr")[3] - 22,
     labels = modelLabels, 
     srt = 45, 
     pos = 1, 
     xpd = TRUE)
    
```


\bigskip

#### 16.3.	Modeling the subjective opportunity cost in each condition. 

  **16.3.1.**	*Response times (RT) for quit responses will be presented in a descriptive manner in order to examine whether participants tended to quit early or late within individual trials. Each cost group’s response time distribution will contain the pooled RT across its corresponding participants, and we will display the empirical cumulative distribution functions for each condition. Short RT would suggest confident and stable decisions (in support of 4.3.1.).*
  
We can see in the ECDFs below that most decisions to quit happened within the first second into the handling time. The reason why the physical and easy condition show slightly lagged responses is that the experiment allowed a one second grace period for participants to begin gripping. This results in some of them choosing to wait for that second to indicate an offer rejection.

``` {r 16.3.1. RTs, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# RTs
# let's try to do some survival curves
RTW <- unlist(lapply(waitData, function(data) {data$RT[data$Choice==0]})) # vector of all decision-related RTs for the wait group
RTC <- unlist(lapply(cogData, function(data) {data$`Choice RT`[data$Choice==0]})) # same for the cog group
RTP <- unlist(lapply(physData, function(data) {data$RT[data$Choice==0]}))
RTE <- unlist(lapply(easyData, function(data) {data$RT[data$Choice==0]}))

# create a data frame for plotting
RTgroups <- c(rep("Wait",length(RTW)), rep("Effort",length(RTC)), rep("Physical",length(RTP)), rep("Easy",length(RTE)))
RTall <- c(RTW,RTC,RTP,RTE)
RTframe <- data.frame(RTgroups, RTall)

# plot
ggplot(RTframe, aes(RTall, color = RTgroups)) + 
  stat_ecdf(lwd = 1) + 
  scale_color_manual(values = cols) + 
  scale_x_continuous(breaks = seq(14)) +
  labs(title = "ECDFs for the Response Times", x = "Seconds", y = "ECDF") +
  theme_classic()

# # and CIs for each (unnecessary unless I end up plotting)
# CIC <- bootstrap(group = RTC, statType = median)  #qt(.975,nSubs-1)*(sd(RTC)/sqrt(nSubs))
# CIC <- median(CIC) - (quantile(CIC,0.025))
# CIW <- bootstrap(group = RTW, statType = median)  #qt(.975,nSubs-1)*(sd(RTW)/sqrt(nSubs))
# CIW <- median(CIW) - (quantile(CIW,0.025))
# CIP <- bootstrap(group = RTP, statType = median)  #qt(.975,nSubs-1)*(sd(RTW)/sqrt(nSubs))
# CIP <- median(CIP) - (quantile(CIP,0.025))
# CIE <- bootstrap(group = RTE, statType = median)  #qt(.975,nSubs-1)*(sd(RTW)/sqrt(nSubs))
# CIE <- median(CIE) - (quantile(CIE,0.025))
# lines(range,ecdfC(range) + CIC,col = cols[2], lty = 2, lwd = lthick)
# lines(range,ecdfC(range) - CIC,col = cols[2], lty = 2, lwd = lthick) 
# lines(range,ecdfW(range) + CIW,col = cols[1], lty = 2, lwd = lthick)
# lines(range,ecdfW(range) - CIW,col = cols[1], lty = 2, lwd = lthick)
# lines(range,ecdfP(range) + CIP,col = cols[3], lty = 2, lwd = lthick)
# lines(range,ecdfP(range) - CIP,col = cols[3], lty = 2, lwd = lthick)
# lines(range,ecdfE(range) + CIE,col = cols[4], lty = 2, lwd = lthick)
# lines(range,ecdfE(range) - CIE,col = cols[4], lty = 2, lwd = lthick)

```

\bigskip

  **16.3.2.**	*In order to further examine choice stability (hypothesis 4.3.1.), we will compute each participant’s total proportion of acceptances pre- and post-midpoint. For each cost condition separately, we will fit a linear model that predicts post-midpoint acceptance from before-midpoint rates. We will report the slopes and 95% confidence intervals (CI) for each cost group. CIs containing 1 will denote that participants in that group produced consistent choices.*
  
``` {r 16.3.2. Pre/Post Break, echo=FALSE,fig.align="center",fig.width=5,fig.height=5}
# THIS CODE CAN BE VASTLY IMPROVED, BUT RIGHT NOW IT WORKS SO I WON'T CARE
# This one just plots the correlation between pre-post acceptance rates, both overall and per cell
# The overall acceptance looks great, with mostly consistent rates, and with the physical group accepting slightly less post-break
# However, the per-cell distribution looks messy, although you can still tell that there's a linear relationship
# If I do a linear model predicting post with pre using cell-specific data, the betas are close to 1 and significant (also good R2s). That's good.

# create vdata frames that store acceptance data per combination cell pre-post break
waitB <- data.frame()
waitA <- data.frame()
cogB <- data.frame()
cogA <- data.frame()
physB <- data.frame()
physA <- data.frame()
easyB <- data.frame()
easyA <- data.frame()

# and for overall proportion accepted
waitBAll <- numeric()
waitAAll <- numeric()
cogBAll <- numeric()
cogAAll <- numeric()
physBAll <- numeric()
physAAll <- numeric() 
easyBAll <- numeric()
easyAAll <- numeric()


for (i in seq(nSubsW)){
  
  # wait
  tempPropB <- aggregate(waitBefore[[i]]$Choice, by = list(waitBefore[[i]]$Handling, waitBefore[[i]]$Reward), FUN = 'mean')
  tempPropA <- aggregate(waitAfter[[i]]$Choice, by = list(waitAfter[[i]]$Handling, waitAfter[[i]]$Reward), FUN = 'mean')
  waitB <- rbind(waitB,tempPropB)
  waitA <- rbind(waitA,tempPropA)
  waitBAll[i] <- mean(waitBefore[[i]]$Choice) 
  waitAAll[i] <- mean(waitAfter[[i]]$Choice) 
  
}

for (i in seq(nSubsC)){
  
  # cognitive
  tempPropB <- aggregate(cogBefore[[i]]$Choice, by = list(cogBefore[[i]]$Handling, cogBefore[[i]]$Reward), FUN = 'mean')
  tempPropA <- aggregate(cogAfter[[i]]$Choice, by = list(cogAfter[[i]]$Handling, cogAfter[[i]]$Reward), FUN = 'mean')
  cogB <- rbind(cogB,tempPropB)
  cogA <- rbind(cogA,tempPropA)
  cogBAll[i] <- mean(cogBefore[[i]]$Choice) 
  cogAAll[i] <- mean(cogAfter[[i]]$Choice)   
  
}

for (i in seq(nSubsP)){
  
  # physical
  tempPropB <- aggregate(physBefore[[i]]$Choice, by = list(physBefore[[i]]$Handling, physBefore[[i]]$Reward), FUN = 'mean')
  tempPropA <- aggregate(physAfter[[i]]$Choice, by = list(physAfter[[i]]$Handling, physAfter[[i]]$Reward), FUN = 'mean')
  physB <- rbind(physB,tempPropB)
  physA <- rbind(physA,tempPropA)
  physBAll[i] <- mean(physBefore[[i]]$Choice) 
  physAAll[i] <- mean(physAfter[[i]]$Choice) 
  
}

for (i in seq(nSubsE)){
  
  # easy
  tempPropB <- aggregate(easyBefore[[i]]$Choice, by = list(easyBefore[[i]]$Handling, easyBefore[[i]]$Reward), FUN = 'mean')
  tempPropA <- aggregate(easyAfter[[i]]$Choice, by = list(easyAfter[[i]]$Handling, easyAfter[[i]]$Reward), FUN = 'mean')
  easyB <- rbind(easyB,tempPropB)
  easyA <- rbind(easyA,tempPropA)
  easyBAll[i] <- mean(easyBefore[[i]]$Choice) 
  easyAAll[i] <- mean(easyAfter[[i]]$Choice) 
  
}

# Pre-post linear models
prepostLM <- list()
prepostLM$Wait <- lm(waitBAll ~ waitAAll)
prepostLM$Cognitive <- lm(cogBAll ~ cogAAll)
prepostLM$Physical <- lm(physBAll ~ physAAll)
prepostLM$Easy <- lm(easyBAll ~ easyAAll)

prepostSummary <- do.call(rbind, lapply(prepostLM, function(data) c(coef(data)[2], confint(data)[2,])))

```

The plot below shows that acceptance rates before and after the mid-point were mostly consistent, although it was more likely for participants to accept less in the second half of the experiment.

``` {r 16.3.2. Plot Pre/Post, echo=FALSE,fig.align="center",fig.width=5,fig.height=5}

# overall proportion
plot(waitBAll,
     waitAAll,
     xlim = c(0,1),
     ylim = c(0,1),
     main = "Proportion accepted pre- and post-midpoint",
     xlab = "Acceptance Rate Before",
     ylab = "Acceptance Rate After",     
     col = cols[1],
     pch = 16)
points(cogBAll,
       cogAAll, 
       col = cols[2],
       pch = 16)
points(physBAll,
       physAAll, 
       col = cols[3],
       pch = 16)
points(easyBAll,
       easyAAll, 
       col = cols[4],
       pch = 16)
lines(seq(0,1,length = 10),
      seq(0,1,length = 10))
legend("bottomright",
       lbls,
       fill = cols,
       cex = 0.8)

# dataAll %>%
#   filter(Choice < 2) %>%
#   group_by(SubjID) %>%
#   #mutate(Block = do.call(c, lapply(c(2,4,6), function(x) {rep(x/2, sum(Block %in% c(x-1, x)))}))) %>%
#   group_by(SubjID, Cost, Block) %>%
#   summarise(propAccept_subj = mean(Choice)) %>%
#   group_by(Cost, Block) %>%
#   summarize(propAccept = mean(propAccept_subj),
#             seAccept = sd(propAccept_subj)/sqrt(nSubjs)) %>%
#   ggplot(aes(Block, propAccept, fill = Cost)) +
#     geom_point(size = 2, pch = 21, color = "black") +
#     geom_errorbar(aes(ymin=propAccept - seAccept, ymax=propAccept + seAccept, color = Cost), width = 0.1) +
#     geom_line(aes(color = Cost)) +
#     ylim(0.5, 1) +
#     scale_x_continuous(breaks = seq(6)) +
#     scale_color_manual(values = cols) +
#     scale_fill_manual(values = cols) +
#     theme_classic()
```

The following table shows the coefficients and CI for each linear model, and show that pre-post acceptance rates were very similar for each condition (i.e. one is present in all confidence intervals).

| Condition   |            Beta           |          CI-Low           |          CI-High          |
| :---------- | :-----------------------: | ------------------------: | ------------------------: |
| Wait        | $`r prepostSummary[1,1]`$ | $`r prepostSummary[1,2]`$ | $`r prepostSummary[1,3]`$ |
| Cognitive   | $`r prepostSummary[2,1]`$ | $`r prepostSummary[2,2]`$ | $`r prepostSummary[2,3]`$ |
| Physical    | $`r prepostSummary[3,1]`$ | $`r prepostSummary[3,2]`$ | $`r prepostSummary[3,3]`$ |
| Easy        | $`r prepostSummary[4,1]`$ | $`r prepostSummary[4,2]`$ | $`r prepostSummary[4,3]`$ |
Table: Predicting Post-mid acceptance from pre-mid choices: Coefficients and CI.

\bigskip

  **16.3.3.**	*To estimate the subjective opportunity cost (hypothesis 4.3.2.), we will use a logistic function to model each participant’s probability of completing a trial based on the difference between the delayed reward’s magnitude and the estimated opportunity cost (OC) for each cost type. OC will be computed as the product of a free parameter (gamma) and the handling time. Both gamma and the temperature parameter of the logistic function will be estimated at the subject level, independently for each subject.*

The gamma optimization search space was bound by extreme choice values. For example, someone with an extremely high OC would not accept even the most beneficial offers, and would reject a 20 cent offer when the handling time was 2 seconds (or 20/2). Conversely, a participant experiencing low opportunity costs would accept the unbeneficial offer of 4 cents for a 14 second delay (or 4/14). The model was optimized to find the lowest value of gamma that significantly reduced the negative log likelihood. The resulting gamma values per participant are shown per group below (gray lines denote the rate of earnings under optimal behavior for all 3 timing contexts), and reflect what was seen in the previous sections: participants in the wait and physical conditions showed higher gamma values (and thus opportunity costs) than in the other two groups. Given the high variability of the easy condition, it might be worth modeling each timing context independently, or perhaps fitting the model on subgroups defined by a median split of the participants based on their acceptance rate.

``` {r 16.3.3. OC modeling, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# Estimate gamma per subject for each group separately
waitOC <- sapply(waitData, function(Data) {optimizeOCModel(Data)$Gamma})
cogOC <- sapply(cogData, function(Data) {optimizeOCModel(Data)$Gamma})
physOC <- sapply(physData, function(Data) {optimizeOCModel(Data)$Gamma})
easyOC <- sapply(easyData, function(Data) {optimizeOCModel(Data)$Gamma})

# Summarize for plotting
summaryOC <- data.frame(Group = rep(cond, each = nSubsC),
                        Gamma = c(waitOC, cogOC, physOC, easyOC))
summaryOC$Group <- factor(summaryOC$Group, levels = cond, ordered = T)

# plot
ggplot(summaryOC, aes(Group, Gamma, fill = Group)) +
  geom_hline(yintercept = 0.64, alpha = 0.9, color = "gray20", size = 1) +
  geom_hline(yintercept = 0.7, alpha = 0.9, color = "gray40", size = 1) +
  geom_hline(yintercept = 0.9, alpha = 0.9, color = "gray60", size = 1) +
  geom_dotplot(stackdir = 'center', binaxis = 'y', show.legend = F) +
  ylim(0,1.5) +
  labs(title = "Gamma Values per Group", x = "") +
  scale_fill_manual(values = cols) +
  theme_classic()

```

\bigskip

  **16.3.4.**	*We will cross-validate each subject’s OC value using the pre-midpoint data for estimation, and post-midpoint choices for testing. The estimates will be used to predict acceptances in the testing sample, and the mean percent correctly predicted will be reported for each group. This will also provide information on the stability of each participant’s choices (4.3.1.).*
  
As can be seen below, the gamma parameter from the OC model was able to predict post-midpoint choices successfully regardless of group. 

``` {r 16.3.4. OC cross-validation, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# Compute the OC for pre-midpoint
waitOCbefore <- sapply(waitBefore, function(Data) {optimizeOCModel(Data)$Gamma})
cogOCbefore <- sapply(cogBefore, function(Data) {optimizeOCModel(Data)$Gamma})
physOCbefore <- sapply(physBefore, function(Data) {optimizeOCModel(Data)$Gamma})
easyOCbefore <- sapply(easyBefore, function(Data) {optimizeOCModel(Data)$Gamma})

# Percent correctly predicted by pre-midpoint gamma
# Wait
predictOCWait <- numeric()
for (i in seq(nSubsW)) {
  temp <- (waitAfter[[i]]$Reward > (waitOCbefore[i] * waitAfter[[i]]$Handling))
  temp[temp == TRUE] <- 1
  predictOCWait[i] <- (sum(temp==waitAfter[[i]]$Choice) / length(waitAfter[[i]]$Choice)) * 100 
}

# Cognitive
predictOCCog <- numeric()
for (i in seq(nSubsC)) {
  temp <- (cogAfter[[i]]$Reward > (cogOCbefore[i] * cogAfter[[i]]$Handling))
  temp[temp == TRUE] <- 1
  predictOCCog[i] <- (sum(temp==cogAfter[[i]]$Choice) / length(cogAfter[[i]]$Choice)) * 100 
}

# Physical
predictOCPhys <- numeric()
for (i in seq(nSubsW)) {
  temp <- (physAfter[[i]]$Reward > (physOCbefore[i] * physAfter[[i]]$Handling))
  temp[temp == TRUE] <- 1
  predictOCPhys[i] <- (sum(temp==physAfter[[i]]$Choice) / length(physAfter[[i]]$Choice)) * 100 
}

# Easy
predictOCEasy <- numeric()
for (i in seq(nSubsW)) {
  temp <- (easyAfter[[i]]$Reward > (easyOCbefore[i] * easyAfter[[i]]$Handling))
  temp[temp == TRUE] <- 1
  predictOCEasy[i] <- (sum(temp==easyAfter[[i]]$Choice) / length(easyAfter[[i]]$Choice)) * 100 
}


# Summarize for plotting
summaryOCPredict <- data.frame(Group = rep(cond, each = nSubsC),
                        percentPredicted = c(predictOCWait, predictOCCog, predictOCPhys, predictOCEasy))
summaryOCPredict$Group <- factor(summaryOCPredict$Group, levels = cond, ordered = T)

meanOCPredict <- aggregate(percentPredicted ~ Group, data = summaryOCPredict,FUN = 'mean')


# plot
ggplot(summaryOCPredict, aes(Group, percentPredicted, fill = Group)) +
  #geom_dotplot(stackdir = 'center', binaxis = 'y', show.legend = F) +
  labs(title = "Percent of Choices Correctly Predicted by Gamma per Group", x = "") +
  geom_boxplot(show.legend = F) +
  scale_fill_manual(values = cols) +
  labs(y = "Percent Predicted") +
  ylim(0,100) +
  theme_classic()
```

\bigskip

  **16.3.5.**	*The OC estimates for each group will be compared using an ANOVA with condition as a factor. This will let us determine which cost type produced the highest discounting.*

``` {r 16.3.5. OC comparisons, echo=FALSE,fig.align="center",fig.width=6,fig.height=4}
# ANOVA
anovaOC <- summary(aov(Gamma ~ Group, data = summaryOC))
Fval <- anovaOC[[1]]$`F value`[1]
Pval <- anovaOC[[1]]$`Pr(>F)`[1]
Rsquared_aov <- round(anovaOC[[1]]$`Sum Sq`[1] / sum(anovaOC[[1]]$`Sum Sq`), digits = 2)
  
# Post-hoc pairwise tests
permuteOCTest <- list()
permuteOCTest$WvC <- permute(waitOC, cogOC)
permuteOCTest$WvP <- permute(waitOC, physOC)
permuteOCTest$WvE <- permute(waitOC, easyOC)
permuteOCTest$CvP <- permute(cogOC, physOC)
permuteOCTest$CvE <- permute(cogOC, easyOC)
permuteOCTest$PvE <- permute(physOC, easyOC)
```

The analysis of variance was significant (F = `r round(Fval, digits = 2)`, p = `r ifelse(Pval < 0.001, "< 0.001", paste("=", round(Pval, digits = 2)))`, R-squared = `r Rsquared_aov`). Pairwise post-hoc permutations showed significantly greater values for the wait group versus the cognitive (p = `r ifelse(permuteOCTest$WvC$Pval < 0.001, "< 0.001", paste("=", round(permuteOCTest$WvC$Pval, digits = 2)))`) and easy (p = `r ifelse(permuteOCTest$WvE$Pval < 0.001, "< 0.001", paste("=", round(permuteOCTest$WvE$Pval, digits = 2)))`) groups, as well as significantly greater gammas for the physical group and the cognitive group (p = `r ifelse(permuteOCTest$CvP$Pval < 0.001, "< 0.001", paste("=", round(permuteOCTest$CvP$Pval, digits = 2)))`). These results suggest that participants engaged in cognitively effortful demands experience significantly lower opportunity costs than those whose demands involve physical effort and pure delay. The reason for this is unclear, and we will work on it in the near future. 

\bigskip
\bigskip
\bigskip
\bigskip
\bigskip

## Notes

- Update the AIC, RT ECDF, pre-post plots with ggplot.

- This plot is just to show participant-specific behavior for all groups.

``` {r Testing grounds, echo=FALSE,fig.align="center",fig.width=10,fig.height=8}
cond <- as.character(unique(summaryData$Group))

i <- 1
test <- allProp2[allProp2$Group == cond[i], ]
test2 <- trueProp[trueProp$Group == cond[i], ]
t1 <- ggplot(data = test, aes(x=interaction(Reward, Handling), y=propComplete)) + 
  geom_line(aes(group = interaction(Handling, SubjID)), size=1, color = cols[i], alpha = 0.5) +
  geom_point(size=3, color = cols[i]) +
  geom_point(data = test2, aes(group = interaction(Reward, Handling), y = propComplete), size = 3.5, color = "black", fill = "grey60", pch=24) +
  labs(title = paste("Acceptance rates for all subjects in the", cond[i], "group")) +
  theme_classic()

i <- 2
test <- allProp2[allProp2$Group == cond[i], ]
test2 <- trueProp[trueProp$Group == cond[i], ]
t2 <- ggplot(data = test, aes(x=interaction(Reward, Handling), y=propComplete)) + 
  geom_line(aes(group = interaction(Handling, SubjID)), size=1, color = cols[i], alpha = 0.5) +
  geom_point(size=3, color = cols[i]) +
  geom_point(data = test2, aes(group = interaction(Reward, Handling), y = propComplete), size = 3.5, color = "black", fill = "grey60", pch=24) +
  labs(title = paste("Acceptance rates for all subjects in the", cond[i], "group")) +
  theme_classic()

i <- 3
test <- allProp2[allProp2$Group == cond[i], ]
test2 <- trueProp[trueProp$Group == cond[i], ]
t3 <- ggplot(data = test, aes(x=interaction(Reward, Handling), y=propComplete)) + 
  geom_line(aes(group = interaction(Handling, SubjID)), size=1, color = cols[i], alpha = 0.5) +
  geom_point(size=3, color = cols[i]) +
  geom_point(data = test2, aes(group = interaction(Reward, Handling), y = propComplete), size = 3.5, color = "black", fill = "grey60", pch=24) +
  labs(title = paste("Acceptance rates for all subjects in the", cond[i], "group")) +
  theme_classic()

i <- 4
test <- allProp2[allProp2$Group == cond[i], ]
test2 <- trueProp[trueProp$Group == cond[i], ]
t4 <- ggplot(data = test, aes(x=interaction(Reward, Handling), y=propComplete)) + 
  geom_line(aes(group = interaction(Handling, SubjID)), size=1, color = cols[i], alpha = 0.5) +
  geom_point(size=3, color = cols[i]) +
  geom_point(data = test2, aes(group = interaction(Reward, Handling), y = propComplete), size = 3.5, color = "black", fill = "grey60", pch=24) +
  labs(title = paste("Acceptance rates for all subjects in the", cond[i], "group")) +
  theme_classic()


grid.arrange(t1,t3,t2,t4, ncol = 2)

```











































