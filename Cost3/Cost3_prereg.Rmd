---
title: "Preregistration analysis, Cost 3 study"
author: "Claudio Toro Serey & Joseph T. McGuire"
output:
  pdf_document: default
  html_document: default
---

This document follows the steps planned for the second pre-registration (https://osf.io/2rsgm/). The bullet points match the organization shown on the website as closely as possible. First, here is a restatement of the questions and hypotheses.

```{r Global Options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

```{r setup, include=FALSE}
# global options
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)

# aesthetic options
cols <- c("#D9541A", "#78AB05", "dodgerblue4", "deepskyblue3")#"grey30", "grey70") # plot colors (wait, effort)

library(tidyverse)
library(gridExtra)
library(lme4)
library(corrplot)
library(nloptr)
library(pander)

# personal functions
# get the number of continuous quits before current trial
seqQuits <- function(subjData = 1){
  
    # get the lag
    tempSeq <- sequence(rle(subjData)$lengths) # sequence
    tempSeq[subjData==1] <- 0 # not interested in completed runs, just quits
    tempSeq <- c(0,tempSeq) # lag it so it can be used as a t-1 regressor

    # return data so that the length is equivalent
    out <- tempSeq[1:length(subjData)]
    
    return(out)
    
}

# generic permutation for 2 groups
permute <- function(group1 = 1, group2 = 2, statType = mean, nPerms = 5000, paired = FALSE, simple = FALSE){
  
    # prep data
    summaryPerm <- list()
    lOne <- length(group1)
    lTwo <- length(group2)
    bigSample <- c(group1,group2)  
    
    if (paired == FALSE) {
      
        
        for (i in 1:nPerms){
          
            # relabel samples
            tempBig <- sample(bigSample)
            tempOne <- tempBig[1:lOne]
            tempTwo <- tempBig[(lOne+1):length(bigSample)]
            
            # stats
            tempDiffs <- statType(tempOne,na.rm=T) - statType(tempTwo,na.rm=T)
            summaryPerm$jointDist[i] <- tempDiffs # statType(tempDiffs, na.rm = T) 
          
        }  
        
    } else {
      
        for (i in 1:nPerms){
          
            # shift labels in a pairwise fashion
            tempDiffs <- statType((-1)^rbinom(lOne,1,0.5) * (group1 - group2))
            summaryPerm$jointDist[i] <- tempDiffs
          
        }
      
    }
  
    # get the observed difference
    diffs <- statType(group1,na.rm=T) - statType(group2,na.rm=T)
    observedAbs <- abs(diffs) # maybe leave it as means here
    observed <- diffs
    summaryPerm$Pval <- 2 * (1 - ecdf(summaryPerm$jointDist)(observedAbs))
    if (length(unique(abs(summaryPerm$jointDist))) == 1) {summaryPerm$Pval <- 1} # if the difference is always the same, then p = 1
    summaryPerm$Observed <- observed
    
    # return the results
    if (simple) {
      
      return(summaryPerm$Pval)
      
    } else if (!simple) {
      
      return(summaryPerm)
      
    }
    
}

# OC-specific computation of the negative log likelihood for model optimization
negLogLik <- function(params, choice, handling, reward) {
  gamma <- exp(params[2]) * handling
  model <- exp(params[1]) * (reward - gamma)
  p = 1 / (1 + exp(-model))
  p[p == 1] <- 0.999
  p[p == 0] <- 0.001
  tempChoice <- rep(NA, length(choice))
  tempChoice[choice == 1] <- log(p[choice == 1])
  tempChoice[choice == 0] <- log(1 - p[choice == 0]) # log of probability of choice 1 when choice 0 occurred
  negLL <- -sum(tempChoice) 
  return(negLL)
}

# optimize the OC model
optimizeOCModel <- function(Data, Algorithm = "NLOPT_LN_NEWUOA", simplify = F, optfun = negLogLik) {
  
  # Data: The participant's log
  # Algorithm: probably let be
  # optfun: an external function to minimize (in this case OC, separately defined as negloglik)
  
  # Prep data
  handling <- Data$Handling
  reward <- Data$Offer
  choice <- Data$Choice
  
  # Prep list of results to be returned
  out <- list()
  out$percentQuit <- mean(choice == 0) * 100
  out$percentAccept <- mean(choice== 1) * 100 
  miss <- (choice != 1) & (choice != 0)
  out$percentMiss <- mean(miss)  * 100
  choice <- choice[!miss]
  reward <- as.numeric(reward[!miss])
  handling <- as.numeric(handling[!miss])
  
  # Establish lower and upper bounds
  LB <- round(log((min(reward)/max(handling)) * 0.99), digits = 4)
  UB <- round(log((max(reward)/min(handling)) * 1.01), digits = 4) # in reality this should be the second largest, since no one would reject the highest val
  
  # Begin defining parameters
  # If choices are one-sided (i.e. all accepted)
  if ((sum(choice) == length(choice)) | (sum(choice) == 0)) {
    ifelse(sum(choice) == length(choice), out$Gamma <- exp(LB), out$Gamma <- exp(UB)) 
    out$Scale <- 1 # it was NA, but in theory a temperature of 1 also indicates noiseless estimates, and allows for easier fit computations
    out$LL <- 0
  } else {
    # Create a feasible region (search space)
    params <- as.matrix(expand.grid(scale = c(-1, 1), gamma = seq(LB, UB, length = 3)))
    # Create a list to check the minimization of the negative log lik.
    info <- list()
    info$negLL <- Inf
    # Define the options to be used during optimization
    # Consider looking into other optimization algorithms and global minima
    opts <- list("algorithm" = Algorithm,
               "xtol_rel" = 1.0e-8)
    # Optimize the sOC over all possible combinations of starting points
    for (i in seq(nrow(params))) {
      tempInfo <- nloptr(x0 = params[i, ], 
                   eval_f = optfun, 
                   lb = c(log(0.001), LB),
                   ub = c(-log(0.001), UB),
                   opts = opts,
                   choice = choice, 
                   handling = handling,
                   reward = reward)
      if (tempInfo$objective < info$negLL) {
        #print("Minimized")
        info$negLL <- tempInfo$objective
        info$params <- tempInfo$solution
      }
    }
    out$Gamma <- exp(info$params[2])
    out$Scale <- exp(info$params[1])
    out$LL <- -info$negLL
  }
  
  # Summarize the outputs
  out$LL0 <- log(0.5) * length(choice)
  out$Rsquared <- 1 - (out$LL/out$LL0)
  out$subjOC <- out$Gamma * handling
  out$p <- 1 / (1 + exp(-(out$Scale*(reward - out$subjOC))))
  out$predicted <- reward > out$subjOC
  out$predicted[out$predicted == TRUE] <- 1
  out$percentPredicted <- mean(out$predicted == choice) 
  
  # adjust the probabilities in case of extreme gammas
  if (out$Gamma <= exp(LB)) {
    out$Gamma <- exp(LB) # temporary condition because Nlopt is not respecting the lower bound
    out$p <- rep(1, length(choice))
  } else if (out$Gamma == exp(UB)) {
    out$p <- rep(0, length(choice))
  } 
  
  # if doing this with dplyr::do(), return a simplified data.frame instead with the important parameters
  if (simplify) {
    out <- data.frame(out[c(seq(8), 12)])
  }   
  
  return(out)

}

# summary matrices for refrence-changing models
betaMatrix <- function(model, rearrange = NA) {
# get a similarity matrix of the resulting coefficient pairings for the cost conditions
# first, do a full_join based on column names on the list of coefficient vectors from each dummy code relevel
# then match the names of columns and rows so NAs are in the diagonal
  
  # get the names of the reference group per model iteration
  refnames <- names(model)
  
  # coefficient matrix
  temp <- lapply(model, function(data) {coefficients(data)$SubjID[1, 2:4]})
  mixCoeffs <- bind_rows(temp) 
  preln <- ifelse("Cost" %in% substr(names(mixCoeffs), 1, 4), 4, 5) # count how many characters precede the name of each cost (diff across studies)
  dimnames(mixCoeffs) <- list(refnames, substr(names(mixCoeffs), preln + 1, 20))
  mixCoeffs <- as.matrix(mixCoeffs[, match(rownames(mixCoeffs), colnames(mixCoeffs))])
  mixCoeffs[is.na(mixCoeffs)] <- 0
  
  # now the pvals
  temp <- lapply(model, function(data) {as.list(summary(data)$coefficients[2:4, 4])})
  mixPvals <- as.matrix(bind_rows(temp)) 
  dimnames(mixPvals) <- list(refnames, substr(colnames(mixPvals), preln + 1, 20))
  mixPvals <- as.matrix(mixPvals[, match(rownames(mixPvals), colnames(mixPvals))])
  mixPvals[is.na(mixPvals)] <- 1

  # if you would like to re-arrange the coefficient order, supply a vector with the desired sequence
  if (length(rearrange) > 1) {
    mixCoeffs <- mixCoeffs[rearrange, rearrange]
    dimnames(mixCoeffs) <- list(rearrange, rearrange)
    mixPvals <- mixPvals[rearrange, rearrange]
    dimnames(mixPvals) <- list(rearrange, rearrange)
  }
  
  # combine matrices into list to return
  out <- list(Betas = round(mixCoeffs, digits = 2),
              Pvals = round(mixPvals, digits = 5))
  
  return(out)
  
}
```

```{r load, echo = FALSE}
# First looks at the new data
# The RT is upper-bounded because a glitch in the code made one 10s last 14s
setwd('./data/')
files <- dir(pattern = 'main_log.csv')

# load the data and remove extreme subjects
dataAll <- data_frame(SubjID = files) %>% 
  mutate(contents = map(SubjID, ~ read_csv(., col_types = cols())))  %>%
  mutate(SubjID = substring(SubjID, 0, 3)) %>%
  unnest() %>%
  mutate(RT = ifelse(RT > 10.1, 10, RT),
         Half = ifelse(Block < 4, "Half_1", "Half_2"),
         Btype = BlockType) %>%
  unite(Cost, Cost, BlockType) %>%
  mutate(rawChoice = Choice,
         Choice = ifelse(Choice == 2, 1, Choice),
         Cost = case_when(Cost == "WAIT_0" ~ "Wait_C",
                           Cost == "COGNITIVE_0" ~ "Cognitive",
                           Cost == "GRIP_1" ~ "Physical",
                           Cost == "WAIT_1" ~ "Wait_P")) %>%
  group_by(SubjID) %>%
  mutate(BlockOrder = ifelse(Btype[1] == 0, "Cognitive1st", "Physical1st"))


# Get just the subject list and number of subjects
subjList <- unique(data$SubjID)
nSubjs <- length(subjList)
```

## 3. Research Questions

###	3.1. Main question 

Similar to phase 1 (between subjects experiment), we ask whether decisions are affected differently by equivalent time periods of pure delay, cognitive effort, and physical effort. However, we now ask if the acceptance pattern previously observed in the between-subject experiment changes when each participant experiences all forms of cost. The current number of participants is `r nSubjs`.

## 4. Hypotheses

###	4.1. 

Single-option, accept/reject decisions will be influenced by within-subject manipulations of reward magnitude in line with a theoretical reward-maximizing strategy.

4.1.1. Participants will more frequently accept high-reward prospects than low-reward prospects. 

4.1.2. This pattern of acceptances will resemble that of groups that previously experienced each effort separately for equivalent time/reward combinations.

###	4.2. 

Prospect acceptance rates will differ across three within-subject conditions, in which the delays associated with rewards (a) are unfilled, (b) include a cognitive effort requirement, or (c) include a physical effort requirement. Each effort type will be paired with unfilled delay trials in a blocked manner.

4.2.1. Acceptance rates for unfilled-delay trials will be higher than those for both physical and cognitive effort trials.

4.2.2. Acceptance rates for physical effort trials will be lower than those for cognitive effort trials.

4.2.3. Acceptance rates for unfilled-delay trials will not differ based on their pairing with each effort condition.

###	4.3. 

Choices will be well fit by a computational model in which the subjective opportunity cost of time is free to vary across the four between-subject conditions.

4.3.1. Participants will display stable preferences, meaning that the reward amounts they accept in a given cost condition will be similar throughout the experiment.

4.3.2. Subject-specific opportunity cost (OC) estimates will vary inversely with acceptance rates. Thus, the unfilled-delay condition will produce lower OC estimates than both effort conditions, while physical effort will produce the highest OC.


## 16. Analyses

### 16.1. Tests of whether decision makers integrate reward information.

### 16.1.1. 

*To address hypothesis 4.1., A logistic regression will be fit for each participant in order to predict trial-wise acceptances, using reward amount as predictor. The resulting beta coefficients will be pooled across all participants, and we will perform a one-sample rank-sum test to examine whether the they are significantly positive or negative (compared to 0). If the group coefficients are significantly positive, it would mean that a predictor reliably increases the likelihood of acceptance. This will allow us to determine whether increments in reward amounts increased the likelihood of acceptance for each participant.*
	
``` {r 16.1.1., fig.align="center", fig.width = 4, fig.height = 4, echo = FALSE}
# adapt the data so the fits are only performed on choices, not failed attempts (revisit?)
dataList <- dataAll %>% plyr::dlply("SubjID", identity)

# fits
logisticFits_base <- lapply(dataList, function(data) glm(Choice ~ Offer, data = data, family = "binomial")) 

# Summarize
baseCoeffs <- as.data.frame(t(sapply(logisticFits_base, "[[", "coefficients")))
baseCoeffs$ConvIters <- sapply(logisticFits_base, function(x) {summary(x)$iter})
baseCoeffs$R2 <- sapply(logisticFits_base, function(x) {1 - (summary(x)$deviance / summary(x)$null.deviance)})
baseCoeffs$aic <- sapply(logisticFits_base, function(x) {summary(x)$aic})
baseCoeffs$propAccept_total <- sapply(dataList, function(data) {mean(data$Choice)})
baseCoeffs$propAccept_totalSQRD <- baseCoeffs$propAccept_total^2
baseCoeffs$totalEarned <- (dataAll %>% group_by(SubjID) %>% summarize(totalEarned = sum(Offer[Choice == 1])))$totalEarned
baseCoeffs$SubjID <- subjList # add subject list column to join the demographics by it below

# plot to match the one in Cost 2, even though we only have offer as an influence
ggplot(data = baseCoeffs, aes(x = "Offer", y = Offer)) +
  geom_boxplot() +
  ylim(-10, 10) +
  labs(x = "", y = "Coefficients") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme(legend.position = c(0.9, 0.7),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black"),
        text = element_text(size = 16))

```

The Wilcoxon signed rank test showed that the reward coefficients were significantly greater than 0 (V = `r wilcox.test(baseCoeffs$Offer)$statistic`, *p* = `r wilcox.test(baseCoeffs$Offer)$p.value`), indicating that participants were properly influenced by reward amounts.

### 16.1.2. 

*We will perform an extension of the logistic regression from 16.1.1., this time adding an autoregressive covariate containing the number of consecutive quits prior to a given trial. In this way, we will examine the possibility that participant choices were governed by recent quitting history rather than the experimental parameters. Coefficients not significantly different from 0 will denote that a participant did not rely on recent quitting history.*
	
``` {r 16.1.2., echo = FALSE, include = FALSE}
# Create an autoregressive predictor based on previous consecutive quits
dataList <- lapply(dataList, function(data) data %>% mutate(AR = seqQuits(Choice)))

# fits
logisticFits_base <- lapply(dataList, function(data) {glm(Choice ~ Offer + AR, data = data, family = "binomial")}) 

# get the low/high 95% CIs for AR
baseCoeffs <- as.data.frame(t(sapply(logisticFits_base, confint, "AR"))) %>% 
  dplyr::rename(AR_CILow = `2.5 %`, AR_CIHigh = `97.5 %`) %>%
  mutate(SubjID = subjList) %>%
  left_join(baseCoeffs, by = "SubjID")

# reorder so that SubjID comes first
#baseCoeffs <- baseCoeffs[ , c(3,4,5,6,7,8,9,1,2)]

# create a vector indicating whether 0 is included or not
# but first change NA to 0 (visual inspection showed that NAs were tied to p's with near full p(accept), meaning they weren't influenced by history)
baseCoeffs <- baseCoeffs %>% mutate(AR_CILow = ifelse(is.na(AR_CILow), 0, AR_CILow),
                                    AR_CIHigh = ifelse(is.na(AR_CIHigh), 0, AR_CIHigh))
baseCoeffs$AR_effect <- apply(baseCoeffs, 1, function(x) (!(as.numeric(x["AR_CILow"]) <= 0 & as.numeric(x["AR_CIHigh"]) >= 0)))

# alternative method, just do pvalues (maybe adjusted when n is higher)
# mean(sapply(logisticFits_base, function(x) {summary(x)$coefficients[3,4]}) < 0.05)
```

This regression showed that `r sum(baseCoeffs$AR_effect)` out of `r nSubjs` of the subjects were influenced by recent quit history.

###	16.1.3. 

*A general linear model with constant, linear, and quadratic terms will be used to estimate the correspondence between proportion accepted (independent variable) and total earnings (dependent variable). No other covariates will be used, as this analysis is to confirm that over and under accepting are detrimental to total earnings. The quadratic term will be defined as the squared deviation from the optimal overall acceptance rate?*
	
``` {r 16.1.3. prop accept x earnings, echo = FALSE, include = T, fig.align="center",fig.width=4,fig.height=4}
# Notes: I also performed this comparing halves and block types x half, and it doesn't look like participants were accepting less to make more.
# calculate the proportions and earnings per subject, while keeping group info
temp <- dataAll %>%
  filter(rawChoice < 2) %>%
  group_by(SubjID) %>%
  summarise(Earnings = sum(Offer[Choice == 1] / 100),
            pComplete = mean(Choice)) %>%
  mutate(pComplete2 = pComplete^2) %>%
  ungroup()

# linear fit of earnings x prop accept
# adding a regressor for half shows no diffs in earnings
lmEarn <- lm(Earnings ~ pComplete + pComplete2, data = temp)

# plot
(earnFitplot <- ggplot(data = temp, aes(pComplete, Earnings)) + 
         scale_color_manual(values = cols) + 
         geom_point(pch = 21, color = "black", size = 3, show.legend = T) + 
         stat_smooth(aes(y = Earnings), method = "lm", formula = y ~ x + I(x^2), color = "grey30") +
         labs(x = "Proportion Completed", y = "Total Earnings (dollars)") +
         scale_fill_manual(values = cols) +
         ylim(12, 16) +
         theme(panel.grid.major = element_blank(), 
           panel.grid.minor = element_blank(), 
           panel.background = element_blank(), 
           axis.line = element_line(colour = "black"),
           text = element_text(size = 16)))

# extra analyses
# a pre-post paired permutation on earnigs confirms no differences (more evidence in section 16.3.3--prepost)
# this means that, at least in general, participants are not changing their behavior due to monetary incentives
# but because of something else, maybe like fatigue (this pattern holds even at finer subdivisions)
# PPearn <- dataAll %>%
#           #filter(Block %in% c(1,2,5,6)) %>%
#           mutate(Choice = ifelse(Choice > 1, 0, Choice)) %>%
#           group_by(SubjID, Half) %>%
#           summarise(Earnings = sum(Offer[Choice == 1] / 100),
#                     pComplete = mean(Choice)) %>%
#           ungroup() %>% 
#           select(SubjID, Half, Earnings) %>% 
#           spread(Half, Earnings)
# 
# permute(PPearn$Half_1, PPearn$Half_2, paired = TRUE)
```

The table shows the results from this linear model. In accord with the design of the experimental task, participants who either rejected or accepted too many trials received the least amount of money overall. This is confirmed by significance of the quadratic term below.

``` {r 16.1.3. Earnings table, echo=FALSE,fig.align="center",fig.width=9,fig.height=5}
# Summary table for the linear model
panderOptions("digits", 2)
pander(lmEarn, style = "rmarkdown")
```  

###	16.1.4. 

*To determine the optimality of the group's decisions, we will examine whether each cost type produced a bias to over or under-accept at 4 and 8 cents (assuming that 20 cents will always be accepted). The reward-maximizing strategy is to always reject 4 cents and accept 8 cents, yielding a combined optimal proportion of acceptances of 50%. We will perform a two-sided one-sample chi-squared test of proportions against the null probability of 0.5 for each type of cost. We will report the estimated overall proportion of acceptances for these rewards along with the statistics to show the direction of the bias.*

``` {r 16.1.4., echo = FALSE, fig.align="center",fig.width=6,fig.height=4}
# get the proportion of acceptances per reward and trial type across subjects, and do a one sample t test versus a null mu given by the optimal strategy
# note that in the very likely case that every participant will accept or reject for a given reward/cost (i.e. sd = 0): 
# I am just providing a pseudo pvalue as 1 minus the diff. between prevalent choice and the null (so, removing the standard deviation from the t test equation, otherwise you can't divide by 0)
# this is just to track the optimality.
# I am also adding the mean/sd of acceptances per cost/reward and optimality as reference
# summary_costvsreward <- data %>% 
#                           group_by(SubjID, Offer, Cost) %>% 
#                           summarize(prop = mean(Choice)) %>%
#                           group_by(Offer, Cost) %>%
#                           summarize(pval = ifelse(sd(prop) == 0, 
#                                                   1 - (mean(prop) - ifelse(mean(Offer) == 4, 0, 1)), 
#                                                   t.test(prop, mu = (ifelse(mean(Offer) == 4, 0, 1)))$p.value),
#                                     propAccept = mean(prop),
#                                     sdAccept = sd(prop)) %>%
#                           ungroup() %>%
#                           mutate(Optimal = rep(c(0,1,1), each = 4),
#                                  Padjusted = p.adjust(pval, method = "BY"))
# 
# temp <- data %>%
#   group_by(SubjID, Offer, Cost) %>%
#   filter(Offer < 20) %>%
#   summarize(propAccept = mean(Choice)) %>% 
#   spread(Cost, propAccept) %>% 
#   ungroup() %>% 
#   select(Cognitive:Wait_P) %>% 
#   apply(., 2, t.test, mu = 0.5) 
# 
# summary_costvsreward <- data.frame(tstat = sapply(temp, "[[", "statistic"), 
#                                    pval = sapply(temp, "[[", "p.value"))
# rownames(summary_costvsreward) <- names(temp)

# get quits and acceptances per cost type for each subject
# revisit this eventually to clean it up
temp <- dataAll %>%
  group_by(SubjID, Offer, Cost) %>%
  filter(Offer < 20) %>%
  summarize(propAccept = mean(Choice),
            accept = sum(Choice),
            quit = sum(Choice == 0)) %>% 
  gather(act, counts, accept:quit) %>% 
  unite(temp, Cost, act, sep = ".") %>% 
  select(SubjID, Offer, temp, counts) %>% 
  spread(temp, counts) %>%
  ungroup() %>% 
  select(Cognitive.accept:Wait_P.quit) 

# perform a prop.test based on the group's acceptances and quits per cost
temp2 <- lapply(seq(1, 8, by = 2), function(i) {prop.test(sum(temp[,i]), sum(temp[,i] + temp[,i+1]), p = 0.5, conf.level = 0.95)})

# store stats in a dataframe
summary_costvsreward <- data.frame(probability = sapply(temp2, "[[", "estimate"),
                                   chisquared = sapply(temp2, "[[", "statistic"), 
                                   confint = t(sapply(temp2, "[[", "conf.int")),
                                   pval = sapply(temp2, "[[", "p.value"))
rownames(summary_costvsreward) <- unique(gsub("[.].*","",names(temp)))

rm(temp, temp2)

# create table
pander(round(summary_costvsreward, digits = 4))
```

The table shows a significant bias towards low acceptance for 4 and 8 cent rewards for all but one condition (wait trials associated with cognitive effort). The plot in 16.2.1. shows the proportion of acceptances per reward for each cost condition.

### 16.2. Comparisons among the four delay and effort conditions.

###	16.2.1. 

*We will compute the probability of accepting a trial with a mixed-effects logistic regression. Based on the task structure and our main question, our a priori model of interest will include cost condition and reward amount as fixed main effects, and subject ID as a random effect. Cost condition will be modeled with three categorical terms, with the fourth condition as the reference condition (two effort conditions, and two delay conditions corresponding to each effort type). We will run three versions of the model with different reference conditions, in order to test all relevant pairwise differences among the four cost conditions (per 4.2). We anticipate significant main effects (coefficients different than zero) for reward and cost condition. We hypothesize that the differences among cost conditions will follow the pattern described in 4.2.* 

``` {r 16.2.1. Overall Proportion diffs, fig.align = "center", fig.width = 6, fig.height = 4, echo = FALSE}  
# ## ECDF of proportion completed per cost type
# temp <- dataAll %>%
#         filter(Choice < 2) %>%
#         #mutate(Cost = factor(Cost, levels = list("Physical", "Cognitive", "Wait_0", "Wait_1"))) %>%
#         group_by(SubjID, Cost) %>%
#         summarise(pComplete = mean(Choice))
# 
# 
# # plots
# (p2 <- ggplot(temp, aes(Cost, pComplete, fill = Cost)) + 
#         geom_boxplot(show.legend = F) +
#         scale_fill_manual(values = cols) +
#         ylim(c(0, 1)) +
#         labs(y = "Proportion Completed") +
#         theme(panel.grid.major = element_blank(), 
#             panel.grid.minor = element_blank(), 
#             panel.background = element_blank(), 
#             axis.line = element_line(colour = "black"),
#             text = element_text(size = 16)))


```



``` {r 16.2.1., echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 5}
# both methods yield similar outputs, at least coefficients and pvalues
# but the likelihoods will differ of course. The second one might be better in terms of computational needs.
# method 1
# mixLogis_main <- list()
# mixLogis_main$Cog <- glmer(Choice ~ Cost + Offer + (1 | SubjID), family = "binomial", data = data)

# method 2
mixLogis_main <- list()
mixData <- dataAll %>%
  group_by(SubjID, Cost, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_main$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait_C"))
mixLogis_main$Wait_C <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait_P"))
mixLogis_main$Wait_P <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Physical"))
mixLogis_main$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)


# plot proportion accepted per trial/offer combo
(n2 <- dataAll %>%
        filter(Choice < 2) %>%
        group_by(Cost, Offer) %>%
        summarise(propAccept = mean(Choice),
                  SE = sd(Choice) / sqrt(nSubjs)) %>%
        ungroup() %>%
        mutate(Optimal = rep(c(0, 1, 1), length(unique(dataAll$Cost))),
               #BlockType = as.factor(rep(c(0,1,0,1), each = 3)),
               Offer = ifelse(Offer == 20, 12, Offer)) %>%
        ggplot(aes(Offer, propAccept, color = Cost)) +
          geom_point(size = 3, show.legend = T) +
          geom_point(aes(y = Optimal), size = 3, pch = 21, fill = "grey75", color = "grey30") +
          geom_line(lwd = 1, show.legend = T) +
          scale_color_manual(values = cols) +
          scale_fill_manual(values = cols) +
          geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.4, size = 1) +
          scale_y_continuous(limits = c(0, 1), breaks = c(0,0.5,1)) +
          scale_x_continuous(breaks = c(4, 8, 12), labels = c("4", "8", "20")) +
          labs(y = "Proportion Accepted") +
          theme(legend.position = c(0.8, 0.25),
                legend.key = element_blank(),
                panel.grid.major = element_blank(),
                panel.grid.minor = element_blank(),
                panel.background = element_blank(),
                axis.line = element_line(colour = "black"),
                text = element_text(size = 22)))

```



The plot above shows that the pattern of reward acceptances does indeed resemble that from the between-subject experiment for an equivalent time combination. However, the differences in cost are mostly reversed from the previous experiment, following the order stipulated in 4.2. The matrix below shows the coefficients for all cost comparisons, with coefficient size in color and respective p-values enclosed in each box. This figure formally confirms the pattern seen above, although the wait conditions do trend towards significance.

``` {r 16.2.1. corrplot, echo = FALSE, fig.align="center", fig.width=5, fig.height=4}
# get summary of coeffs
rearrange <- c("Cognitive", "Wait_C", "Wait_P", "Physical")
mixSummary <- betaMatrix(mixLogis_main, rearrange = rearrange)
diag(mixSummary$Pvals) <- NA

# remove uninteresting comparisons
mixSummary$Betas[rbind(c(3, 1), c(4,2))] <- NA
mixSummary$Pvals[rbind(c(3, 1), c(4,2))] <- NA

# invert colors
col2 <- colorRampPalette(c("#053061", "#2166AC", "#4393C3", "#92C5DE", "#D1E5F0", "#FFFFFF", "#FDDBC7", "#F4A582", "#D6604D", "#B2182B", "#67001F"))

corrplot(mixSummary$Betas, 
         is.corr = F, 
         p.mat = mixSummary$Pvals, 
         type = "lower",
         outline = T,
         #insig = "p-value",
         sig.level = 0.05, 
         na.label = "square", 
         na.label.col = "grey",
         method = "color", 
         tl.col = "black",
         tl.cex = 0.8,
         col = col2(200))
         
```

###	16.2.2.

*Next, we will examine whether the a priori model from 16.2.1. outperforms both simpler and more complex models. Unlike the individual logistic regression fits in 16.1.1., a mixed-effects approach gives us a better goodness of fit measure for model comparisons. We will determine the best model (combination of predictors) using Akaike's Information Criterion (AIC) and Bayesian Information Criterion (BIC) to determine the model that minimizes the negative log-likelihood. The regression with each combination of predictors will be fitted in the following order: 1) intercept only; 2) condition only; 3) reward only; 4) condition and reward main effects (from 16.2.3.); and 5) adding a two-way interaction. We predict that model 4 will have the last considerable decrease in the negative log-likelihood.*

``` {r 16.2.2., echo = FALSE, fig.align="center", fig.width=6, fig.height=4}
# Quasibinomial GLM versions (with proper proportion setup)
mixLogis_compare <- list()
mixLogis_compare$Base <- glmer(cbind(totalAccepted, totalQuits) ~ 1 + (1 | SubjID), family = "binomial", data = mixData) # intercept only
mixLogis_compare$Cost <- glmer(cbind(totalAccepted, totalQuits) ~ Cost + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare$Reward <- glmer(cbind(totalAccepted, totalQuits) ~ Offer + (1 | SubjID), family = "binomial", data = mixData)
mixLogis_compare$AllMain <- mixLogis_main$Cognitive
mixLogis_compare$Interaction <- glmer(cbind(totalAccepted, totalQuits) ~ Cost * Offer + (1 | SubjID), family = "binomial", data = mixData)

# Model selection 
abicLogis_compare <- data.frame(Model = names(mixLogis_compare),
                                AIC = sapply(mixLogis_compare, AIC),
                                BIC = sapply(mixLogis_compare, BIC))


# plot 
(mixLogis_compare$plot <- abicLogis_compare %>% 
  gather(Comparison, Value, AIC:BIC) %>%
  mutate(Model = factor(Model, levels = names(mixLogis_compare))) %>%
  ggplot(aes(Model, Value, fill = Comparison)) +
    geom_point(color = "black", pch = 21, size = 3) +
    theme_classic())
  
# effect size
mixLogis_compare$Rsq <- 1 - (deviance(mixLogis_compare$AllMain)/deviance(mixLogis_compare$Base)) # denominator is the null deviance
```

The figure shows that the model with all main effects had the lowest AIC, but not BIC (pseudo R-square: `r round(mixLogis_compare$Rsq, digits = 2)`). We performed a likelihood ratio test to formally determine whether adding cost type to the reward-only model significantly improved the fit. We computed the difference in deviance between the models, and evaluated it under a chi-squared distribution with 3 degrees of freedom (i.e. the number of parameters added to the model, as cost was dummy-coded). The table below summarizes the results, showing that the addition of cost type significantly improved the model fit. (personal note: sure, AIC is 1 value lower when considering the difference in dfs, but that's barely making it..)

``` {r 16.2.2. LRT (anova) table, echo = F}
test <- anova(mixLogis_compare$Reward, mixLogis_compare$AllMain)
rownames(test) <- c("Reward Only", "All Main")

pander(test[c(1, 2, 3, 5, 6, 8)])
```

### 16.3. Modeling the subjective opportunity cost in each condition.

###	16.3.1. 

*Response times (RT) for quit responses will be presented in a descriptive manner in order to examine whether participants tended to quit early or late within individual trials. Each cost condition's response time distribution will contain the pooled RT across participants, and we will display the empirical cumulative distribution functions for each condition. Short RT would suggest confident and stable decisions (in support of 4.3.1.).*

``` {r 16.3.1., echo = FALSE, fig.align="center", fig.width=6, fig.height=4}
# Proportion of forced travels per subject and cost type
propFails <- dataAll %>%
  filter(Cost != "WAIT") %>%
  group_by(SubjID, Cost) %>%
  summarise(propFails = mean(rawChoice == 2)) %>%
  group_by(Cost) %>%
  summarise(meanQuit = mean(propFails))
# 
# # Reaction times ECDFs including offer and in-trial quits
# (RTs <- data %>%
#   mutate(RT = ifelse(ResponseType != "OfferQuit", RT + 2, RT)) %>%
#   filter(rawChoice < 2) %>% 
#   ggplot(aes(RT, color = Cost)) + 
#     #stat_ecdf(lwd = 1.2) +
#     geom_line(aes(y = 1 - ..y..), stat = 'ecdf', lwd = 1.5) +
#     geom_vline(xintercept = 2, lty = 2) + # end of the offer period
#     scale_color_manual(values = cols) +
#     scale_x_continuous(breaks = seq(12), limits = c(0, 14)) +
#     labs(x = "Time in Cost", y = "Proportion Completed") +
#     theme(panel.grid.major = element_blank(),
#       panel.grid.minor = element_blank(),
#       panel.background = element_blank(),
#       axis.line = element_line(colour = "black"),
#       text = element_text(size = 16)))

# NEW VERSION OF SURVIVAL. BUT STILL NEEDS A BIT OF WORK. ALSO, CHECK THAT RT > 14s

library(survival)
library(ggfortify)

# this version of a survival curve forces all successful acceptances to have an RT of 14
# that's to avoid jumps from 2s to 10s to 14s
# 1 - Choice is to reverse the coding of choices, since the survival package assumes 0 = censor?
# to note re: x-axis. In this exp, p's could quit within the 2s offer window. Cost 2 didn't allow that, so it counts from when trial begins.
temp <- dataAll %>%
    mutate(RT = ifelse(ResponseType != "OfferQuit", RT + 2, RT)) %>%
    #filter(rawChoice < 2) %>% 
    mutate(#RT = ifelse(Choice == 1, 14, RT),
           Choice = 1 - Choice) 

survData <- survfit(Surv(RT, Choice) ~ Cost, data = temp)

autoplot(survData) +
  geom_vline(xintercept = 2, lty = 2) + # end of the grace period for physical trials
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  scale_x_continuous(breaks = seq(12)) +
  ylim(0, 1) +
  scale_x_continuous(breaks = seq(0, 12), labels = seq(-2, 10)) +
  labs(x = "Time in Trial", y = "Proportion Accepted", fill = "Cost", color = "Cost") +
  theme(legend.position = c(0.35, 0.2),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        text = element_text(size = 22))

# ecdf of RTs across blocks for all subjects combined
dataAll %>%
  mutate(RT = ifelse(ResponseType != "OfferQuit", RT + 2, RT)) %>%
  mutate(Choice = 1 - Choice)  %>%
  filter(ResponseType == "OfferQuit") %>%
  ggplot(aes(RT, group = Block, color = Block)) +
    stat_ecdf() +
    #facet_wrap(vars(Cost)) +
    theme_classic()

```

Participants can quit either during the offer period (2s) or during the handling time (10s), allowing for a total of 12 seconds to give up on the current trial. The survival curve above shows quitting throughout the trial for all participants. Censored points signal completions (at end of interval) and forced travels (throughout the trial). The figure suggests that participants made most of their choices quickly within the offer window. The overall proportion of forced trials was scarce (overall mean proportion of fails for cognitive and physical trials were `r unlist(propFails[1:2,2])`, respectively). 

###	16.3.2. 

*We will also compute the proportion of quits that were performed during the choice window versus during the handling time. If over 80% of quits occurred during the choice window on average, this will suggest that participants were confident in their choices.*

``` {r 16.3.2., echo = FALSE}
# Proportion of quits once the trial started per subject
PropQuits <- dataAll %>%
  filter(!(ResponseType %in% c("Forced Travel", "Forced travel", "Reward"))) %>%
  group_by(SubjID) %>%
  summarise(propAccept = mean(ResponseType == "Quit"))

```

The mean proportion of quits during the handling time was `r round(mean(PropQuits$propAccept), digits = 2)` (SD = `r round(sd(PropQuits$propAccept), digits = 2)`), with `r sum(PropQuits$propAccept > 0.2)` participants accepting fewer than 80% of trials during the offer window. 

###	16.3.3. 

*In order to further examine choice stability (hypothesis 4.3.1.), we will compute each participant's total proportion of acceptances on the first two and last two blocks. We will compute a mixed-effects linear model to predict post-midpoint proportion of acceptances from pre-midpoint rates, with subjects as random intercepts and condition as a dummy-coded fixed effect. We will report the 95% confidence interval (CI) for the pre-midpoint predictor. A CI containing 1 will denote that participants in that condition produced consistent choices, while accounting for the overall acceptance patterns of a given subject across cost types. (**This is a more elaborate version of the between-subject analysis, which did a pre-post prediction based on the proportion of acceptances per subject across groups**).*

``` {r 16.3.3., echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 4, include = F}
# restructure the data
temp <- dataAll %>%
  filter(!(Block %in% c(3,4))) %>%
  group_by(SubjID, Cost, Half) %>%
  summarise(propAccepted = mean(Choice)) %>%
  ungroup() %>%
  spread(Half, propAccepted)

# compute the mixed effects model (linear for this)
mixLogis_prepost <- lmer(Half_2 ~ Half_1 + Cost -1 + (1 | SubjID), data = temp)

# get the coefficients and confidence intervals
t1 <- coefficients(mixLogis_prepost)$SubjID[1, 2:6]
t2 <- as.data.frame(t(confint(mixLogis_prepost)[3:7,]))

# and concatenate them
summary_prepost <- t(bind_rows(t1, t2))
colnames(summary_prepost) <- c("Coefficient", "CI-low", "CI-high")

# # prepost half paired perms
# temp <- temp %>% plyr::dlply("Cost", identity) 
# prepost <- list()
# prepost$cohen <- sapply(temp, function(data) {DescTools::CohenD(data$Half_1, data$Half_2)})
# prepost$perm <- sapply(temp, function(data) {permute(data$Half_1, data$Half_2, paired = T, simple = T)})

## paired tests for first versus last block per cost type
FirstvsLast <- dataAll %>%
  filter(Choice < 2) %>%
  group_by(SubjID) %>%
  mutate(Block = case_when(
    Block %in% c(1, 2) ~ 1,
    Block %in% c(3, 4) ~ 2,
    Block %in% c(5, 6) ~ 3
  )) %>%
  group_by(SubjID, Cost, Block) %>%
  summarise(propAccept_subj = mean(Choice)) %>%
  spread(Block, propAccept_subj) %>%
  rename(First = `1`, Second = `2`, Third = `3`) %>%
  plyr::dlply("Cost", identity)

prepost <- list()
prepost$Perms <- sapply(FirstvsLast, function(x) {permute(x$First, x$Third, paired = T, simple = T)})
prepost$CohenD <- sapply(FirstvsLast, function(x) {DescTools::CohenD(x$First, x$Third)})

rm(temp, t1, t2)
```

``` {r 16.3.3. plots, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 4}
# various plots
# acceptances across blocks
propBlocks <- dataAll %>%
    filter(Choice < 2) %>%
    group_by(SubjID) %>%
    mutate(Block = case_when(
      Block %in% c(1, 2) ~ 1,
      Block %in% c(3, 4) ~ 2,
      Block %in% c(5, 6) ~ 3
    )) %>%
    group_by(SubjID, Cost, Block) %>%
    summarise(propAccept_subj = mean(Choice)) %>%
    group_by(Cost, Block) %>%
    summarize(propAccept = mean(propAccept_subj),
              seAccept = sd(propAccept_subj)/sqrt(nSubjs)) %>%
    ggplot(aes(Block, propAccept, color = Cost)) +
      geom_point(size = 2) +
      geom_errorbar(aes(ymin = propAccept - seAccept, ymax = propAccept + seAccept, color = Cost), width = 0.1) +
      geom_line() +
      ylim(0, 1) +
      labs(y = "Proportion Accepted") +
      scale_x_continuous(breaks = c(1, 2, 3)) +
      scale_color_manual(values = cols) +
      scale_fill_manual(values = cols) +
      theme(legend.key = element_blank(),
         legend.position = c(0.8, 0.25),
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank(),
         axis.line = element_line(colour = "black"),
         text = element_text(size = 16))

# Pre-post midpoint
# calculate mean per trial type to draw horizontal/vertical lines pre/post
df_mean <- dataAll %>%
            filter(Choice < 2, !(Block %in% c(3,4))) %>%
            group_by(SubjID, Cost) %>%
            summarise(propAccept_1 = mean(Choice[Half=="Half_1"]),
                      propAccept_2 = mean(Choice[Half=="Half_2"])) %>%
            group_by(Cost) %>%
            summarise(propAccept_1 = mean(propAccept_1),
                      propAccept_2 = mean(propAccept_2))

mid <- dataAll %>%
  filter(Choice < 2, !(Block %in% c(3,4))) %>%
  group_by(SubjID, Cost) %>%
  summarise(propAccept_1 = mean(Choice[Half=="Half_1"]),
            propAccept_2 = mean(Choice[Half=="Half_2"])) %>%
  ggplot(aes(propAccept_1, propAccept_2, fill = Cost)) +
    geom_point(size = 3, pch = 21, color = "black", show.legend = F) +
    labs(x = "Proportion Accepted - First Block", y = "Proportion Accepted - Last Block") +
    scale_fill_manual(values = cols) +
    scale_color_manual(values = cols) +
    xlim(0, 1) +
    ylim(0, 1) +
    geom_abline(slope = 1, intercept = 0, lty = 2) +
    geom_hline(data = df_mean, aes(yintercept = propAccept_2, color = Cost), linetype="dashed", show.legend = F) +
    geom_vline(data = df_mean, aes(xintercept = propAccept_1, color = Cost), linetype="dashed", show.legend = F) +
    theme(legend.key = element_blank(),
       legend.position = c(0.8, 0.25),
       panel.grid.major = element_blank(),
       panel.grid.minor = element_blank(),
       panel.background = element_blank(),
       axis.line = element_line(colour = "black"),
       text = element_text(size = 16))


# wait vs effort across effort conditions
pAll <- dataAll %>%
  filter(Choice < 2) %>%
  mutate(Cost2 = ifelse(!(Cost %in%  c("Wait_C", "Wait_P")), "EFFORT", "WAIT"),
         BlockType = ifelse(gsub(".*_", "", Cost) == 1, "Physical", "Cognitive")) %>%
  group_by(SubjID, BlockType) %>%
  summarise(Wait_accept = mean(Choice[Cost2 == "WAIT"]),
            Effort_accept = mean(Choice[Cost2 == "EFFORT"])) %>%
  ggplot(aes(Wait_accept, Effort_accept, color = SubjID, shape = BlockType)) +
    geom_point(size = 5, show.legend = T) +
    xlim(0,1) +
    ylim(0,1) +
    geom_abline(slope = 1, intercept = 0, lty = 2) +
    scale_color_manual(values = cols) +
    scale_fill_manual(values = cols) +
    theme_classic()

# load the plots
grid.arrange(mid, propBlocks, ncol = 2)


# plot showing the difference in proportion of acceptances between first and last block, effort vs delay for each block type
# lines connect the same subject between the blocks
dataAll %>%
  group_by(SubjID) %>%
  mutate(Block = case_when(
    Block %in% c(1, 2) ~ 1,
    Block %in% c(3, 4) ~ 2,
    Block %in% c(5, 6) ~ 3
  )) %>%
  #filter(Half == "Half_1", Offer < 20) %>%
  filter(Block != 2) %>%
  mutate(Block = ifelse(Block == 1, "First Block", "Last Block")) %>%
  group_by(SubjID, Cost, Block, Offer) %>%
  summarise(propAccept = mean(Choice)) %>%
  ungroup() %>%
  #mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
  spread(Block, propAccept) %>%
  mutate(Difference = `Last Block` - `First Block`) %>%
  group_by(SubjID, Cost) %>%
  summarise(Diffscore = sum(Difference)/3) %>%
  mutate(Type = ifelse(Cost %in% c("Cognitive", "Physical"), "Effort", "Delay"),
         Cost = ifelse(Cost %in% c("Cognitive", "Wait_C"), "Cognitive", "Physical")) %>%
  spread(Type, Diffscore) %>%
  ggplot(aes(Effort, Delay, group = SubjID, fill = Cost)) +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) +
    geom_path(show.legend = F, alpha = 0.5, linetype = "dashed") +
    geom_point(show.legend = T, pch = 21, color = "black", size = 4) +
    scale_fill_manual(values = cols) +
    annotate("text", x = -0.4, y = 0.6, label = "Increased preference for delay over time", size = 5, color = "grey30") +
    annotate("text", x = -0.5, y = -0.6, label = "All acceptances decreased", size = 5, color = "grey30") +
    annotate("text", x = 0.5, y = 0.6, label = "All acceptances increased", size = 5, color = "grey30") +
    annotate("text", x = 0.4, y = -0.6, label = "Increased preference for effort over time", size = 5, color = "grey30") +
    labs(x = "P(Effort): Last - First Block", y = "P(Delay): Last - First Block", fill = "Block type") +
    ylim(-0.7, 0.7) +
    xlim(-0.7, 0.7) +
    theme(legend.key = element_blank(),
          legend.position = c(0.9, 0.35),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          panel.background = element_blank(), 
          axis.line = element_line(colour = "black"),
          text = element_text(size = 20))


### CODE TO LOOK AT EARNINGS PER BLOCK AND CALCULATE OPTIMAL EARNINGS
# # code to calculate the rwd/sec rate for each acceptance threshold per handling
# rwd <- c(4, 8, 20)
# handling <- c(14, 10, 2) + 2 # add 2 to H and T to account for offer and reward disclosure windows
# travel <- c(2, 6, 14) + 2
# rwdRates <- sapply(3:1, function(thresh) {(sum(tail(rwd, n = thresh))) / ((handling * thresh) + (travel * 3))})
# 
# # the highest reward amount for a typical 7-min block per handling is:
# apply(rwdRates, 1, max) * (7 * 60)

# # this helps make sense of the toal earnings
# # plotting mean earnings per cost and block
# dataAll %>%
#   filter(rawChoice == 1) %>%
#   group_by(SubjID) %>%
#   mutate(Block = case_when(
#     Block %in% c(1, 2) ~ 1,
#     Block %in% c(3, 4) ~ 2,
#     Block %in% c(5, 6) ~ 3
#   )) %>%
#   group_by(SubjID, Cost, Block) %>%
#   summarise(subEarnings = sum(Offer)) %>%
#   group_by(Cost, Block) %>%
#   summarise(meanEarned = mean(subEarnings),
#             seEarned = sd(subEarnings) / sqrt(nSubjs)) %>%
#   ggplot(aes(Block, meanEarned, color = Cost)) +
#     geom_point(size = 2) +
#     geom_errorbar(aes(ymin = meanEarned - seEarned, ymax = meanEarned + seEarned, color = Cost), width = 0.1, size = 1.5) +
#     geom_line(size = 1.5) +
#     labs(y = "Mean Earnings +- SE (cents)") +
#     scale_x_continuous(breaks = c(1, 2, 3)) +
#     scale_color_manual(values = cols) +
#     scale_fill_manual(values = cols) +
#     theme(legend.key = element_blank(),
#           #legend.position = c(0.1, 0.8),
#           panel.grid.major = element_blank(),
#           panel.grid.minor = element_blank(),
#           panel.background = element_blank(),
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 22))

```

These participants tended to accept fewer trials on the second half of the experiment (Beta = `r round(summary_prepost[1,1], digits = 2)`, 95% CI: `r round(summary_prepost[1,2:3], digits = 2)`). The figure on the left shows this tendency, although some subjects shows considerable consistency. The plot on the right divides the proportion of acceptances across blocks (each cost block is experienced thrice), and shows a global decrease in acceptances as a function of experimental time.

A better test is to perform pairwise comparisons between the first and last block per cost. Paired permutations on means of proportions showed that all participants accepted fewer trials during the second half, regardless of cost (all p < 0.05; Cohen's D: cognitive = `r prepost$CohenD[1]`, physical = `r prepost$CohenD[2]`, wait-cognitive = `r prepost$CohenD[1]`, wait-physical = `r prepost$CohenD[1]`).

Now let's divide them by reward and handling amounts. The plots below show that initially participant choices resembled those from the between-subject experiment, accepting more cognitive trials than any other, and displaying higher costs for physical/wait_p. However, by the end of the experiment all cost types were exacerbated. As seen before, cognitive trials had the greatest decrease in acceptances across reward amounts. This re-evaluation was not present when participants faced only one type of cost.

``` {r 16.3.3. Plot Pre/Post reward x handling, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 4}
# # first block
# t1 <- data %>%
#   group_by(SubjID) %>%
#   mutate(Block = do.call(c, lapply(c(2, 4, 6), function(x) {rep(x / 2, sum(Block %in% c(x - 1, x)))}))) %>%
#   #filter(Half == "Half_1", Offer < 20) %>%
#   filter(Block == 1) %>%
#   group_by(Cost, Offer) %>%
#   summarise(propAccept = mean(Choice),
#             SE = sd(Choice) / sqrt(nSubjs)) %>%
#   ungroup() %>%
#   mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
#   ggplot(aes(Offer, propAccept, color = Cost)) +
#     geom_point(size = 3, show.legend = F) +
#     #geom_point(aes(y = Optimal), size = 3, pch = 21, color = "darkgrey") +
#     geom_line(show.legend = F) +
#     scale_color_manual(values = cols) +
#     scale_fill_manual(values = cols) +
#     geom_errorbar(aes(ymin=propAccept-SE, ymax=propAccept+SE), width=0.2, size=1, show.legend = T) +
#     scale_y_continuous(limits = c(-0.1,1.1), breaks = c(0,0.5,1)) +
#     scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
#     labs(x = "Reward", y = "Proportion Accepted", title = "First Block") +
#     theme(legend.key = element_blank(),
#           legend.position = c(0.8, 0.25),
#           panel.grid.major = element_blank(), 
#           panel.grid.minor = element_blank(), 
#           panel.background = element_blank(), 
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))
# 
# 
# # last block
# t3 <- data %>%
#     group_by(SubjID) %>%
#     mutate(Block = do.call(c, lapply(c(2, 4, 6), function(x) {rep(x / 2, sum(Block %in% c(x - 1, x)))}))) %>%
#     #filter(Half == "Half_2", Offer < 20) %>%
#     filter(Block == 3) %>%
#     group_by(Cost, Offer) %>%
#     summarise(propAccept = mean(Choice),
#               SE = sd(Choice) / sqrt(nSubjs)) %>%
#     ungroup() %>%
#     mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
#     ggplot(aes(Offer, propAccept, color = Cost)) +
#       geom_point(size = 3, show.legend = F) +
#       geom_line(show.legend = F) +
#       scale_color_manual(values = cols) +
#       scale_fill_manual(values = cols) +
#       geom_errorbar(aes(ymin=propAccept-SE, ymax=propAccept+SE), width=0.2, size=1, show.legend = F) +
#       scale_y_continuous(limits = c(-0.1,1.1), breaks = c(0,0.5,1)) +
#       scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
#       labs(x = "Reward", y = "", title = "Last Block") +
#       theme(legend.position = c(0.1, 0.7),
#             panel.grid.major = element_blank(), 
#             panel.grid.minor = element_blank(), 
#             panel.background = element_blank(), 
#             axis.line = element_line(colour = "black"),
#             text = element_text(size = 16))
# 
# grid.arrange(t1, t3, ncol = 2)

dataAll %>%
  group_by(SubjID) %>%
  mutate(Block = case_when(
    Block %in% c(1, 2) ~ 1,
    Block %in% c(3, 4) ~ 2,
    Block %in% c(5, 6) ~ 3
  )) %>%
  #filter(Half == "Half_1", Offer < 20) %>%
  filter(Block != 2) %>%
  mutate(Block = ifelse(Block == 1, "First Block", "Last Block")) %>%
  group_by(Cost, Block, Offer) %>%
  summarise(propAccept = mean(Choice),
            SE = sd(Choice) / sqrt(nSubjs)) %>%
  ungroup() %>%
  mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
  ggplot(aes(Offer, propAccept, color = Cost)) +
    geom_point(size = 3, show.legend = T) +
    #geom_point(aes(y = Optimal), size = 3, pch = 21, color = "darkgrey") +
    geom_line(size = 1, show.legend = F) +
    scale_color_manual(values = cols) +
    scale_fill_manual(values = cols) +
    geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.3, size = 1, show.legend = T) +
    scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1)) +
    scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
    labs(x = "Reward", y = "Proportion Accepted") +
    facet_wrap(vars(Block)) +
    theme(legend.key = element_blank(),
          legend.position = c(0.9, 0.25),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          panel.background = element_blank(), 
          axis.line = element_line(colour = "black"),
          text = element_text(size = 22))

```

This prompted the question of whether fitting the mixed logistic model could confirm this shift in preferences. Just as before, the matrix below show the coefficient magnitude (color) and pvalues for differences based on iterating through costs as reference dummy codes.

``` {r 16.3.3. mix pre/post separately, echo = FALSE, fig.align="center", fig.width=5, fig.height=4}
## compute the same mixed effects logistic for each half
# mixed logistic for the first half
mixLogis_pre <- list()
mixData <- dataAll %>%
  filter(Half == "Half_1") %>%
  group_by(SubjID, Cost, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_pre$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait_C"))
mixLogis_pre$Wait_C <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait_P"))
mixLogis_pre$Wait_P <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Physical"))
mixLogis_pre$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)


# mixed logistic for the second half
mixLogis_post <- list()
mixData <- dataAll %>%
  filter(Half == "Half_2") %>%
  group_by(SubjID, Cost, Offer) %>%
  summarize(totalChoices = length(Choice),
            totalAccepted = sum(Choice),
            totalQuits = sum(Choice == 0),
            propAccepted = mean(Choice))

mixLogis_post$Cognitive <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait_C"))
mixLogis_post$Wait_C <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Wait_P"))
mixLogis_post$Wait_P <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)
mixData <- within(mixData, Cost <- relevel(Cost, ref = "Physical"))
mixLogis_post$Physical <-  glmer(cbind(totalAccepted, totalQuits) ~ Cost + Offer + (1 | SubjID), family = "binomial", data = mixData)

## now produce a summary matrix
# get the beta and pvalue matrices 
betasPre <- betaMatrix(mixLogis_pre)
betasPost <- betaMatrix(mixLogis_post)

# and combine them
betaMat <- matrix(NA, nrow = nrow(betasPre$Betas), ncol = nrow(betasPre$Betas))
betaMat[lower.tri(betaMat)] <- betasPre$Betas[lower.tri(betasPre$Betas)]
betaMat[upper.tri(betaMat)] <- betasPost$Betas[upper.tri(betasPost$Betas)]
dimnames(betaMat) <- list(names(mixLogis_pre), names(mixLogis_pre))

pvalMat <- matrix(NA, nrow = nrow(betasPre$Pvals), ncol = nrow(betasPre$Pvals))
pvalMat[lower.tri(pvalMat)] <- betasPre$Pvals[lower.tri(betasPre$Pvals)]
pvalMat[upper.tri(pvalMat)] <- betasPost$Pvals[upper.tri(betasPost$Pvals)]
dimnames(pvalMat) <- list(names(mixLogis_pre), names(mixLogis_pre))

# remove uninteresting comparisons 
diag(betaMat) <- 0
betaMat[rbind(c(1, 3), c(2, 4), c(3, 1), c(4,2))] <- NA
pvalMat[rbind(c(1, 3), c(2, 4), c(3, 1), c(4,2))] <- NA

# aand plot
corrplot(betaMat, 
         is.corr = F, 
         p.mat = pvalMat, 
         outline = T,
         #insig = "p-value",
         sig.level = 0.05, 
         na.label = "square", 
         na.label.col = "grey",
         method = "color", 
         tl.col = "black",
         tl.cex = 0.8,
         col = col2(200))
```

Finally, we can compare how the difference in proportion accepted between last and first blocks changes between trial types. The plots below show that effort and delay trials in the cognitive blocks tended to be rejected more on the last block (i.e. they land on the negative side of both axes). The plot on the right shows a similar pattern for trials on the physical blocks, but these are more similar across subjects, and also show a small tendency to accept more on either first or last blocks.

``` {r 16.3.3 temp plot, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 4}

cw <- dataAll %>%
  group_by(SubjID) %>%
  mutate(Block = case_when(
    Block %in% c(1, 2) ~ 1,
    Block %in% c(3, 4) ~ 2,
    Block %in% c(5, 6) ~ 3
  )) %>%
  filter(Offer < 20) %>%
  filter(Block != 2) %>%
  group_by(SubjID, Block, Cost, Offer) %>%
  summarise(pAccept = mean(Choice)) %>%
  spread(Block, pAccept) %>%
  mutate(diff = `3` - `1`) %>%
  ungroup() %>%
  select(-c(`1`, `3`)) %>%
  spread(Cost, diff) %>%
  ggplot(aes(Cognitive, Wait_C, fill = as.character(Offer))) +
    geom_point(pch = 21, color = "black", size = 3) +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    ylim(-1, 1) +
    xlim(-1, 1) +
    labs(#title = "P(Accepted): Last - First Block",
         x = "P(Cognitive): Last - First",
         y = "P(Wait_C): Last - First",
         fill = "Offer") +
    theme(legend.key = element_blank(),
          legend.background = element_rect(fill = "grey80"),
          legend.position = c(0.8, 0.8),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          panel.background = element_blank(), 
          axis.line = element_line(colour = "black"),
          text = element_text(size = 16))

gw <- dataAll %>%
  group_by(SubjID) %>%
  mutate(Block = case_when(
    Block %in% c(1, 2) ~ 1,
    Block %in% c(3, 4) ~ 2,
    Block %in% c(5, 6) ~ 3
  )) %>%
  filter(Offer < 20) %>%
  filter(Block != 2) %>%
  group_by(SubjID, Block, Cost, Offer) %>%
  summarise(pAccept = mean(Choice)) %>%
  spread(Block, pAccept) %>%
  mutate(diff = `3` - `1`) %>%
  ungroup() %>%
  select(-c(`1`, `3`)) %>%
  spread(Cost, diff) %>%
  ggplot(aes(Physical, Wait_P, fill = as.character(Offer))) +
    geom_point(pch = 21, color = "black", size = 3, show.legend = F) +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    ylim(-1, 1) +
    xlim(-1, 1) +
    labs(#title = "P(Accepted): Last - First Block",
         x = "P(Physical): Last - First",
         y = "P(Wait_P): Last - First",
         fill = "Offer") +
    theme(legend.position = c(0.8, 0.8),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          panel.background = element_blank(), 
          axis.line = element_line(colour = "black"),
          text = element_text(size = 16))


grid.arrange(cw, gw, ncol = 2)

# more prepost tests
# diffscore prepost for effort vs wait
dataAll %>%
  group_by(SubjID) %>%
  mutate(Block = case_when(
    Block %in% c(1, 2) ~ 1,
    Block %in% c(3, 4) ~ 2,
    Block %in% c(5, 6) ~ 3
  )) %>%
  #filter(Half == "Half_1", Offer < 20) %>%
  filter(Block != 2) %>%
  mutate(Block = ifelse(Block == 1, "First Block", "Last Block")) %>%
  group_by(SubjID, Cost, Block, Offer) %>%
  summarise(propAccept = mean(Choice)) %>%
  ungroup() %>%
  #mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
  spread(Block, propAccept) %>%
  mutate(Difference = `Last Block` - `First Block`) %>%
  group_by(SubjID, Cost) %>%
  summarise(Diffscore = sum(Difference)/3) %>%
  spread(Cost, Diffscore) %>%
  ggplot() +
    geom_point(aes(Cognitive, Wait_C), color = cols[1]) +
    geom_point(aes(Physical, Wait_P), color = cols[2]) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    ylim(-0.5, 0.5) +
    xlim(-0.5, 0.5) +
    #geom_hline(yintercept = 0, linetype = "dashed", size = 1) +
    #geom_boxplot(show.legend = F, size = 1.5) +
    #geom_jitter(width=0.1, alpha = 0.3, show.legend = F) +
    #scale_color_manual(values = cols) +
    #scale_fill_manual(values = cols) +
    theme(legend.key = element_blank(),
          legend.position = c(0.9, 0.25),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          panel.background = element_blank(), 
          axis.line = element_line(colour = "black"),
          text = element_text(size = 22))

# diffscore per cost type (you don't see if specific subjects are prone to increasing/decreasing acceptances across them)
dataAll %>%
  group_by(SubjID) %>%
  mutate(Block = case_when(
    Block %in% c(1, 2) ~ 1,
    Block %in% c(3, 4) ~ 2,
    Block %in% c(5, 6) ~ 3
  )) %>%
  #filter(Half == "Half_1", Offer < 20) %>%
  filter(Block != 2) %>%
  mutate(Block = ifelse(Block == 1, "First Block", "Last Block")) %>%
  group_by(SubjID, Cost, Block, Offer) %>%
  summarise(propAccept = mean(Choice)) %>%
  ungroup() %>%
  #mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
  spread(Block, propAccept) %>%
  mutate(Difference = `Last Block` - `First Block`) %>%
  group_by(SubjID, Cost) %>%
  summarise(Diffscore = sum(Difference)/3) %>%
  ggplot(aes(Cost, Diffscore, fill = Cost)) +
    geom_hline(yintercept = 0, linetype = "dashed", size = 1) +
    geom_boxplot(show.legend = F, size = 1.5) +
    geom_jitter(width=0.1, alpha = 0.3, show.legend = F) +
    scale_color_manual(values = cols) +
    scale_fill_manual(values = cols) +
    theme(legend.key = element_blank(),
          legend.position = c(0.9, 0.25),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          panel.background = element_blank(), 
          axis.line = element_line(colour = "black"),
          text = element_text(size = 22))
```

### 16.3.4. 

*To estimate the subjective opportunity cost (hypothesis 4.3.2.), we will use a logistic function to model each participant's probability of completing a trial based on the difference between the delayed reward's magnitude and the estimated opportunity cost (OC) for each cost type. OC will be computed as the product of a free parameter (gamma) and the handling time. Both gamma and the temperature parameter of the logistic function will be estimated at the subject level, independently for each subject and cost type.*

``` {r 16.3.4., echo = FALSE, fig.align="center", fig.width=6, fig.height=4}
# Here it might be best to estimate gamma for the wait trials only at first
# once the wait OCs are estimated, then they can be modulated by effort
# Estimate gamma per subject for each group separately
summaryOC <- list()
summaryOC$all <- dataAll %>%
  mutate(Block = case_when(
    Block %in% c(1, 2) ~ 1,
    Block %in% c(3, 4) ~ 2,
    Block %in% c(5, 6) ~ 3
  )) %>%
  #filter(Choice < 2) %>%
  group_by(SubjID, Cost) %>% #group_by(SubjID, Cost, Block) %>%
  do(optimizeOCModel(., simplify = T)) %>%
  ungroup()

# plot
(summaryOC$plot <- ggplot(summaryOC$all, aes(Cost, Gamma, fill = Cost)) +
    geom_boxplot(show.legend = F) +
    geom_hline(yintercept = 0.64, alpha = 0.9, color = "gray20", size = 1) +
    geom_hline(yintercept = 0.7, alpha = 0.9, color = "gray40", size = 1) +
    geom_hline(yintercept = 0.9, alpha = 0.9, color = "gray60", size = 1) +
    ylim(0,1.5) +
    labs(x = "") +
    scale_fill_manual(values = cols) +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.background = element_blank(),
          axis.line = element_line(colour = "black"),
          text = element_text(size = 16)))

# try to replicate the group-level proportion of acceptances from 16.2
# this looks pretty much like the empirical proportion data, so the stochasticity is playing a major part in the estimation
# the estimate probability even matches actual proportions at the single subject level 
# logis <- mapply(function(g, t) {1 / (1 + exp((-1 * t) * (c(4, 8, 20) - (g * 10))))}, summaryOC$all$Gamma, summaryOC$all$Temperature)
# colnames(logis) <- unite(summaryOC, test, SubjID, Cost)$test
# logis2 <- as.data.frame(logis) %>% 
#   gather(Cost, Probability) %>%
#   mutate(SubjID = substr(Cost, 1,3), 
#          Cost = substr(Cost, 5,20),
#          Reward = rep(c(4,8,20), nrow(.)/3)) %>%
#   group_by(Cost, Reward) %>%
#   summarize(propAccept = mean(Probability)) %>%
#   ggplot(logis2, aes(Reward, propAccept, color = Cost)) +
#     geom_point(size = 3, show.legend = T) +
#     geom_line(lwd = 1, show.legend = T) +
#     scale_color_manual(values = cols) +
#     scale_fill_manual(values = cols) +
#     #geom_errorbar(aes(ymin = propAccept - SE, ymax = propAccept + SE), width = 0.4, size = 1) +
#     scale_y_continuous(limits = c(-0.1,1.1), breaks = c(0,0.5,1)) +
#     scale_x_continuous(breaks = c(4, 8, 12), labels = c("4", "8", "20")) +
#     labs(y = "Proportion Accepted") +
#     theme(legend.key = element_blank(),
#           panel.grid.major = element_blank(),
#           panel.grid.minor = element_blank(),
#           panel.background = element_blank(),
#           axis.line = element_line(colour = "black"),
#           text = element_text(size = 16))
```

The figure on the left shows the distribution of fitted gammas for each cost type, and inversely matches the proportion of acceptances described previously. The plot on the right shows the OC for each subject, divided by cost type. The horizontal line denotes total earnings per second under the optimal strategy for the time and reward combinations.

###	16.3.5.

*We will cross-validate each subject's OC value using the first four blocks (two cognitive, two physical) for estimation, and the last two blocks (one of each effort type) choices for testing. The estimates will be used to predict acceptances in the testing sample, and the mean percent correctly predicted will be reported for each cost type. This will also provide information on the stability of each participant's choices (4.3.1.).*

``` {r 16.3.5., echo = FALSE, fig.width=5, fig.height=4, fig.align="center"}
# subdivide data
dataTrain <- dataAll %>%
   filter(Block < 5)

dataTest <- dataAll %>%
  filter(Block > 4)

# estimate
# per-cost, but consider the idea above
OCs_validation <- list()
temp <- dataTrain %>% filter(Cost == "Wait_C") %>% plyr::dlply("SubjID", identity)
OCs_validation$Wait_C <- sapply(temp, function(Data) {optimizeOCModel(Data)$Gamma})
temp <- dataTrain %>% filter(Cost == "Wait_P") %>% plyr::dlply("SubjID", identity)
OCs_validation$Wait_P <- sapply(temp, function(Data) {optimizeOCModel(Data)$Gamma})
temp <- dataTrain %>% filter(Cost == "Cognitive") %>% plyr::dlply("SubjID", identity)
OCs_validation$Cognitive <- sapply(temp, function(Data) {optimizeOCModel(Data)$Gamma})
temp <- dataTrain %>% filter(Cost == "Physical") %>% plyr::dlply("SubjID", identity)
OCs_validation$Physical <- sapply(temp, function(Data) {optimizeOCModel(Data)$Gamma})

# reshape data
temp <- bind_rows(OCs_validation) %>% 
  mutate(SubjID = subjList) %>%
  gather(Cost, gamma, Wait_C:Physical) %>%
  left_join(dataTest, ., by = c("SubjID", "Cost")) %>% # assign OCs to each subject per cost type
  mutate(prediction = Offer > Handling * gamma) %>%
  # plyr::dlply("SubjID", identity)
  group_by(SubjID, Cost) %>%
  summarize(predicted = mean(Choice == prediction))

# plot prediction %
qplot(Cost, predicted, geom = "boxplot", data = temp) + 
  ylim(0,1) +
  theme_classic()
```

The figure above shows that each participant's estimated gamma successfully predicted choices in the last two blocks to a high degree (suspisiouly, considering the decrease in the acceptance rate as a function of time).  

###	16.3.6. 

*The OC estimates for each group will be compared using a repeated measures ANOVA with cost as a factor. This will let us determine which cost type produced the highest discounting.*

``` {r 16.3.6., echo = FALSE}
# repeated measures anova
OCs_anova <- list()
OCs_anova$aov <- summary(aov(Gamma ~ Cost + Error(SubjID / Cost), data = summaryOC$all))

# table
pander(OCs_anova$aov)

# do all pairwise comparisons
OCs_anova$gammas <- summaryOC$all %>% 
  reshape2::dcast(SubjID ~ Cost, value.var = "Gamma") %>%
  select(-SubjID)

# perform a paired permutation among every pair of cost types
OCs_anova$perms <- outer(OCs_anova$gammas, OCs_anova$gammas, FUN = Vectorize(permute), simple = T, paired = T)
OCs_anova$perms[rbind(c(1,4),c(2,3),c(3,2),c(4,1))] <- NA

# and the effect sizes
OCs_anova$ES <- abs(outer(OCs_anova$gammas, OCs_anova$gammas, FUN = Vectorize(DescTools::CohenD)))
OCs_anova$ES[rbind(c(1,4),c(2,3),c(3,2),c(4,1))] <- NA
```

(note: here 'Cost' is cost type). The ANOVA table shows that there are no significant differences among cost types for the current sample. The following plot shows the resulting p-values from performing a paired permutation analysis among all relevant cost types, with color indicating the effect size of the comparison. The differences only partially match those seen in 16.2.1., so the OC estimates seem noisy.


``` {r 16.3.6. corrplot, echo = FALSE, fig.align="center", fig.width=5, fig.height=5}
corrplot(OCs_anova$ES,
         is.corr = F, 
         p.mat = OCs_anova$perms,
         type = "upper",
         insig = "p-value",
         sig.level = -1,
         na.label = "square",
         na.label.col = "grey",
         method = "color", 
         tl.col = "black", 
         tl.cex = 0.8,
         outline = T)
```

### Testing grounds


``` {r test plots, echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4}
# per-subject acceptances per trial and reward amount
# tp1 <- dataAll %>%
#         group_by(SubjID) %>%
#         mutate(FirstBlock = ifelse(Btype[1] == 0, "Cognitive", "Physical")) %>%
#         group_by(FirstBlock, SubjID, Cost, Offer) %>%
#         summarise(pAccept = mean(Choice),
#                   SE = sd(Choice) / sqrt(length(Choice))) %>%
#         ggplot(aes(Offer, pAccept, color = SubjID)) + 
#           geom_point(size = 3) + 
#           geom_line(aes(group = SubjID), size = 1) +
#           labs(x = "Offer", y = "Proportion Accepted") +
#           facet_wrap(vars(Cost), ncol = 2) +
#           theme(legend.key = element_blank(),
#                 panel.grid.major = element_blank(), 
#                 panel.grid.minor = element_blank(), 
#                 panel.background = element_blank(), 
#                 axis.line = element_line(colour = "black"),
#                 text = element_text(size = 16))

    
# acceptances per cost x reward based on what block was experienced first
# REPLICATE LIAMS PLOTS TOO
tp2 <- dataAll %>%
        group_by(SubjID) %>%
        mutate(FirstBlock = ifelse(Btype[1] == 0, "Cognitive", "Physical")) %>%
        group_by(FirstBlock, SubjID, Cost, Offer) %>%
        summarise(pAccept_subj = mean(Choice)) %>%
        group_by(FirstBlock, Cost, Offer) %>%
        summarise(pAccept = mean(pAccept_subj),
                  SE = sd(pAccept_subj) / sqrt(length(pAccept_subj))) %>%
        mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
        ggplot(aes(Offer, pAccept, color = Cost)) + 
          geom_point(size = 3) + 
          geom_line(aes(group = Cost), size = 1) +
          geom_errorbar(aes(ymin = pAccept - SE, ymax = pAccept + SE), width = 0.6, size = 1, show.legend = F) +
          scale_color_manual(values = cols) +
          scale_fill_manual(values = cols) +
          labs(x = "Offer", y = "Proportion Accepted") +
          facet_wrap(vars(FirstBlock), ncol = 2) +
          scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
          scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1)) +
          theme(legend.key = element_blank(),
                legend.position = c(0.8, 0.25),
                panel.grid.major = element_blank(), 
                panel.grid.minor = element_blank(), 
                panel.background = element_blank(), 
                axis.line = element_line(colour = "black"),
                text = element_text(size = 16))

# prop acceptances per reward amount, divided by first/last block and first block experienced
tp3 <- dataAll %>%
        mutate(Block = case_when(
          Block %in% c(1, 2) ~ 1,
          Block %in% c(3, 4) ~ 2,
          Block %in% c(5, 6) ~ 3
        )) %>%
        filter(Choice < 2, Block %in% c(1, 3)) %>%
        group_by(SubjID) %>%
        mutate(FirstBlock = ifelse(Btype[1] == 0, "Cognitive", "Physical")) %>%
        group_by(Half, FirstBlock, SubjID, Cost, Offer) %>%
        summarise(pAccept_subj = mean(Choice)) %>%
        group_by(Half, FirstBlock, Cost, Offer) %>%
        summarise(pAccept = mean(pAccept_subj),
                  SE = sd(pAccept_subj) / sqrt(length(pAccept_subj))) %>%
        mutate(Offer = ifelse(Offer == 20, 12, Offer)) %>%
        ggplot(aes(Offer, pAccept, color = Cost)) + 
          geom_point(size = 3) + 
          geom_line(aes(group = Cost), size = 1) +
          geom_errorbar(aes(ymin = pAccept - SE, ymax = pAccept + SE), width = 0.6, size = 1, show.legend = F) +
          scale_color_manual(values = cols) +
          scale_fill_manual(values = cols) +
          labs(x = "Offer", y = "Proportion Accepted") +
          scale_x_continuous(breaks = c(4, 8, 12), labels = c(4, 8, 20)) +
          scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1)) +
          facet_wrap(vars(FirstBlock, Half), ncol = 4) +
          theme(legend.key = element_blank(),
                panel.grid.major = element_blank(), 
                panel.grid.minor = element_blank(), 
                panel.background = element_blank(), 
                axis.line = element_line(colour = "black"),
                text = element_text(size = 16))

# Liams plots
tp4_overall <- dataAll %>%
                  filter(Choice < 2) %>%
                  group_by(SubjID) %>%
                  mutate(FirstBlock = ifelse(Btype[1] == 0, "Cognitive", "Physical")) %>%
                  group_by(FirstBlock, SubjID) %>%
                  summarise(pAccept_subj = mean(Choice)) %>%
                  group_by(FirstBlock) %>%
                  summarise(pAccept = mean(pAccept_subj),
                            SE = sd(pAccept_subj) / sqrt(length(pAccept_subj))) %>%
                  qplot(FirstBlock, pAccept, geom = "point", data = .) + 
                  geom_errorbar(aes(ymin = pAccept - SE, ymax = pAccept + SE), width = 0.6, size = 1, show.legend = F) + 
                  ylim(0, 1) +
                  theme_classic()

tp4_percost <- dataAll %>%
                filter(Choice < 2) %>%
                group_by(SubjID) %>%
                mutate(FirstBlock = ifelse(Btype[1] == 0, "Cognitive", "Physical")) %>%
                group_by(FirstBlock, SubjID, Half, Cost) %>%
                summarise(pAccept_subj = mean(Choice)) %>%
                group_by(FirstBlock, Half, Cost) %>%
                summarise(pAccept = mean(pAccept_subj),
                          SE = sd(pAccept_subj) / sqrt(length(pAccept_subj))) %>%
                ggplot(aes(Cost, pAccept, color = FirstBlock)) +
                  geom_point(aes(fill = FirstBlock), size = 3, show.legend = T) +
                  geom_errorbar(aes(ymin = pAccept - SE, ymax = pAccept + SE), width = 0.4, size = 1, show.legend = F) +
                  ylim(0, 1) +
                  facet_wrap(vars(Half), ncol = 2) +
                  theme(legend.key = element_blank(),
                        panel.grid.major = element_blank(), 
                        panel.grid.minor = element_blank(), 
                        panel.background = element_blank(), 
                        axis.line = element_line(colour = "black"),
                        text = element_text(size = 16))

# number of forced travels as a function of time for cognitive and grip trials
dataAll %>%
  mutate(Block = case_when(
    Block %in% c(1, 2) ~ 1,
    Block %in% c(3, 4) ~ 2,
    Block %in% c(5, 6) ~ 3
  )) %>%
  #filter(ResponseType == "Forced Travel") %>% 
  group_by(SubjID, Cost, Block) %>%
  filter(Cost %in% c("Cognitive", "Physical")) %>%
  summarise(pTravel = mean(rawChoice == 2)) %>%
  group_by(Cost, Block) %>%
  summarise(mTravels = mean(pTravel),
            SE = sd(pTravel) / sqrt(nSubjs)) %>%
  ggplot(aes(Block, mTravels, color = Cost)) +
    geom_line(size = 1) +
    geom_point(size = 3) +
    geom_errorbar(aes(ymin = mTravels - SE, ymax = mTravels + SE), width = 0.2) +
    labs(y = "Prop. of forced travels") +
    scale_x_continuous(breaks = seq(3)) +
    ylim(0, NA) +
    theme(legend.key = element_blank(),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          panel.background = element_blank(), 
          axis.line = element_line(colour = "black"),
          text = element_text(size = 16))
  

# ### COGNITIVE LOGS
# # no changes in time, but might be worth revisiting
# # First looks at the new data
# setwd('./data/')
# files <- dir(pattern = 'coglog')
# nSubjs <- length(files)
# 
# # load data and assign subject id (lazy way because this data has no headers)
# temp <- lapply(files, read.csv, header = F)
# for (i in seq_along(temp)) {
#   temp[[i]]$SubjID <- substring(files[i], 8, 10)
# }
# data <- do.call(rbind, temp)
# colnames(data) <- list("Handling", "Offer", "Response", "RT", "CostTime", "ExpTime", "wat", "Task", "Options", "SubjID")
#   
# # Get just the subject list and number of subjects
# subjList <- unique(data$SubjID)
# nSubjs <- length(subjList)
# 
# # create a trial starter indicator per subject
# test <- data %>%
#   filter(Response != 0) %>%
#   group_by(SubjID)  %>%
#   mutate(CostTime = round(CostTime),
#          OfferLag = c(0, lag(Offer)[-1]),
#          TimeLag = c(0, lag(CostTime)[-1]),
#          trialStart = (Offer != OfferLag) | (TimeLag > CostTime)) %>%
#   plyr::dlply("SubjID", identity)
# 
# # now input trial number 
# for (x in seq_along(test)) {
#   data <- test[[x]]
#   trial <- 0
#   temp <- numeric()
#   for(i in seq(nrow(data))) {
#     if (data$trialStart[i]) {
#       trial <- trial + 1
#     }
#     
#     temp <- c(temp, trial)
#   }
#   
#   test[[x]]$trialN <- temp
# }
# 
# # put everyone into a single dataframe
# cogLogs <- do.call(rbind, test) %>%
#   select(SubjID, Handling, Offer, Response, RT, CostTime, trialN) %>%
#   group_by(SubjID, trialN) %>%
#   summarise(nErrors = sum(Response == 2),
#             medianRT = median(RT)) %>%
#   group_by(trialN) %>%
#   summarise(meanErrors = mean(nErrors),
#             meanRT = mean(medianRT)) %>%
#   ggplot(aes(trialN, meanErrors)) +
#     geom_line(show.legend = F, size = 1) +
#     #ylim(0, 1) +
#     theme_classic()

```


## 17. Inference criteria

These criteria will remain the same as before. For the ANOVAs, we will report the F statistic as well as the p-value. For the multiple linear regressions and logistic regression, we will report the betas (coefficients) and their associated p-values. When comparing two groups of values as in pairwise comparisons, we will use non-parametric permutation tests. We will report p-values and effect sizes, and all tests will be two-tailed. The alpha threshold for p-value evaluation will be set at 0.05.

## 18. Data exclusion

### 18.1. Missing data and quitting early. 

Participants are told that they are allowed to stop participation in the study at any time without consequence. Data from participants who do not finish the experiment will be excluded from the study, and those participants will be replaced. 

### 18.2. Inattentiveness

To guard against inattentiveness, a single 30 second catch trial will be placed at the end of each experimental block, merely prompting participants to press a key. From the pilot study, participants are able to respond within three seconds. Based on this, a response within three seconds will be considered as proper engagement, and a participant that fails two or more of these checks will be excluded and replaced. 

### 18.3. Reasonable choices

The task is structured so that one reward amount must always be accepted. Thus, a participant who quits every trial in at least one block will be assumed not to have followed or understood task instructions, or to have disengaged from the task altogether, and will be excluded and replaced. 

### 18.4. Performance

In the cognitive effort condition, participants were forced to travel if they made 2 mistakes in a trial. Any participant with more than 30% forced travels will be excluded and replaced. 

## 5. Existing data 

Registration prior to creation of data

## 6. Data collection
	
The study will use human subjects recruited from the university community, who will be recruited through the university job board. We will pay up to $30 for a session (1 hour), which includes incentive-based rewards. Participants will be ineligible if they have participated in our pilot study, or they are under 18 years of age. The study will run for however long it takes to reach our desired sample size. 
	
## 7. Sample size 

We will acquire a sample of 48 participants. 
	
## 8. Sample size rationale? 

Sample sizes were determined by means of power analysis (repeated measures ANOVA), using a significance level of 0.05, power of 0.8, a desired effect size of f = 0.5, and four groups (one for each condition). The resulting sample size was 45, which we increased to 48 in order to match the potential order of blocks and colors (section 15) while further ensuring that we have enough power. 

## 9. Variables

### 9.1 The present study has three levels of manipulation. 

9.1.1. Cost condition, manipulated within subjects. The conditions are (a) unfilled delay, (b) cognitive effort, and (c) physical effort. Unfilled delays will be paired with each effort type in a blocked manner (7-minute blocks). Each combination will be experienced three times.
		
9.1.2. Handling and travel times. These will dictate how long each trial is. The "handling time" governs how long it is necessary to wait or work to obtain a reward if a trial is accepted. The "travel time" governs how long the inter-trial interval will be, regardless of whether the trial is accepted. Participants will know that the handling time will be always 10 seconds, and the travel time 6 seconds (matching one of the between-group experiment conditions). 
		
9.1.3. Reward magnitude, manipulated within-subject across individual trials. On each trial, participants will learn the amount of money they can earn if they complete the trial (i.e. 4, 8, or 20 cents). The amounts will be sampled uniformly, while guaranteeing that each participant experiences each amount at least twice during each block.


## 10. Measured variables 

### 10.1. Behavioral variables 
		
10.1.1. Choice (complete or quit a given trial). 

10.1.2. Response times. 

10.1.3. Total earnings. 

10.1.3. Performance in the cognitive effort tasks (right or wrong answer). 

10.1.4. Grip force throughout the experiment and maximum grip strength (physical effort condition). 


### 10.2. Questionnaire 

10.2.1. Age.

10.2.2. Gender.

10.2.3. Post-experimental oral assessment of choice strategy adoption. We will ask participants the following: "Did you follow a specific acceptance strategy? If so, what was it?" and "Which condition felt most effortful to you?". Answers will be written down by the experimenter, and recorded on an excel spreadsheet. 

## 11. Indices

### 11.1. Behavioral data 

11.1.1. Proportion of prospects accepted. 

11.1.2. Percentage of cognitive effort trials with above-threshold errors (over 2 errors), in which participants are forced to travel to the next trial. 

### 11.2. Optimal behavior 

The theoretical opportunity cost of time will be calculated for each handling/travel/reward acceptance combination in order to assess choice optimality. This measure is computed as the cents per second possible when consistently accepting only certain reward amounts (e.g. 8 and 20 cents) in a given timing condition. This ground truth measure is independent of cost type, allowing us to assess subject optimality.

## 12. Study type

Experiment - A researcher randomly assigns treatments to study subjects, this includes field or lab experiments. This is also known as an intervention experiment and includes randomized controlled trials.

## 13. Blinding

No blinding is involved in this study.

## 14. Study design 

The present study complements our previous one, which evaluated cost types through a between-group design. Many of the details remain the same. However, this time every participant will experience all forms of cost (delay, physical effort, and cognitive effort). Delay trials will be intermixed with both physical and cognitive trials, providing a common condition to compare these effortful costs. On each trial, participants will have the opportunity to either wait or work for 10 seconds (the "handling time") in order to earn a reward; alternatively, they can quit by pressing the space bar and skip to the next trial (which incurs a 6 second travel time). Two types of trial combinations (physical/wait and cognitive/wait) will be experienced three times by each participant in 7-minute-long, interleaved blocks. Reward amounts can be 4, 8, or 20 cents, uniformly distributed. Timing information will be disclosed at the beginning of each block, and reward prospects displayed before each trial begins. 

## 15. Randomization

### 15.1. Block type 
Each block will contain unfilled delay trials and trials of one effortful cost type (se 14.1). Blocks with each cost type will be interleaved, and participants will be assigned to one of two orders: beginning with a cognitive/wait or a physical/wait block. A row will be assigned to subject IDs prior to recruitment, ensuring that each order is experienced equally often. Participants will be offered a break after completing three out of the six blocks. 

### 15.2. Rewards and cost type 
Participants can earn 4, 8, or 20 cents per trial at the cost of performing one of the three conditions. These values are sampled without replacement from a vector containing every combination of cost (effort defined by the current block, and delay) and reward. The vector is reshuffled once completed. This ensures that each combination of cost and reward is presented at least twice per block.

### 15.3. Cost color 
During the offer phase, each cost type will appear in a unique color in order to ease its identification. Colors were chosen to contrast each other while being amenable to red-green color-blindness. Colors will be assigned to each cost type so that all three colors are equally paired to each cost type across participants. A row will be assigned to subject IDs prior to recruitment to indicate their block order and color pairing. 
